import json
import pandas as pd
import numpy as np
import random
from scipy import sparse
import os
import pickle
from tqdm import tqdm
# from pulp import LpProblem, LpVariable, LpMinimize, LpBinary, lpSum, PULP_CBC_CMD


'''å¯ä¿®æ”¹åƒæ•¸'''
INPUT_PATH = "../data/ml/dataset"

OUTPUT_PATH = "../data/ml/dataset/final_input/keyword_classifier"

# MIN_COUNT = 10  # è¨­å®šåˆªæ‰å‡ºç¾æ¬¡æ•¸ <= MIN_COUNT çš„é—œéµè© (column)

TOLERANCE = 100  # è³‡æ–™é›†ä¸­èª¤å·®æ¨æ–‡æ•¸å€¼ (100 => +-100)

# LABEL_COUNT = 5  # Y ä¸­æœ‰å¹¾çµ„ labels

random.seed(42)  # 42 å¯ä»¥æ›æˆä½ æƒ³è¦çš„æ•¸å­—

IS_FILTERED = False  # çœ‹æ˜¯å¦æœ‰åˆ† normal èˆ‡ bot
'''å¯ä¿®æ”¹åƒæ•¸'''

SUFFIX = "" if IS_FILTERED else "_non_filtered"




# --- å±•é–‹æˆæ¯æ¢æ¨æ–‡ä¸€å€‹å–®ä½ ---
def expand_by_tweet(df):
    expanded = []
    for _, row in df.iterrows():
        expanded.extend([row['date'].strftime("%Y-%m-%d")] * row['tweet_count'])
    return np.array(expanded)



# --- åˆ—å°æ¼²è·Œæ¯”ä¾‹ ---
def print_label_distribution(Y_train, Y_test, COIN_SHORT_NAME):
    def get_stats(y):
        up = np.sum(y >= 0)
        down = np.sum(y < 0)
        total = len(y)
        return up, down, total, up / total, down / total

    train_up, train_down, train_total, train_up_ratio, train_down_ratio = get_stats(Y_train)
    test_up, test_down, test_total, test_up_ratio, test_down_ratio = get_stats(Y_test)

    print(f"{COIN_SHORT_NAME}{SUFFIX} æ¨™ç±¤åˆ†ä½ˆï¼š")
    print(f"  Train: ç¸½æ•¸ {train_total}, æ¼² {train_up} ({train_up_ratio:.2%}), è·Œ {train_down} ({train_down_ratio:.2%})")
    print(f"  Test : ç¸½æ•¸ {test_total}, æ¼² {test_up} ({test_up_ratio:.2%}), è·Œ {test_down} ({test_down_ratio:.2%})\n")



def categorize_array_multi(Y, t1=-0.0590, t2=-0.0102, t3=0.0060, t4=0.0657, ids=None):
    """
    Y: np.ndarray, shape = (num_labels,), åƒ¹æ ¼è®ŠåŒ–ç‡
    t1, t2: äº”å…ƒåˆ†é¡é–¾å€¼ï¼Œç™¾åˆ†æ¯”
    """

    # äº”å…ƒåˆ†é¡
    labels = np.full_like(Y, 2, dtype=int)  # é è¨­æŒå¹³
    labels[Y <= t1] = 0  # å¤§è·Œ
    labels[(Y > t1) & (Y <= t2)] = 1  # è·Œ
    labels[(Y >= t3) & (Y < t4)] = 3  # æ¼²
    labels[Y >= t4] = 4  # å¤§æ¼²

    if ids is not None:
        # æ‰¾å‡º Y==0 çš„ç´¢å¼•
        zero_idx = np.where(Y == 0)[0]
        # åªå–å°æ‡‰çš„ ids
        dates_is_0 = set((ids[i][0], ids[i][1]) for i in zero_idx)
        if len(dates_is_0) > 0:
            print(f"å…±æœ‰ {len(dates_is_0)} å¤© Y==0")
            for id in sorted(dates_is_0):
                print(id)

    if np.any(Y == 0):  # æª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½•å…ƒç´ ç­‰æ–¼ 0
        count = np.sum(Y == 0)
        print(f"å…±æœ‰ {count} å€‹ Y == 0")
        labels[Y == 0] = 4  # ç‚ºäº†æ ¡æ­£ TRUMP å‰å…©å¤©çš„åƒ¹æ ¼ç›¸åŒ ç¬¬ä¸€å¤©è¨­ç‚ºå¤§æ¼²

    return labels



def balance_category_proportion_swap(train_df, test_df, category_id, target_ratio=0.8, tolerance=0.001):
    """
    é€šéäº¤æ› train_df å’Œ test_df ä¸­çš„è¡Œä¾†èª¿æ•´ç‰¹å®šé¡åˆ¥çš„æ¨æ–‡ç¸½æ•¸æ¯”ä¾‹ã€‚
    äº¤æ›è¦æ±‚ï¼šå¿…é ˆæ˜¯ç›¸åŒ category å’Œç›¸åŒ weekday_numã€‚
    """
    
    # å‰µå»ºå‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹ DataFrame (é›–ç„¶åœ¨å‡½æ•¸å¤–éƒ¨å·²ç¶“æ˜¯å‰¯æœ¬ï¼Œä½†é€™æ˜¯è‰¯å¥½çš„å¯¦è¸)
    current_train_df = train_df.copy()
    current_test_df = test_df.copy()
    
    # è¿­ä»£èª¿æ•´è¿´åœˆ
    iteration = 0
    MAX_ITER = 200  # è¨­ç½®æœ€å¤§è¿­ä»£æ¬¡æ•¸ä»¥é˜²æ­¢ç„¡é™è¿´åœˆ
    
    while iteration < MAX_ITER:
        iteration += 1
        
        # 1. è¨ˆç®—ç•¶å‰ç¸½å’Œå’Œæ¯”ä¾‹
        # ç¯©é¸å‡ºç•¶å‰é¡åˆ¥ k çš„æ•¸æ“š
        train_k = current_train_df[current_train_df['category'] == category_id]
        test_k = current_test_df[current_test_df['category'] == category_id]
        
        train_sum_k = train_k['tweet_count'].sum()
        test_sum_k = test_k['tweet_count'].sum()
        total_sum_k = train_sum_k + test_sum_k
        
        # é¿å…é™¤ä»¥é›¶
        if total_sum_k == 0:
            print(f"  é¡åˆ¥ {category_id} ç¸½æ•¸ç‚ºé›¶ï¼Œè·³éèª¿æ•´ã€‚")
            break
            
        current_ratio = train_sum_k / total_sum_k
        ratio_diff = current_ratio - target_ratio
        
        # 2. æª¢æŸ¥æ˜¯å¦æ»¿è¶³ç›®æ¨™æ¯”ä¾‹
        if abs(ratio_diff) <= tolerance:
            print(f"  âœ… é¡åˆ¥ {category_id} æ¯”ä¾‹ {current_ratio:.4f} æ»¿è¶³ç›®æ¨™ {target_ratio:.4f} (å·®ç•° {ratio_diff:.4f})ï¼Œè€—æ™‚ {iteration} è¼ªã€‚")
            break
            
        # 3. æ±ºå®šèª¿æ•´æ–¹å‘
        if ratio_diff > tolerance:
            # Train éå¤š (éœ€è¦ Train ç¸½å’Œä¸‹é™)ã€‚å°‹æ‰¾ Train(å¤§) å’Œ Test(å°) é€²è¡Œäº¤æ›
            
            # å¾ Train ä¸­é¸å‡ºè¦ç§»é™¤çš„è¡Œ (tweet_count æ‡‰è©²è¼ƒå¤§)
            # å¾ Test ä¸­é¸å‡ºè¦æ›å…¥çš„è¡Œ (tweet_count æ‡‰è©²è¼ƒå°)
            # å¿…é ˆåŒ¹é… category å’Œ weekday_num
            
            # å°‡ Train/Test æ•¸æ“šæŒ‰ weekday_num åˆ†çµ„ï¼Œä»¥ä¾¿åœ¨åŒä¸€æ˜ŸæœŸå…§æ‰¾åˆ°åŒ¹é…å°è±¡
            train_k_grouped = train_k.groupby('weekday_num')
            test_k_grouped = test_k.groupby('weekday_num')
            
            best_swap_net_change = -np.inf # è¿½è¹¤æœ€å¤§çš„ net_change (è² å€¼ï¼Œå› ç‚ºæˆ‘å€‘è¦ä¸‹é™ Train ç¸½å’Œ)
            best_train_idx = None
            best_test_idx = None
            
            # éæ­·æ¯å€‹æ˜ŸæœŸ
            for weekday_num in range(7):
                if weekday_num in train_k_grouped.groups and weekday_num in test_k_grouped.groups:
                    train_week_data = train_k_grouped.get_group(weekday_num)
                    test_week_data = test_k_grouped.get_group(weekday_num)
                    
                    # å°‹æ‰¾ Train(å¤§) å’Œ Test(å°) çš„é…å°
                    # ç‚ºäº†ä½¿ Train ç¸½å’Œä¸‹é™æœ€å¤šï¼Œæˆ‘å€‘æ‡‰è©²æ‰¾ï¼š
                    # 1. Train ä¸­æœ€å¤§çš„ tweet_count (row_out)
                    # 2. Test ä¸­æœ€å°çš„ tweet_count (row_in)
                    
                    if not train_week_data.empty and not test_week_data.empty:
                    
                        if(iteration <= 100):
                            # æ‰¾åˆ° Train ä¸­æœ€å¤§çš„è¡Œ
                            train_row_out = train_week_data.loc[train_week_data['tweet_count'].idxmax()]
                            # æ‰¾åˆ° Test ä¸­æœ€å°çš„è¡Œ
                            test_row_in = test_week_data.loc[test_week_data['tweet_count'].idxmin()]
                        else:
                            try:
                                train_row_out = train_week_data.loc[train_week_data['tweet_count'].nlargest(2).index[1]]
                                test_row_in = test_week_data.loc[test_week_data['tweet_count'].nsmallest(2).index[1]]
                            except:
                                train_row_out = train_week_data.loc[train_week_data['tweet_count'].idxmax()]
                                test_row_in = test_week_data.loc[test_week_data['tweet_count'].idxmin()]
                        
                        
                        
                        # è¨ˆç®—äº¤æ›å¾Œçš„æ·¨è®ŠåŒ–ï¼š Net Change = tweet_count(in) - tweet_count(out)
                        net_change = test_row_in['tweet_count'] - train_row_out['tweet_count']
                        
                        # åªæœ‰åœ¨æ·¨è®ŠåŒ–ç‚ºè² å€¼ (Train ç¸½æ•¸ä¸‹é™) ä¸”æ¯”ç•¶å‰æœ€ä½³äº¤æ›æ›´å¥½æ™‚æ‰æ›´æ–°
                        if net_change < 0 and net_change > best_swap_net_change:
                            best_swap_net_change = net_change
                            best_train_idx = train_row_out.name
                            best_test_idx = test_row_in.name

            
            # åŸ·è¡Œäº¤æ›
            if best_train_idx is not None:
                # å–å¾—è¦äº¤æ›çš„å…©è¡Œ
                row_out = current_train_df.loc[best_train_idx].copy()
                row_in = current_test_df.loc[best_test_idx].copy()
                
                # äº¤æ›ï¼š
                current_train_df.loc[best_train_idx] = row_in
                current_test_df.loc[best_test_idx] = row_out

                print(f"  -> é¡åˆ¥ {category_id} (Train:{current_ratio:.4f}): äº¤æ› Train {row_out['date'].strftime('%Y-%m-%d')} ({row_out['tweet_count']}) å’Œ Test {row_in['date'].strftime('%Y-%m-%d')} ({row_in['tweet_count']})ã€‚ æ·¨è®Šå‹•: {best_swap_net_change}")

            else:
                print(f"  âŒ é¡åˆ¥ {category_id} Train éå¤šï¼Œä½†æ‰¾ä¸åˆ°åˆé©çš„äº¤æ›å°è±¡ (Train(å¤§) > Test(å°))ã€‚åœæ­¢ã€‚")
                break
                
        else: # ratio_diff < -tolerance
            # Test éå¤š (éœ€è¦ Train ç¸½å’Œä¸Šå‡)ã€‚å°‹æ‰¾ Train(å°) å’Œ Test(å¤§) é€²è¡Œäº¤æ›
            
            # é‚è¼¯èˆ‡ Train éå¤šç›¸å
            train_k_grouped = train_k.groupby('weekday_num')
            test_k_grouped = test_k.groupby('weekday_num')
            
            best_swap_net_change = np.inf # è¿½è¹¤æœ€å°çš„ net_change (æ­£å€¼ï¼Œå› ç‚ºæˆ‘å€‘è¦ä¸Šå‡ Train ç¸½å’Œ)
            best_train_idx = None
            best_test_idx = None
            
            for weekday_num in range(7):
                if weekday_num in train_k_grouped.groups and weekday_num in test_k_grouped.groups:
                    train_week_data = train_k_grouped.get_group(weekday_num)
                    test_week_data = test_k_grouped.get_group(weekday_num)
                    
                    if not train_week_data.empty and not test_week_data.empty:
                        # å°‹æ‰¾ Train(å°) å’Œ Test(å¤§) çš„é…å°
                        # 1. Train ä¸­æœ€å°çš„ tweet_count (row_out)
                        # 2. Test ä¸­æœ€å¤§çš„ tweet_count (row_in)
                        
                        train_row_out = train_week_data.loc[train_week_data['tweet_count'].idxmin()]
                        test_row_in = test_week_data.loc[test_week_data['tweet_count'].idxmax()]
                        
                        net_change = test_row_in['tweet_count'] - train_row_out['tweet_count']
                        
                        # åªæœ‰åœ¨æ·¨è®ŠåŒ–ç‚ºæ­£å€¼ (Train ç¸½æ•¸ä¸Šå‡) ä¸”æ¯”ç•¶å‰æœ€ä½³äº¤æ›æ›´å¥½æ™‚æ‰æ›´æ–°
                        if net_change > 0 and net_change < best_swap_net_change:
                            best_swap_net_change = net_change
                            best_train_idx = train_row_out.name
                            best_test_idx = test_row_in.name
            
            # åŸ·è¡Œäº¤æ›
            if best_train_idx is not None:
                row_out = current_train_df.loc[best_train_idx].copy()
                row_in = current_test_df.loc[best_test_idx].copy()
                
                # äº¤æ›ï¼š
                current_train_df.loc[best_train_idx] = row_in
                current_test_df.loc[best_test_idx] = row_out

                print(f"  <- é¡åˆ¥ {category_id} (Train:{current_ratio:.4f}): äº¤æ› Train {row_out['date'].strftime('%Y-%m-%d')} ({row_out['tweet_count']}) å’Œ Test {row_in['date'].strftime('%Y-%m-%d')} ({row_in['tweet_count']})ã€‚ æ·¨è®Šå‹•: {best_swap_net_change}")
            else:
                print(f"  âŒ é¡åˆ¥ {category_id} Test éå¤šï¼Œä½†æ‰¾ä¸åˆ°åˆé©çš„äº¤æ›å°è±¡ (Test(å¤§) > Train(å°))ã€‚åœæ­¢ã€‚")
                break
                
    # å¦‚æœé”åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•¸ï¼Œä¹Ÿåœæ­¢
    if iteration == MAX_ITER:
        print(f"  âš ï¸ é¡åˆ¥ {category_id} é”åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•¸ {MAX_ITER}ï¼Œå¯èƒ½ç„¡æ³•æ”¶æ–‚ã€‚")


    # éšæ®µæ€§æª¢æŸ¥
    print("\n======== éšæ®µæ€§æª¢æŸ¥ ========")
    # é‡æ–°è¨ˆç®—ä¸¦è¼¸å‡ºçµæœ
    final_train_sum = current_train_df.groupby('category')['tweet_count'].sum()
    final_test_sum = current_test_df.groupby('category')['tweet_count'].sum()
    final_combined_sum_df = pd.DataFrame({'train_sum': final_train_sum, 'test_sum': final_test_sum}).fillna(0)
    final_combined_sum_df['total_sum'] = final_combined_sum_df['train_sum'] + final_combined_sum_df['test_sum']
    final_combined_sum_df['train_ratio'] = final_combined_sum_df['train_sum'] / final_combined_sum_df['total_sum']

    print(final_combined_sum_df[['train_sum', 'total_sum', 'train_ratio']])

    print()
    print("len(current_train_df):", len(current_train_df))
    print("len(current_test_df):", len(current_test_df))
    print("current_train_df['tweet_count'].sum():", current_train_df['tweet_count'].sum())
    print("current_test_df['tweet_count'].sum():", current_test_df['tweet_count'].sum())

    return current_train_df, current_test_df



def split_by_week(df):
    current_date_pointer = 0  # ç´€éŒ„ç›®å‰æ˜¯æ­£åœ¨çœ‹å“ªä¸€å¤©
    next_date_pointer = 0  # è¨˜éŒ„ä¸‹ä¸€å€‹ pointer è¦æŒ‡çš„ä½ç½® (current_date_pointer += next_date_pointer)
    run_count = 1  # ç´€éŒ„ç›®å‰è·‘åˆ°ç¬¬å¹¾æ¬¡ ( 4 æ¬¡è¦çµ¦ Trainï¼Œ 1 æ¬¡è¦çµ¦ Test)

    current_all_week_date = []  # æ”¾ç•¶å‘¨å…¨éƒ¨çš„æ—¥æœŸ
    train_list, test_list = [], []  # æ”¾æœ€çµ‚è¦å›å‚³çš„ dfï¼Œè£¡é¢å­˜ trainã€test å¯¦éš›çš„è³‡æ–™

    # å…ˆæŠŠ 'date' æ¬„ä½è½‰æˆ datetime æ ¼å¼
    df['date'] = pd.to_datetime(df['date'], format="%Y-%m-%d")
    # print(df.info())
    # print(df)

    # ç•¶å¦‚æœ df è£¡é¢æ²’æœ‰è³‡æ–™çš„æ™‚å€™è·³å‡ºè¿´åœˆ
    while(current_date_pointer < len(df)):
        start_week = df.iloc[current_date_pointer]['weekday_num']
        start_date = pd.to_datetime(df.iloc[current_date_pointer]['date'], format="%Y-%m-%d")
        # print("start_date:", start_date)

        if start_week != 6:  # å¦‚æœç¬¬ä¸€å¤©ä¸æ˜¯ æ˜ŸæœŸæ—¥(6) å°±æ‰¾å‡ºç•¶å‘¨çš„æ˜ŸæœŸæ—¥æ˜¯å¹¾è™Ÿ
            current_date = pd.to_datetime(df.iloc[current_date_pointer]['date'], format="%Y-%m-%d")
            start_date = current_date - pd.Timedelta(days = (start_week + 1))  # å¦‚æœæ˜¯ æ˜ŸæœŸä¸€(0) ç•¶å‘¨èµ·å§‹æ—¥æœŸå°± -1ï¼Œä¾æ­¤é¡æ¨

        for i in range(7):  # æŠŠç•¶å‘¨å…¶ä»–åŸæœ¬çš„æ—¥æœŸæ‰¾å‡ºä¾† ä¸¦ä¸” åªç•™çœŸæ­£æœ‰è³‡æ–™çš„æ—¥æœŸ
            current_date = start_date + pd.Timedelta(days = i)
            # print(current_date)
            # print(df['date'])
            if current_date in df['date'].values:
                current_all_week_date.append(start_date + pd.Timedelta(days = i))
                next_date_pointer += 1
        # print(current_all_week_date)


        if current_all_week_date: # ç¢ºä¿æœ‰è³‡æ–™æ‰è™•ç†
            # ç¯©é¸æ­¥é©Ÿï¼šä½¿ç”¨ isin() é€²è¡Œé«˜æ•ˆç¯©é¸
            filtered_df = df[df['date'].isin(current_all_week_date)].copy()
            
            if run_count % 5 != 0:
                # é€™æ˜¯ Train Data
                train_list.append(filtered_df)
                # print(f"ç¬¬ {run_count} é€±ï¼šåŠ å…¥ Train Dataï¼Œå…± {len(filtered_df)} ç­†è³‡æ–™")
                # print(train_list)
            else:
                # é€™æ˜¯ Test Data (æ¯ 5 æ¬¡ä¸€æ¬¡)
                test_list.append(filtered_df)
                # print(f"ç¬¬ {run_count} é€±ï¼šåŠ å…¥ Test Dataï¼Œå…± {len(filtered_df)} ç­†è³‡æ–™")
        # input("pause...")

        current_all_week_date = []
        current_date_pointer += next_date_pointer
        next_date_pointer = 0
        run_count += 1
    

    # å°‡ train_list, test_list è½‰æˆ Dataframe
    train_df = pd.concat(train_list, ignore_index=True)
    test_df = pd.concat(test_list, ignore_index=True)
    print("train_df:\n", train_df)
    print("test_df:\n", test_df)
    print("\ntrain_df['tweet_count'].sum():", train_df['tweet_count'].sum())
    print("test_df['tweet_count'].sum():", test_df['tweet_count'].sum())
    print()


    # è¨ˆç®—è¨“ç·´é›† (train_df) ä¸­æ¯å€‹ category çš„ç¸½ tweet_count
    train_category_sum = train_df.groupby('category')['tweet_count'].sum()
    test_category_sum = test_df.groupby('category')['tweet_count'].sum()

    # å°‡å…©å€‹ Series åˆä½µåˆ°ä¸€å€‹ DataFrameï¼Œä¸¦è™•ç†å¯èƒ½ä¸é‡ç–Šçš„é¡åˆ¥ (é›–ç„¶åœ¨æ‚¨çš„ä¾‹å­ä¸­æ˜¯é‡ç–Šçš„)
    combined_sum_df = pd.DataFrame({
        'train_sum': train_category_sum,
        'test_sum': test_category_sum
    }).fillna(0) # å¦‚æœæŸå€‹é¡åˆ¥åªå‡ºç¾åœ¨å…¶ä¸­ä¸€å€‹é›†åˆï¼Œç”¨ 0 å¡«å……

    # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„ç¸½å’Œ (Train + Test)
    combined_sum_df['total_sum'] = combined_sum_df['train_sum'] + combined_sum_df['test_sum']

    # Train çš„æ¯”ä¾‹ï¼š train_sum / total_sum
    combined_sum_df['train_ratio'] = combined_sum_df['train_sum'] / combined_sum_df['total_sum']
    combined_sum_df['test_ratio'] = combined_sum_df['test_sum'] / combined_sum_df['total_sum']

    def format_output(row, mode='train'):
        """æ ¹æ“š Train æˆ– Test æ ¼å¼åŒ–è¼¸å‡ºå­—ä¸²"""
        count = row[f'{mode}_sum']
        ratio = row[f'{mode}_ratio']
        total = row['total_sum']
        
        # ä½¿ç”¨ f-string é€²è¡Œæ ¼å¼åŒ–ï¼Œä¿ç•™è¼ƒå¤šå°æ•¸ä½æ•¸ä»¥ç¬¦åˆæ‚¨çš„ç¯„ä¾‹
        return f"{int(count)} ({ratio} = {int(count)} / {int(total)} )"

    print("--- è¨“ç·´é›† (Train) æ¯å€‹é¡åˆ¥çš„ç¸½æ¨æ–‡æ•¸ ---")
    # å° train æ¬„ä½æ‡‰ç”¨æ ¼å¼åŒ–å‡½æ•¸
    train_formatted_output = combined_sum_df.apply(lambda row: format_output(row, mode='train'), axis=1)
    print(train_formatted_output.to_string(header=False))

    print("\n--- æ¸¬è©¦é›† (Test) æ¯å€‹é¡åˆ¥çš„ç¸½æ¨æ–‡æ•¸ ---")
    # å° test æ¬„ä½æ‡‰ç”¨æ ¼å¼åŒ–å‡½æ•¸
    test_formatted_output = combined_sum_df.apply(lambda row: format_output(row, mode='test'), axis=1)
    print(test_formatted_output.to_string(header=False))

    return train_df, test_df



# --- ç”¨æ—¥æœŸç‚ºå–®ä½æŠŠè³‡æ–™åˆ‡æˆ 8:2 (train : test) ---
def splitset_dates(COIN_SHORT_NAME):

    # --- è®€å–æ¯æ¢æ¨æ–‡çš„ ID .pkl ---
    with open(f"{INPUT_PATH}/ids_input/{COIN_SHORT_NAME}/{COIN_SHORT_NAME}_ids{SUFFIX}.pkl", "rb") as f:   # rb = read binary
        ids = pickle.load(f)  # array[('coin', 'date', 'no.'), (str, '%Y-%m-%d', int)]
    dates = np.array([row[1] for row in ids])  # åªæŠŠ 'date' å–å‡ºä¾†ï¼Œä¸¦è½‰æˆ np.array

    # --- è®€å–æ¯æ¢æ¨æ–‡çš„ åƒ¹æ ¼è®ŠåŒ–ç‡ ---
    price_diff_rate = np.load(f"{INPUT_PATH}/y_input/{COIN_SHORT_NAME}/{COIN_SHORT_NAME}_price_diff_original{SUFFIX}.npy")
    price_diff_categories = categorize_array_multi(price_diff_rate)
    
    # å¦‚æœæ˜¯ bytesï¼Œè¦è½‰æˆ str
    if isinstance(dates[0], bytes):
        dates = dates.astype(str)

    # è½‰æˆ datetime æ–¹ä¾¿æ’åº
    dates_dt = pd.to_datetime(dates, format="%Y-%m-%d")

    # çµ±è¨ˆæ¯å¤©å‡ºç¾æ¬¡æ•¸
    unique_dates, counts = np.unique(dates_dt, return_counts=True)

    if len(price_diff_rate) != len(unique_dates):
        print("âš ï¸ è­¦å‘Šï¼šåƒ¹æ ¼è®ŠåŒ–ç‡é™£åˆ—é•·åº¦èˆ‡å”¯ä¸€æ—¥æœŸæ•¸é‡ä¸åŒ¹é…ã€‚è«‹æª¢æŸ¥ price_diff_rate çš„ç”Ÿæˆé‚è¼¯ã€‚")
        print(f" price_diff_rate é•·åº¦: {len(price_diff_rate)}")
        print(f" unique_dates é•·åº¦: {len(unique_dates)}")
        raise ValueError(f"åƒ¹æ ¼è®ŠåŒ–ç‡é™£åˆ—é•·åº¦èˆ‡å”¯ä¸€æ—¥æœŸæ•¸é‡ä¸åŒ¹é…")

    # å»ºç«‹ DataFrame
    df = pd.DataFrame({
        "date": unique_dates,
        "tweet_count": counts,
        "price_diff_rate": price_diff_rate,
        "category": price_diff_categories
    })

    # åŠ ä¸Š weekday
    df["weekday_name"] = df["date"].dt.day_name()   # e.g., Friday
    df["weekday_num"] = df["date"].dt.weekday       # Monday=0, Sunday=6

    # å„²å­˜ç‚º CSV
    csv_output_path = f"{INPUT_PATH}/y_input/{COIN_SHORT_NAME}"
    os.makedirs(csv_output_path, exist_ok=True)
    df.to_csv(f"{csv_output_path}/{COIN_SHORT_NAME}_confirmed_tweet_count{SUFFIX}.csv", index=False, encoding="utf-8-sig")

    print(f"âœ… å·²å„²å­˜ CSV è‡³: {csv_output_path}/{COIN_SHORT_NAME}_confirmed_tweet_count{SUFFIX}.csv")
    # input("pause...")


    # --- åˆ‡åˆ† ---
    train_df, test_df = split_by_week(df)
    print("âœ… å·²åˆ©ç”¨é€±ç‚ºå–®ä½åˆ‡å‰²å¥½è³‡æ–™é›†ï¼Œæº–å‚™é€²è¡Œå¾®èª¿...")
    # input("pause...")

    # --- éš¨æ©Ÿäº¤æ›å¾®èª¿ tolerance: èª¤å·®æ¨æ–‡æ•¸å€¼ (100 => +-100) ---
    all_categories = train_df['category'].unique()  # æ‰¾å‡ºæ‰€æœ‰é¡åˆ¥

    # è¿­ä»£èª¿æ•´æ¯å€‹é¡åˆ¥
    for cat_id in sorted(all_categories):
        print(f"\n======== é–‹å§‹èª¿æ•´é¡åˆ¥ {cat_id} ========")
        train_df, test_df = balance_category_proportion_swap(train_df, test_df, cat_id, target_ratio=0.8, tolerance=0.001)

    print("âœ… å·²æˆåŠŸå®Œæˆå¾®èª¿")
    # input("pause...")


    # å…ˆæŒ‰ç…§ tweet_count ç”±å¤§åˆ°å°æ’åº
    train_df_sorted = train_df.sort_values(by='tweet_count', ascending=False)
    test_df_sorted  = test_df.sort_values(by='tweet_count', ascending=False)

    csv_output_path = f"{INPUT_PATH}/split_dates"
    os.makedirs(csv_output_path, exist_ok=True)
    train_df_sorted.to_csv(f"{csv_output_path}/{COIN_SHORT_NAME}_train_dates{SUFFIX}.csv", index=False, encoding="utf-8-sig")
    test_df_sorted.to_csv(f"{csv_output_path}/{COIN_SHORT_NAME}_test_dates{SUFFIX}.csv", index=False, encoding="utf-8-sig")
    print("âœ… å·²æˆåŠŸå„²å­˜è³‡æ–™é›†çš„æ—¥æœŸ")
    input("pause...")

    # --- å±•é–‹æˆæ¯æ¢æ¨æ–‡ä¸€å€‹å–®ä½ ---
    dates_train_expanded = expand_by_tweet(train_df)
    dates_test_expanded = expand_by_tweet(test_df)

    return dates_train_expanded, dates_test_expanded


# --- ç”¨å¹³è¡¡å¥½çš„çµæœä¾†æŒ‰æ—¥æœŸåˆ‡å‰² X, Yï¼Œä¸¦å¯é¸æ“‡æ˜¯å¦è¦å†åˆ†å‡º val (é è¨­ False) ---
def splitset_XY(COIN_SHORT_NAME):

    # è®€å–ç¨€ç–çŸ©é™£
    X = sparse.load_npz(f"{INPUT_PATH}/X_input/keyword_classifier/{COIN_SHORT_NAME}/{COIN_SHORT_NAME}_X_sparse{SUFFIX}.npz")  # äºŒç¶­é™£åˆ—ï¼šcolunm(é—œéµè©) row(æŸå¤©æŸæ¨æ–‡) (ä½†é€™è£¡æ˜¯ç¨€ç–çŸ©é™£çš„æ ¼å¼)
    print("X.shape[0]:", X.shape[0])
    print("X.shape[1]:", X.shape[1])
    input("pause...")

    # ä¸€ç¶­é™£åˆ—ï¼šå­˜æ”¾èˆ‡ X row å°æ‡‰çš„ ID
    with open(f"{INPUT_PATH}/ids_input/{COIN_SHORT_NAME}/{COIN_SHORT_NAME}_ids{SUFFIX}.pkl", "rb") as f:   # rb = read binary
        ids = pickle.load(f)  # array[('coin', 'date', 'no.'), (str, '%Y-%m-%d', int)
    ids = np.array(ids)  # æŠŠ ids è½‰æˆ numpy array
    dates = np.array([row[1] for row in ids])  # åªæŠŠ 'date' å–å‡ºä¾†ï¼Œä¸¦è½‰æˆ np.array

    # è®€å–ä¸‰å€‹é›†åˆçš„æ—¥æœŸ (åˆ‡å‰²å¥½çš„ CSV) 
    train_dates = pd.read_csv(f"{INPUT_PATH}/split_dates/{COIN_SHORT_NAME}_train_dates{SUFFIX}.csv")['date']
    test_dates = pd.read_csv(f"{INPUT_PATH}/split_dates/{COIN_SHORT_NAME}_test_dates{SUFFIX}.csv")['date']

    # è®€å– price_diff.npy
    Y_all = np.load(f"{INPUT_PATH}/y_input/{COIN_SHORT_NAME}/{COIN_SHORT_NAME}_price_diff{SUFFIX}.npy")  # shape = (ç¸½æ¨æ–‡æ•¸, 1)


    # è¼¸å‡ºé•·åº¦ ç¢ºä¿ä¸€è‡´æ€§
    print(f"{COIN_SHORT_NAME}{SUFFIX}ï¼š")
    print("X.shape[0] =", X.shape[0])
    print("ids.shape[0] =", len(ids))
    print("Y.shape[0] =", Y_all.shape[0],"\n")

    print(f"{COIN_SHORT_NAME}{SUFFIX} çš„ Y_all æœ‰ {np.sum(Y_all == 0)} å€‹ 0")



    # æŠŠ date è½‰æˆ datetime æ ¼å¼ï¼Œæ–¹ä¾¿æ¯”å°
    dates_datetime = pd.to_datetime(dates)

    print(dates_datetime)

    # è½‰æˆ datetime
    train_dates = pd.to_datetime(train_dates)
    test_dates  = pd.to_datetime(test_dates)

    print(train_dates)

    # æ‰¾å‡º index   é€ç­†æª¢æŸ¥ date ä¸­çš„æ¯ä¸€å€‹å€¼ï¼Œåˆ¤æ–·å®ƒæ˜¯å¦åœ¨ train_dates è£¡
    train_mask = dates_datetime.isin(train_dates)
    test_mask  = dates_datetime.isin(test_dates)

    # åˆ‡å‰² X
    X_train = X[train_mask, :]
    X_test  = X[test_mask, :]

    # åˆ‡å‰² Y
    Y_train = Y_all[train_mask]  # shape = (len(train_mask), 1)
    Y_test  = Y_all[test_mask]

    # åˆ‡å‰² ids
    ids_train = ids[train_mask]
    ids_test  = ids[test_mask]

    print(f"{COIN_SHORT_NAME}{SUFFIX} çš„ Y_train æœ‰ {np.sum(Y_train == 0)} å€‹ 0")
    print(f"{COIN_SHORT_NAME}{SUFFIX} çš„ Y_test æœ‰ {np.sum(Y_test == 0)} å€‹ 0")

    # åˆ—å°å„å€‹å¹£ç¨® split å¾Œ æ¯å€‹è³‡æ–™é›†çš„æ¼²è·Œæ¯”ä¾‹
    # print(Y_train.shape)
    # print(Y_test.shape)
    print(ids_train.shape)
    print(ids_test.shape)
    # for i in range(LABEL_COUNT):
    #     print(f"ç¬¬ {i} çµ„ ï¼¹")
    print_label_distribution(Y_train, Y_test, COIN_SHORT_NAME)

    return X_train, X_test, Y_train, Y_test, ids_train, ids_test



# --- å°‡ä¸å¿…è¦çš„é—œéµè©èˆ‡æ¨æ–‡åˆªé™¤ ---
def filter_XY(X_train, X_test, Y_train, Y_test, ids_train, ids_test, all_vocab):
    '''é‡è¤‡ åŠŸèƒ½ 1, åŠŸèƒ½ 2, åŠŸèƒ½ 3 ç›´åˆ°æ²’æœ‰æ±è¥¿å¯ä»¥åˆªç‚ºæ­¢'''

    # å®šç¾© åŠŸèƒ½ 1
    def function_1(X, Y, ids):
        row_sums = np.array(X.sum(axis=1)).ravel()
        valid_rows = np.where(row_sums > 0)[0]
        invalid_rows = X.shape[0] - len(valid_rows)

        X = X[valid_rows, :]  # ä¹Ÿå¯å¯« X = X[valid_rows]
        Y = Y[valid_rows]
        ids = ids[valid_rows]
        
        return X, Y, ids, invalid_rows, len(valid_rows)
            

    invalid_rows = -1
    delete_only_test = -1

    total_delete_rows = 0
    total_delete_columns = 0

    # è‹¥é‚„æœ‰åŠŸèƒ½æ˜¯å¯ä»¥åˆªè³‡æ–™çš„ï¼Œå°±å†ç¹¼çºŒè·‘
    while invalid_rows != 0 or delete_only_test != 0:

        # --- åŠŸèƒ½ 1: åˆªæ‰æ²’æœ‰ä»»ä½•é—œéµè©çš„æ¨æ–‡ (åˆª row) ---
        # train
        X_train, Y_train, ids_train, train_invalid_rows, train_valid_rows = function_1(X_train, Y_train, ids_train)
        
        # test
        X_test, Y_test, ids_test, test_invalid_rows, test_valid_rows = function_1(X_test, Y_test, ids_test)

        # è¨ˆç®—ä¿ç•™ã€åˆªæ‰çš„ç­†æ•¸
        valid_rows = train_valid_rows + test_valid_rows
        invalid_rows = train_invalid_rows + test_invalid_rows
            
        # åªçœ‹ Train çš„æ•¸é‡
        total_delete_rows += train_invalid_rows
        print("åŠŸèƒ½ 1: åˆªæ‰æ²’æœ‰ä»»ä½•é—œéµè©çš„æ¨æ–‡ (row):")
        print(f"\tTrain ä¿ç•™ row æ•¸é‡: {train_valid_rows}")
        print(f"\tTrain åˆªæ‰ row æ•¸é‡: {train_invalid_rows}\n")


        # --- åŠŸèƒ½ 2: åªä¿ç•™ train å‡ºç¾éçš„é—œéµè© (åˆª column) --- 
        '''
        X_train.nonzero() æœƒå›å‚³ä¸€å€‹ tuple (row_idx, col_idx)ï¼š
        row_idx â†’ éé›¶å…ƒç´ æ‰€åœ¨çš„ rowï¼ˆæ¨æ–‡ indexï¼‰ã€‚
        col_idx â†’ éé›¶å…ƒç´ æ‰€åœ¨çš„ columnï¼ˆé—œéµè© indexï¼‰ã€‚

        X_train.nonzero()[1] å–çš„æ˜¯æ‰€æœ‰éé›¶å€¼çš„ column indexã€‚
        â†’ é€™å°±æ˜¯ã€Œæœ‰å“ªäº›é—œéµè©è‡³å°‘åœ¨ train è£¡å‡ºç¾éä¸€æ¬¡ã€ã€‚

        np.unique(...) æŠŠå®ƒå»é‡ï¼Œå¾—åˆ°ä¸€å€‹ åªå‡ºç¾åœ¨ train çš„ column æ¸…å–®ã€‚
        '''

        orig_cols = X_train.shape[1]
        keep_cols = np.unique(X_train.nonzero()[1])  # train å‡ºç¾éçš„ column index
        new_cols = len(keep_cols)

        # æ¯å€‹é—œéµè©çš„å‡ºç¾æ¬¡æ•¸çµ±è¨ˆ
        col_sums = np.array(X_train.sum(axis=0)).ravel()
        print("len(all_vocab) =", len(all_vocab))
        print("col_sums.shape =", col_sums.shape)
        keyword_counts = {all_vocab[i]: int(col_sums[i]) for i in range(len(all_vocab))}
        stats_output_path = os.path.join(INPUT_PATH, "X_input", "keyword_classifier", f"keyword_counts{SUFFIX}.json")
        with open(stats_output_path, "w", encoding="utf-8") as f:
            json.dump(keyword_counts, f, ensure_ascii=False, indent=4)

        # column éæ¿¾
        X_train = X_train[:, keep_cols]  # [:, keep_cols] è¡¨ç¤ºã€Œä¿ç•™æ‰€æœ‰ rowï¼Œä½†åªå–å‡º keep_cols é€™äº› columnã€ã€‚
        X_test  = X_test[:, keep_cols]
        if all_vocab is not None:
            all_vocab = [all_vocab[i] for i in keep_cols]

        delete_only_test = orig_cols - new_cols
        total_delete_columns += delete_only_test
        print("åŠŸèƒ½ 2: åªä¿ç•™ train å‡ºç¾éçš„é—œéµè© (column):")
        print(f"\tåŸå§‹ column æ•¸é‡: {orig_cols}")
        print(f"\tä¿ç•™ column æ•¸é‡: {new_cols}")
        print(f"\tåˆªæ‰ column æ•¸é‡: {delete_only_test}\n")

        with open(os.path.join(INPUT_PATH, "X_input", "keyword_classifier", f"filtered_keywords{SUFFIX}.json"), "w", encoding="utf-8") as f:
            json.dump(all_vocab, f, ensure_ascii=False, indent=4)  

    print(f"ç¸½å…±åˆªé™¤ {total_delete_rows} å€‹æ¨æ–‡ (row), {total_delete_columns} å€‹é—œéµè© (column)")
    print(f"Train ç¸½å…±ä¿ç•™ {X_train.shape[0]} å€‹æ¨æ–‡ (row), {X_train.shape[1]} å€‹é—œéµè© (column)\n")
    print(f"âœ… å·²è¼¸å‡ºæ‰€æœ‰é—œéµè©å‡ºç¾æ¬¡æ•¸çµ±è¨ˆåˆ° {stats_output_path}")
    print(f"âœ… å·²è¼¸å‡ºæ‰€æœ‰è¢«éæ¿¾çš„é—œéµè©åˆ° {INPUT_PATH}/X_input/keyword_classifier\n")

    return X_train, X_test, Y_train, Y_test, ids_train, ids_test



# --- æ‰“äº‚é †åº (shuffle) ---
def shuffle_XY(X, Y, ids, seed=42):
    """
    Shuffle X and Y in unison.
    X: np.ndarray æˆ– scipy.sparse çŸ©é™£
    Y: np.ndarray ä¸€ç¶­æ¨™ç±¤
    seed: éš¨æ©Ÿç¨®å­
    """
    rng = np.random.default_rng(seed)
    indices = np.arange(Y.shape[0])  # å–å¾—æ¨£æœ¬æ•¸ indices = [0, 1, 2, ... , len(X)-1]
    rng.shuffle(indices)  # æŠŠ indices éš¨æ©Ÿé‡æ–°æ’åº

    X_shuffled = X[indices, :]  # æŒ‰ç…§ indices çš„é †åºé‡æ–°æ’åˆ—
    Y_shuffled = Y[indices]
    ids_shuffled = ids[indices]

    return X_shuffled, Y_shuffled, ids_shuffled



# --- è¨ˆç®—æ¯å€‹æ—¥æœŸç¸½å…±çš„æ•¸é‡ (ç”¨ä»¥ç¢ºèªæ˜¯å¦æœ‰åˆ‡æ­£ç¢º) ---
def count_per_day(ids, dataset_name):
    """
    dates: array-like, æ¯æ¢æ¨æ–‡çš„æ—¥æœŸ (str æˆ– np.datetime64)
    dataset_name: ç”¨æ–¼æ‰“å°
    """
    dates = [row[1] for row in ids]
    # å¦‚æœæ˜¯ bytesï¼Œå…ˆè½‰æˆ str
    if isinstance(dates[0], bytes):
        dates = dates.astype(str)

    # è½‰æˆ datetime
    dates_dt = pd.to_datetime(dates)

    # è¨ˆç®—æ¯å¤©å‡ºç¾æ¬¡æ•¸
    date_counts = dates_dt.value_counts().sort_index()  # æŒ‰æ—¥æœŸæ’åº
    df_counts = pd.DataFrame({"date": date_counts.index, "tweet_count": date_counts.values})

    df_counts.to_csv(f"{OUTPUT_PATH}/dates_{dataset_name}_counts{SUFFIX}.csv", index=False)

    return df_counts



def update_single_coin_dataset(X_train, X_test, Y_train, Y_test, ids_train, ids_test):
    print("\nğŸš© é–‹å§‹å°‡éæ¿¾å¾Œçš„è³‡æ–™è¦†å¯«å›åŸå§‹å„å¹£ç¨®çš„ X, Y, ids\n")

    for coin_short_name in ["DOGE", "PEPE", "TRUMP"]:
        # è®€å– X, Y
        X = sparse.load_npz(f"{INPUT_PATH}/X_input/keyword_classifier/{coin_short_name}/{coin_short_name}_X_sparse{SUFFIX}.npz")
        price_diff = np.load(f"{INPUT_PATH}/y_input/{coin_short_name}/{coin_short_name}_price_diff{SUFFIX}.npy")
        price_diff_original = np.load(f"{INPUT_PATH}/y_input/{coin_short_name}/{coin_short_name}_price_diff_original{SUFFIX}.npy")
        price_diff_past5days = np.load(f"{INPUT_PATH}/y_input/{coin_short_name}/{coin_short_name}_price_diff_past5days{SUFFIX}.npy")

        # è®€å– ids
        with open(f"{INPUT_PATH}/ids_input/{coin_short_name}/{coin_short_name}_ids{SUFFIX}.pkl", "rb") as f:   # rb = read binary
            ids = pickle.load(f)  # array[('coin', 'date', 'no.'), (str, '%Y-%m-%d', int)
        ids = np.array(ids)  # æŠŠ ids è½‰æˆ numpy array
        print()
        print(ids[:10])

        print("\næ›´æ–°å‰ï¼š")
        print(f"{coin_short_name} X.shape[0]:", X.shape[0])
        print(f"{coin_short_name} price_diff.shape[0]:", price_diff.shape[0])
        print(f"{coin_short_name} price_diff_original.shape[0]:", price_diff_original.shape[0])
        print(f"{coin_short_name} price_diff_past5days.shape[0]:", price_diff_past5days.shape[0])
        print(f"{coin_short_name} len(ids):", len(ids))


        # -------- é–‹å§‹æ›´æ–° --------
        # å°‡æ–°çš„è³‡æ–™é›† train, test åˆä½µ
        X_new = sparse.vstack([X_train, X_test], format="csr")
        # Y_new = np.concatenate([Y_train, Y_test])
        ids_new = np.concatenate([ids_train, ids_test])

        # åªæŠŠç•¶å‰çš„ coin è³‡æ–™å–å‡ºä¾†
        mask = [c == coin_short_name for (c, d, no) in ids_new]
        # X_new = X_new[mask]
        # Y_new = Y_new[mask]
        ids_new_test = ids_new[mask]
        print(f"æ–°çš„ {coin_short_name} ids:\n", ids_new_test[:10])
        ids_new_set = {(c, d, no) for (c, d, no) in ids_new if c == coin_short_name}
        # print(f"æ–°çš„ {coin_short_name} ids:\n", np.array(sorted(list(ids_new_set), key=lambda x: (x[1], int(x[2]))), dtype=str)[:10])

        # éæ¿¾ X 
        # X_mask = [(c, d, no) in ids_new_set for (c, d, no) in ids]
        # X = X_new[X_mask]
        X = X_new[mask]

        # éæ¿¾ Y
        price_diff_mask = [(c, d, no) in ids_new_set for (c, d, no) in ids]
        ids_dates = sorted({d for (c, d, no) in ids})  # è½‰æˆåªæœ‰æ—¥æœŸçš„é›†åˆ
        print(np.array(ids_dates, dtype=str)[:10])
        ids_dates_new = {d for (c, d, no) in ids_new_set}
        price_diff_original_mask = np.array([d in ids_dates_new for d in ids_dates])
        price_diff_past5days_mask = np.array([d in ids_dates_new for d in ids_dates[(len(price_diff_original) - len(price_diff_past5days)):]])
        print("len(price_diff_original), len(price_diff_past5days):",len(price_diff_original), len(price_diff_past5days))

        price_diff = price_diff[price_diff_mask]
        price_diff_original = price_diff_original[price_diff_original_mask]
        price_diff_past5days = price_diff_past5days[price_diff_past5days_mask]
        
        # éæ¿¾ ids
        # ids_mask = [(c, d, no) in ids_new_set for (c, d, no) in ids]
        # ids = ids_new[ids_mask]
        ids = ids_new[mask]
        # -------- æ›´æ–°çµæŸ -----------

        print("æ›´æ–°å¾Œï¼š")
        print(f"{coin_short_name} X.shape[0]:", X.shape[0])
        print(f"{coin_short_name} price_diff.shape[0]:", price_diff.shape[0])
        print(f"{coin_short_name} price_diff_original.shape[0]:", price_diff_original.shape[0])
        print(f"{coin_short_name} price_diff_past5days.shape[0]:", price_diff_past5days.shape[0])
        print(f"{coin_short_name} len(ids):", len(ids))
        print()
        print(ids[:10])


        # å°‡æ›´æ–°å¥½çš„è¦†è“‹å›åŸæœ¬çš„æª”æ¡ˆ
        sparse.save_npz(f"{INPUT_PATH}/X_input/keyword_classifier/{coin_short_name}/{coin_short_name}_X_sparse{SUFFIX}.npz", X)
        np.save(f"{INPUT_PATH}/y_input/{coin_short_name}/{coin_short_name}_price_diff_original{SUFFIX}.npy", price_diff_original)
        np.save(f"{INPUT_PATH}/y_input/{coin_short_name}/{coin_short_name}_price_diff{SUFFIX}.npy", price_diff)
        np.save(f"{INPUT_PATH}/y_input/{coin_short_name}/{coin_short_name}_price_diff_past5days{SUFFIX}.npy", price_diff_past5days)

        with open(f"{INPUT_PATH}/ids_input/{coin_short_name}/{coin_short_name}_ids{SUFFIX}.pkl", 'wb') as file:
            pickle.dump(ids.tolist(), file)

    print("\nâœ… å·²è¦†å¯«å®Œæˆ\n")

    return



# --- å°‡ä¸‰ç¨®å¹£ç¨®çš„ X, Y åˆä½µæˆå®Œæ•´çš„æ¨¡å‹è¼¸å…¥å€¼ (è¼¸å‡º .npy æª”) ---
def merge(DOGE_X_train, DOGE_X_test, DOGE_Y_train, DOGE_Y_test,
          PEPE_X_train, PEPE_X_test, PEPE_Y_train, PEPE_Y_test,
          TRUMP_X_train, TRUMP_X_test, TRUMP_Y_train, TRUMP_Y_test,
          DOGE_ids_train, DOGE_ids_test,
          PEPE_ids_train, PEPE_ids_test,
          TRUMP_ids_train, TRUMP_ids_test,
          all_vocab):

    # åˆä½µ Xï¼ˆç¨€ç–çŸ©é™£ç”¨ sparse.vstackï¼‰
    X_train_list = [DOGE_X_train, PEPE_X_train, TRUMP_X_train]
    X_test_list  = [DOGE_X_test, PEPE_X_test, TRUMP_X_test]

    print("DOGE_X_train.shape[0]:", DOGE_X_train.shape[0])
    print("DOGE_X_train.shape[1]:", DOGE_X_train.shape[1])
    print("PEPE_X_train.shape[0]:", PEPE_X_train.shape[0])
    print("PEPE_X_train.shape[1]:", PEPE_X_train.shape[1])
    print("TRUMP_X_train.shape[0]:", TRUMP_X_train.shape[0])
    print("TRUMP_X_train.shape[1]:", TRUMP_X_train.shape[1])
    # print("DOGE_X_train.shape():", DOGE_X_train.shape())
    # print("PEPE_X_train.shape():", PEPE_X_train.shape())
    # print("TRUMP_X_train.shape():", TRUMP_X_train.shape())

    X_train = sparse.vstack(X_train_list, format="csr")  # np.vstack = vertical stackï¼ŒæŠŠå¤šå€‹çŸ©é™£åœ¨ã€Œåˆ—æ–¹å‘ã€å †ç–Šèµ·ä¾†
    X_test  = sparse.vstack(X_test_list, format="csr")

    # åˆä½µ Y
    Y_train = np.concatenate([DOGE_Y_train, PEPE_Y_train, TRUMP_Y_train])  # np.concatenate = æŠŠå¤šå€‹ä¸€ç¶­é™£åˆ—ä¸²æ¥èµ·ä¾†
    Y_test  = np.concatenate([DOGE_Y_test, PEPE_Y_test, TRUMP_Y_test])

    # åˆä½µ ids
    ids_train = np.concatenate([DOGE_ids_train, PEPE_ids_train, TRUMP_ids_train])
    ids_test  = np.concatenate([DOGE_ids_test, PEPE_ids_test, TRUMP_ids_test])


    # å°‡ä¸å¿…è¦çš„é—œéµè©èˆ‡æ¨æ–‡åˆªé™¤
    X_train, X_test, Y_train, Y_test, ids_train, ids_test = filter_XY(X_train, X_test, Y_train, Y_test, ids_train, ids_test, all_vocab)
    # print(ids_train.shape)

    # æ›´æ–°æ¯å€‹å¹£ç¨®çš„ X, Y(price_diff_original, price_diff, price_diff_past5days), ids
    update_single_coin_dataset(X_train, X_test, Y_train, Y_test, ids_train, ids_test)

    # æ‰“äº‚é †åº
    X_train, Y_train, ids_train = shuffle_XY(X_train, Y_train, ids_train)
    X_test,  Y_test,  ids_test  = shuffle_XY(X_test, Y_test, ids_test)

    # æª¢æŸ¥è³‡æ–™é›†ç¶­åº¦
    assert X_train.shape[0] == Y_train.shape[0] == len(ids_train), "Train ç¶­åº¦ä¸ä¸€è‡´!"
    assert X_test.shape[0] == Y_test.shape[0] == len(ids_test), "Test ç¶­åº¦ä¸ä¸€è‡´!"

    # åˆ—å° merge, filter å¾Œ æ¯å€‹è³‡æ–™é›†çš„æ¼²è·Œæ¯”ä¾‹
    # print(Y_train.shape)
    # for i in range(LABEL_COUNT):
    #     print(f"ç¬¬ {i} çµ„ ï¼¹")
    print_label_distribution(Y_train, Y_test, "ALL")

    print(f"Y_train{SUFFIX} æœ‰ {np.sum(Y_train == 0)} å€‹ 0")
    print(f"Y_test{SUFFIX} æœ‰ {np.sum(Y_test == 0)} å€‹ 0")

    # å°‡ Y éƒ½è®Šæˆ é¡åˆ¥
    Y_train = categorize_array_multi(Y_train)
    Y_test = categorize_array_multi(Y_test)

    # å„²å­˜
    sparse.save_npz(f"{OUTPUT_PATH}/X_train{SUFFIX}.npz", X_train)
    sparse.save_npz(f"{OUTPUT_PATH}/X_test{SUFFIX}.npz", X_test)
    np.savez_compressed(f"{OUTPUT_PATH}/Y_train{SUFFIX}.npz", Y=Y_train)
    np.savez_compressed(f"{OUTPUT_PATH}/Y_test{SUFFIX}.npz",  Y=Y_test)


    if ids_train is not None:
        print(ids_train.shape)
        print(ids_test.shape)
        with open(f"{OUTPUT_PATH}/ids_train{SUFFIX}.pkl", 'wb') as file:
            pickle.dump(ids_train.tolist(), file)
        with open(f"{OUTPUT_PATH}/ids_test{SUFFIX}.pkl", 'wb') as file:
            pickle.dump(ids_test.tolist(), file)




    print(f"âœ… Merge{SUFFIX} å®Œæˆï¼Œè³‡æ–™å·²è¼¸å‡ºåˆ° ../data/ml/dataset\n")

    # è¨ˆç®—æ¯å€‹è³‡æ–™é›†ä¸­æ¯å¤©çš„æ¨æ–‡ç¸½æ•¸
    count_per_day(ids_train, "train")
    count_per_day(ids_test, "test")

    print("âœ… å·²å°‡ä¸åŒè³‡æ–™é›†æ¯å¤©çš„æ¨æ–‡ç¸½æ•¸è¼¸å‡ºç‚º csv åˆ° ../data/ml/dataset\n")





# --- åˆ—å°å¹³è¡¡å¥½çš„çµæœ ---
def print_split_number(train_expanded, test_expanded, COIN_SHORT_NAME):
    sum = len(train_expanded) + len(test_expanded)
    print(f"{COIN_SHORT_NAME}{SUFFIX}ï¼š")
    print(f"ç†è«–å€¼ Train: {int(sum * 0.8)},                Test: {int(sum * 0.1)}")
    print(f"å¯¦éš›å€¼ Train: {len(train_expanded)} ({round((len(train_expanded) / sum), 10)}), Test: {len(test_expanded)} ({round((len(test_expanded) / sum), 10)})\n")



def main():

    # åˆ†åˆ¥åˆ‡è³‡æ–™é›†
    DOGE_dates_train_expanded, DOGE_dates_test_expanded = splitset_dates("DOGE")
    print_split_number(DOGE_dates_train_expanded, DOGE_dates_test_expanded, "DOGE")
    DOGE_X_train, DOGE_X_test, DOGE_Y_train, DOGE_Y_test, DOGE_ids_train, DOGE_ids_test = splitset_XY("DOGE")  # è‹¥è¦åˆ†å‡º val => splitset_XY("DOGE", True)
    input("æŒ‰ Enter ä»¥ç¹¼çºŒ...")

    PEPE_dates_train_expanded, PEPE_dates_test_expanded = splitset_dates("PEPE")
    print_split_number(PEPE_dates_train_expanded, PEPE_dates_test_expanded, "PEPE")
    PEPE_X_train, PEPE_X_test, PEPE_Y_val, PEPE_Y_test, PEPE_ids_train, PEPE_ids_test = splitset_XY("PEPE")
    input("æŒ‰ Enter ä»¥ç¹¼çºŒ...")
    
    TRUMP_dates_train_expanded, TRUMP_dates_test_expanded = splitset_dates("TRUMP")
    print_split_number(TRUMP_dates_train_expanded, TRUMP_dates_test_expanded, "TRUMP")
    TRUMP_X_train, TRUMP_X_test, TRUMP_Y_train, TRUMP_Y_test, TRUMP_ids_train, TRUMP_ids_test = splitset_XY("TRUMP")
    input("æŒ‰ Enter ä»¥ç¹¼çºŒ...")
    
    
    # è®€å–æ‰€æœ‰é—œéµè©çš„åå­—
    json_path = os.path.join("../data/keyword/machine_learning", f"all_keywords{SUFFIX}.json")
    with open(json_path, "r", encoding="utf-8") as f:
        vocab = json.load(f)

    all_vocab = list(vocab)
    print("len(all_vocab):", len(all_vocab))

    print(DOGE_ids_train.shape)
    print(DOGE_ids_test.shape)
    print(PEPE_ids_train.shape)
    print(PEPE_ids_test.shape)
    print(TRUMP_ids_train.shape)
    print(TRUMP_ids_test.shape)

    # åˆä½µè³‡æ–™é›†
    merge(DOGE_X_train, DOGE_X_test, DOGE_Y_train, DOGE_Y_test,
        PEPE_X_train, PEPE_X_test, PEPE_Y_val, PEPE_Y_test,
        TRUMP_X_train, TRUMP_X_test, TRUMP_Y_train, TRUMP_Y_test,
        DOGE_ids_train, DOGE_ids_test,
        PEPE_ids_train, PEPE_ids_test,
        TRUMP_ids_train, TRUMP_ids_test,
        all_vocab)
    

if __name__ == "__main__":
    main()