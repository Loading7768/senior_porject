from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.metrics import accuracy_score, classification_report
from scipy.stats import loguniform  # ç”¨ä¾†éš¨æ©ŸæŠ½å– C å€¼ï¼ˆå°æ•¸åˆ†å¸ƒï¼‰
from scipy.sparse import csr_matrix
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import json
import os
from scipy import sparse
from sklearn.preprocessing import StandardScaler
import argparse
from collections import defaultdict
import joblib
import pickle
import gc
from tqdm import tqdm


# === utils for FS ===
# from ml.utils.feature_selection import make_selector


'''å¯ä¿®æ”¹åƒæ•¸'''
N_SAMPLES = 1_000_000  # è¨­å®š random sampling è¦å–å¤šå°‘æ¨£æœ¬æ•¸  (0: å–æ‰€æœ‰æ¨£æœ¬ ä¸” ä¸åš random sampling)

N_RUNS = 10  # è¨­å®š random sampling è¦è·‘å¹¾æ¬¡

# LABELS = 5  # æœ‰å¤šå°‘æ¨™ç±¤

# C = 0.18360757138767084
C = 0.01

T1 = 0.0590 # 0.1

T2 = 0.0102 # 0.00125

T3 = 0.0060

T4 = 0.0657

PRICE_CSV_PATH = "../data/coin_price"

INPUT_PATH = "../data/ml/dataset"

OUTPUT_PATH = "../data/ml/classification/logistic_regression"

SAVE_MODEL_PATH = "../data/ml/models/classification"

RUN_FIRST_CLASSIFIER = True  # æ˜¯å¦è¦è·‘ç¬¬ä¸€å€‹åˆ†é¡å™¨

RUN_SECOND_CLASSIFIER = False  # æ˜¯å¦è¦è·‘ç¬¬äºŒå€‹åˆ†é¡å™¨

IS_TRAIN = True  # çœ‹æ˜¯å¦è¦è¨“ç·´
'''å¯ä¿®æ”¹åƒæ•¸'''

os.makedirs(OUTPUT_PATH, exist_ok=True)
os.makedirs(SAVE_MODEL_PATH, exist_ok=True)

# ENABLE_SAMPLING = True





# def get_random_samples_sparse(X: csr_matrix, y: np.ndarray, seed: int = 42):
#     global N_SAMPLES, ENABLE_SAMPLING
#     n_total = X.shape[0]

#     if N_SAMPLES == 0:
#         print(f"[INFO] ä¸åš random samplingï¼Œä½¿ç”¨æ‰€æœ‰æ¨£æœ¬æ•¸: {n_total} ç­†")
#         ENABLE_SAMPLING = False
#         return [(X, y)] * N_RUNS   # ç›´æ¥è¤‡è£½ N_RUNS ä»½ï¼Œä¿æŒè¿´åœˆçµæ§‹ä¸€è‡´

#     if N_SAMPLES > n_total:
#         raise ValueError(f"æ¨£æœ¬æ•¸éå¤šï¼æœ€å¤§åªèƒ½ {n_total} ç­†")

#     samples = []
#     for run in range(N_RUNS):
#         np.random.seed(seed + run)
#         indices = np.random.choice(n_total, N_SAMPLES, replace=False)
#         X_sample = X[indices]             # ä¿æŒ sparse CSR matrix
#         y_sample = y[indices]
#         samples.append((X_sample, y_sample))

#         print(f"[INFO] Run {run}: æŠ½æ¨£å¾Œ X_train={X_sample.shape}, y_train={y_sample.shape}")
    
#     return samples


def get_random_samples_sparse_stratified(X: csr_matrix, y: np.ndarray, seed: int = 42):
    """
    X: csr_matrix
    y: np.ndarray, shape=(N,)  å¤šé¡åˆ¥æ¨™ç±¤
    """
    global N_SAMPLES
    # global ENABLE_SAMPLING
    n_total = X.shape[0]

    if N_SAMPLES == 0:
        print(f"[INFO] ä¸åš random samplingï¼Œä½¿ç”¨æ‰€æœ‰æ¨£æœ¬æ•¸: {n_total} ç­†")
        # ENABLE_SAMPLING = False
        return [(X, y)]  # å›å‚³ä¸€å€‹åŸå§‹æ•¸é‡çš„ (X, y) tuple

    classes = np.unique(y)
    n_classes = len(classes)
    if N_SAMPLES < n_classes:
        raise ValueError(f"æ¨£æœ¬æ•¸ {N_SAMPLES} å¤ªå°‘ï¼Œç„¡æ³•å¹³å‡åˆ†é…åˆ°æ¯å€‹é¡åˆ¥ ({n_classes})")
    
    samples_per_class = N_SAMPLES // n_classes

    # å»ºç«‹ç´¢å¼•å­—å…¸
    class_indices = defaultdict(list)
    for idx, label in enumerate(y):
        class_indices[label].append(idx)

    samples = []
    for run in range(N_RUNS):
        np.random.seed(seed + run)
        selected_indices = []

        for c in classes:
            idx_list = class_indices[c]
            if len(idx_list) <= samples_per_class:
                # å¦‚æœè©²é¡åˆ¥æ•¸é‡ä¸å¤ ï¼Œå°±å…¨éƒ¨æ‹¿
                selected_indices.extend(idx_list)
            else:
                selected_indices.extend(np.random.choice(idx_list, samples_per_class, replace=False))

        # å¦‚æœç¸½æ•¸å°‘æ–¼ N_SAMPLESï¼Œå¾å‰©é¤˜æ¨£æœ¬è£œè¶³
        if len(selected_indices) < N_SAMPLES:
            # set(range(n_total)) æ˜¯æ‰€æœ‰æ¨£æœ¬ç´¢å¼•ï¼ˆ0 ~ n_total-1ï¼‰   set(selected_indices) æ˜¯å·²è¢«é¸éçš„ç´¢å¼•é›†åˆ
            remaining_idx = list(set(range(n_total)) - set(selected_indices))
            remaining_needed = N_SAMPLES - len(selected_indices)
            selected_indices.extend(np.random.choice(remaining_idx, remaining_needed, replace=False))

        np.random.shuffle(selected_indices)  # æ‰“äº‚é †åº
        X_sample = X[selected_indices]
        y_sample = y[selected_indices]
        samples.append((X_sample, y_sample))

        # === æ–°å¢ï¼šçµ±è¨ˆé¡åˆ¥æ•¸é‡èˆ‡æ¯”ä¾‹ ===
        unique, counts = np.unique(y_sample, return_counts=True)
        total = len(y_sample)
        print(f"\n[INFO] Run {run}: Stratified sample X_train={X_sample.shape}, y_train={y_sample.shape}")
        for cls, cnt in zip(unique, counts):
            pct = cnt / total * 100
            print(f"   Class {cls}: {cnt} samples ({pct:.2f}%)")

    return samples





# def evaluate_by_coin_date(ids, y_true, y_pred):
#     """
#     ids: list/array of (coin, date, idx)
#     y_true: shape (N, num_labels) æˆ– (N,) å°æ‡‰çœŸå¯¦æ¨™ç±¤
#     y_pred: shape (N, num_labels) æˆ– (N,) å°æ‡‰é æ¸¬çµæœ
#     """
#     results = defaultdict(list)

#     # å°‡æ¨£æœ¬ä¾ç…§ (coin, date) èšåˆ
#     for (coin, date, _), t, p in zip(ids, y_true, y_pred):
#         results[(coin, date)].append((t, p))

#     daily_summary = {}
#     for (coin, date), samples in results.items():
#         truths, preds = zip(*samples)
#         truths = np.array(truths)
#         preds  = np.array(preds)

#         # å¦‚æœæ˜¯å–®æ¨™ç±¤ï¼Œè½‰æˆ 2D æ–¹ä¾¿çµ±ä¸€è™•ç†
#         if truths.ndim == 1:
#             truths = truths[:, None]
#         if preds.ndim == 1:
#             preds = preds[:, None]

#         num_labels = truths.shape[1]
#         majority_pred = []

#         for i in range(num_labels):
#             up   = np.sum(preds[:, i] == 1)
#             down = np.sum(preds[:, i] == 0)
#             majority_pred.append(1 if up >= down else 0)

#         majority_pred = np.array(majority_pred)
#         # åŒä¸€å¤©çš„çœŸå¯¦æ¨™ç±¤å–ç¬¬ä¸€å€‹æ¨£æœ¬ (å‡è¨­æ¯å¤©åŒå¹£ç¨®æ¼²è·Œç›¸åŒ)
#         true_label = truths[0]

#         # è¨ˆç®—æ¯å€‹æ¨™ç±¤æ˜¯å¦æ­£ç¢º
#         majority_correct = majority_pred == true_label

#         # --- è½‰æ›æˆç¬¦è™Ÿ ---
#         pred_symbols = ["ğŸŸ¢" if p == 1 else "ğŸ”´" for p in majority_pred]
#         true_symbols = ["âœ…" if c else "âŒ" for c in majority_correct]

#         daily_summary.setdefault(coin, {})
#         daily_summary[coin][date] = {
#             "true_label": true_label.tolist(),
#             "majority_pred": majority_pred.tolist(),
#             "majority_correct": majority_correct.tolist(),
#             "up_counts": np.sum(preds == 1, axis=0).tolist(),
#             "down_counts": np.sum(preds == 0, axis=0).tolist(),
#             "total_counts": len(preds),
#             "pred_symbols": pred_symbols,
#             "true_symbols": true_symbols,
#         }

#     return daily_summary, num_labels



def evaluate_by_coin_date(ids, y_true, y_pred):
    if RUN_FIRST_CLASSIFIER:
        results = defaultdict(list)

        LABEL_SYMBOLS = {
            0: "ğŸ”´",  # å¤§è·Œ
            1: "ğŸŸ ",    # è·Œ
            2: "âšª",    # æŒå¹³
            3: "ğŸŸ¡",    # æ¼²
            4: "ğŸŸ¢"   # å¤§æ¼²
        }


        # èšåˆ
        for (coin, date, _), t, p in zip(ids, y_true, y_pred):
            results[(coin, date)].append((t, p))

        daily_summary = {}
        for (coin, date), samples in results.items():
            truths, preds = zip(*samples)
            truths = np.array(truths)
            preds  = np.array(preds)

            # å¤šæ•¸æ±º
            values, counts = np.unique(preds, return_counts=True)
            majority_pred = values[np.argmax(counts)]

            true_label = truths[0]  # å‡è¨­åŒä¸€å¤©çœŸå¯¦æ¨™ç±¤ä¸€è‡´
            correct = (majority_pred == true_label)

            # è¨ˆç®—æ¯å€‹æ¨™ç±¤æ˜¯å¦æ­£ç¢º
            # majority_correct = majority_pred == true_label


            daily_summary.setdefault(coin, {})

            # å°‡å„é¡åˆ¥å‡ºç¾æ¬¡æ•¸è½‰æˆ listï¼ˆä¿æŒåŸæœ¬ up_counts/down_counts çš„æ„Ÿè¦ºï¼‰
            class_counts = [np.sum(preds == i) for i in range(5)]  # 0~4 äº”é¡
            pred_symbols = [LABEL_SYMBOLS[majority_pred]]           # å–®ä¸€é æ¸¬ç¬¦è™Ÿ

            # majority_pred = int(majority_pred)  # è½‰æˆ Python scalar
            # true_label = int(true_label)        # è½‰æˆ Python scalar
            # true_symbols = ["âœ…" if majority_pred == true_label else "âŒ"]

            true_symbols   = [LABEL_SYMBOLS[int(true_label)]]   # çœŸå¯¦ç¬¦è™Ÿ
            result_symbols = ["âœ…" if correct else "âŒ"]         # å°éŒ¯ç¬¦è™Ÿ


            daily_summary[coin][date] = {
                "true_label": int(true_label),
                "majority_pred": int(majority_pred),
                "majority_correct": bool(correct),
                "class_counts": class_counts,    # æ›¿ä»£ up_counts/down_counts
                "total_counts": len(preds),      # åŸæœ¬ total_counts
                "pred_symbols": pred_symbols,
                "true_symbols": true_symbols,     # çœŸå¯¦ç¬¦è™Ÿ
                "result_symbols": result_symbols  # å°éŒ¯ç¬¦è™Ÿ
            }

        return daily_summary, len(np.unique(y_true))




# --- è¨“ç·´ç”¨å‡½å¼ ---
def train_function(X_train, X_test, y_train, y_test, scaler, features_name, pipeline_path):
    if RUN_FIRST_CLASSIFIER:

        # print(f"\n=== Training label column {count} ===")
        # y_train = y_train[:, count]
        # y_test  = y_test[:, count]

            
        all_results = []  # å„²å­˜æ‰€æœ‰è¨“ç·´çµæœ
        best_test_acc = -1
        best_run_info = None

        # --- åˆ†å±¤éš¨æ©ŸæŠ½æ¨£ 50 è¬ ---
        train_sample = get_random_samples_sparse_stratified(X_train, y_train)  # è£¡é¢å­˜[(X_sample, y_sample), ...]

        # å®šç¾©æ¨¡å‹
        log_reg = LogisticRegression(
            solver='saga', 
            max_iter=100000, 
            verbose=1, 
            penalty='l2', 
            C = C, 
            n_jobs=-1,
            multi_class="multinomial"   # å¤šé¡åˆ¥ softmax
            )
        
        # model = OneVsRestClassifier(log_reg, n_jobs=-1)

        # å®šç¾©åƒæ•¸åˆ†å¸ƒï¼ˆéš¨æ©ŸæŠ½æ¨£ï¼‰
        # param_dist = {
        #     'C': 0.001,   # C å€¼åœ¨ [0.001, 1000] ç¯„åœéš¨æ©ŸæŠ½
        # }

        

        for run in range(len(train_sample)):  # ç¸½å…±è¨“ç·´ N_RUNS æ¬¡ (ä½†æ˜¯ä»¥ train_sample çš„é•·åº¦åˆ¤æ–·ï¼Œæ‰€ä»¥è‹¥æ²’æœ‰ç”¨ random sampling è·‘ä¸€æ¬¡ä¾¿æœƒåŸ·è¡Œå®Œæˆ)
            # éš¨æ©Ÿæœå°‹
            # random_search = RandomizedSearchCV(
            #     estimator=log_reg,
            #     param_distributions=param_dist,
            #     n_iter=1,             # éš¨æ©ŸæŒ‘ 10 çµ„
            #     scoring='accuracy',   # è©•ä¼°æ–¹å¼
            #     cv=3,                 # 3 æŠ˜äº¤å‰é©—è­‰
            #     verbose=2,
            #     random_state=42 + run,
            #     n_jobs=1             # ä¸ä½¿ç”¨å¤šæ ¸å¿ƒ
            # )

            # é–‹å§‹è¨“ç·´
            X_train_sample, y_train_sample = train_sample[run]
            log_reg.fit(X_train_sample, y_train_sample)

            # print("Random search æœ€ä½³åƒæ•¸:", random_search.best_params_)
            # print("Random search æœ€ä½³äº¤å‰é©—è­‰æº–ç¢ºç‡:", random_search.best_score_)

            # best_model = random_search.best_estimator_

            # --- è©•ä¼° ---
            train_acc = accuracy_score(y_train_sample, log_reg.predict(X_train_sample))
            test_acc = accuracy_score(y_test, log_reg.predict(X_test))

            print(f"[RUN {run}] Train acc={train_acc:.4f}, Test acc={test_acc:.4f}")

            # --- ä¿å­˜çµæœ ---
            all_results.append({
                "run": run,
                "train_acc": train_acc,
                "test_acc": test_acc,
                # "best_params": random_search.best_params_
            })

            # --- æ›´æ–°æœ€ä½³æ¨¡å‹ ---
            if test_acc > best_test_acc:
                best_test_acc = test_acc
                best_run_info = {
                    "run": run,
                    "model": log_reg,
                    "scaler": scaler,
                    "train_acc": train_acc,
                    "test_acc": test_acc,
                    # "params": random_search.best_params_
                }

            # å¦‚æœæ²’æœ‰ç”¨ random sampling å°±åªè¦è·‘ä¸€æ¬¡è¿´åœˆå°±å¥½
            # if not ENABLE_SAMPLING:
            #     break
            
            # === å¼·åˆ¶æ¸…ç† ===
            # del random_search
            # del best_model
            # gc.collect()


        # --- å…¨éƒ¨çµæœè¼¸å‡º ---
        results_df = pd.DataFrame(all_results)
        print("\n=== æ‰€æœ‰ Run çš„çµæœ ===")
        print(results_df)
        results_df.to_csv(f"{OUTPUT_PATH}/logreg_sampling_results_{N_SAMPLES}.csv", index=False)


        # --- å„²å­˜æœ€ä½³æ¨¡å‹ ---
        joblib.dump({
            "model": best_run_info["model"],
            "scaler": best_run_info["scaler"]
        }, pipeline_path)

        print("\n=== æœ€ä½³æ¨¡å‹ ===")
        print(f"Run {best_run_info['run']} | Train acc={best_run_info['train_acc']:.4f}, Test acc={best_run_info['test_acc']:.4f}")
        # print(f"æœ€ä½³åƒæ•¸: {best_run_info['params']}")
        print(f"å·²å„²å­˜æœ€ä½³ pipeline åˆ° {pipeline_path}")

        print("\nåˆ†é¡å ±å‘Š (Test set):")
        print(classification_report(y_test, best_run_info["model"].predict(X_test)))



        # === ç”¨æœ€ä½³æ¨¡å‹åšè¼¸å‡ºå’Œé æ¸¬ ===
        # most_best_model = best_run_info["model"]


        # é—œéµå­—ä¿‚æ•¸
        # coefficients = pd.Series(most_best_model.coef_[0], index=features_name).sort_values(ascending=False)
        # coeff_dict = coefficients.to_dict()

        # coeff_path = f"{OUTPUT_PATH}/logistic_regression_keyword_coefficients.json"
        # with open(coeff_path, "w", encoding="utf-8") as f:
        #     json.dump(coeff_dict, f, ensure_ascii=False, indent=4)

        # print(f"é—œéµè©ä¿‚æ•¸å·²å­˜æˆ JSONï¼š{coeff_path}")

        # print("\nè¢«æ’é™¤çš„æ—¥æœŸï¼ˆæ²’æœ‰æ¨æ–‡æˆ–ç„¡æ³•è¨ˆç®—åƒ¹æ ¼è®ŠåŒ–ï¼‰:")
        # print(unprocessed_dates)

        # æœ€å¾Œä¸€ç­†ä¹Ÿç„¡æ³•è¨ˆç®—ï¼ˆå› ç‚ºæ²’ã€Œæ˜å¤©ã€ï¼‰
        # unprocessed_dates.append(df.loc[len(df)-1, "date"].strftime("%Y/%m/%d"))



# --- é æ¸¬ç”¨å‡½å¼ ---
def predict_function(X_train, X_test, y_train, y_test, pipeline_path):
    if RUN_FIRST_CLASSIFIER:

        # y_train = y_train[:, count]
        # y_test  = y_test[:, count]

        # === è¼‰å…¥æœ€ä½³æ¨¡å‹ ===
        pipeline = joblib.load(pipeline_path)
        model = pipeline["model"]
        
        # === é æ¸¬æ‰€æœ‰æ¨£æœ¬ ===
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        print(y_pred_train.shape)
        print(y_pred_test.shape)

        # === è¼‰å…¥æ¨æ–‡ ID å°æ‡‰è¡¨ ===
        with open(f"{INPUT_PATH}/ids_train.pkl", "rb") as f:   # rb = read binary
            ids_train = pickle.load(f)
        with open(f"{INPUT_PATH}/ids_test.pkl", "rb") as f:   # rb = read binary
            ids_test = pickle.load(f)

        # å°‡ ids è½‰æˆ np.array æ–¹ä¾¿æ¥ä¸‹ä¾†çš„è™•ç†
        ids_train = np.array(ids_train)
        ids_test = np.array(ids_test)

        
        print("\nåˆ†é¡å ±å‘Š (Test set):")
        print(classification_report(y_test, model.predict(X_test)))


        # === å¥—ç”¨åœ¨ train / test ===
        train_daily, num_labels = evaluate_by_coin_date(ids_train, y_train, y_pred_train)
        test_daily, _           = evaluate_by_coin_date(ids_test,  y_test,  y_pred_test)

        # === å­˜æˆ JSON ===
        with open(f"{OUTPUT_PATH}/logreg_train_daily_results_{N_SAMPLES}.json", "w", encoding="utf-8") as f:
            json.dump(train_daily, f, ensure_ascii=False, indent=4, default=int)

        with open(f"{OUTPUT_PATH}/logreg_test_daily_results_{N_SAMPLES}.json", "w", encoding="utf-8") as f:
            json.dump(test_daily, f, ensure_ascii=False, indent=4, default=int)

        print("å·²è¼¸å‡ºé€æ—¥é æ¸¬çµæœï¼š")
        print(f"- train: {OUTPUT_PATH}/logreg_train_daily_results_{N_SAMPLES}.json")
        print(f"- test:  {OUTPUT_PATH}/logreg_test_daily_results_{N_SAMPLES}.json")

        # === åˆä½µ train + test ===
        combined_daily = {}
        for coin, daily in train_daily.items():
            combined_daily.setdefault(coin, {}).update(daily)
        for coin, daily in test_daily.items():
            combined_daily.setdefault(coin, {}).update(daily)

        # === å­˜æˆåˆä½µå¾Œçš„ TXT ===
        txt_path = f"{OUTPUT_PATH}/logreg_combined_results_{N_SAMPLES}.txt"
        with open(txt_path, "w", encoding="utf-8") as f:
            # === åˆå§‹åŒ–çµ±è¨ˆå™¨ ===
            # label_correct = np.zeros(num_labels, dtype=int)
            # label_total   = np.zeros(num_labels, dtype=int)
            label_correct = np.zeros(1, dtype=int)
            label_total   = np.zeros(1, dtype=int)

            for coin, daily in combined_daily.items():
                f.write(f"\n=== {coin} ===\n")

                # ç”¨ä¾†å­˜æ”¾æ¯å¤©çš„ (date, pred_class)
                records = []

                for date, stats in sorted(daily.items()):
                    # --- æ¯æ—¥è¼¸å‡ºåˆ° TXT ---
                    class_str = " ".join(f"{x:5d}" for x in stats['class_counts'])
                    line = (
                        f"{date} â†’ ğŸ“Š {class_str}  "
                        f"ç¸½æ•¸: {stats['total_counts']:5d}  "
                        f"é æ¸¬: {''.join(stats['pred_symbols'])}  "
                        f"çœŸå¯¦: {''.join(stats['true_symbols'])}  "
                        f"çµæœ: {''.join(stats['result_symbols'])}\n"
                    )
                    f.write(line)

                    # --- æ›´æ–°ç´¯ç©æº–ç¢ºç‡ ---
                    # for i, correct in enumerate(stats["majority_correct"]):
                    #     label_total[i] += 1
                    #     if correct:
                    #         label_correct[i] += 1

                    # --- æ›´æ–°ç´¯ç©æº–ç¢ºç‡ ---
                    label_total[0] += 1
                    if stats["majority_correct"]:
                        label_correct[0] += 1

                    # --- å–ç•¶å¤©é æ¸¬é¡åˆ¥ (class_counts æœ€å¤§çš„ index) ---
                    pred_class = int(np.argmax(stats["class_counts"]))
                    records.append((date, pred_class))

                # --- è¼¸å‡ºæ•´é«”æº–ç¢ºç‡ (ç™¾åˆ†æ¯”) ---
                accuracy_summary = " ".join(
                    f"{(c / t * 100):.2f}%" if t > 0 else "N/A"
                    for c, t in zip(label_correct, label_total)
                )
                f.write(f"\næ•´é«”æº–ç¢ºç‡: {accuracy_summary}\n")

                # === å­˜æˆ .npy (æ¯æ—¥é æ¸¬çµæœï¼Œä¾æ—¥æœŸæ’åº) ===
                if records:
                    records.sort(key=lambda x: x[0])
                    _, preds = zip(*records)
                    preds = np.array(preds, dtype=np.int32)

                    npy_path = f"{OUTPUT_PATH}/{coin}_logreg_classifier_1_result.npy"
                    np.save(npy_path, preds)
                    print(preds[:50])
                    print(f"{coin} â†’ {npy_path} å·²å®Œæˆ, shape={preds.shape}")


        print(f"\nåˆä½µå¾Œçš„äººé¡å¯è®€ç‰ˆçµæœå·²è¼¸å‡ºåˆ°ï¼š{txt_path}")

        # # === åŒæ™‚å°åˆ° console é è¦½ ===
        # with open(txt_path, "r", encoding="utf-8") as f:
        #     print("\n=== è¼¸å‡ºæª”æ¡ˆå…§å®¹é è¦½ ===\n")
        #     print(f.read())



def categorize_array_multi(Y, t1, t2, t3, t4):
    """
    Y: np.ndarray, shape = (num_labels,), åƒ¹æ ¼è®ŠåŒ–ç‡
    t1, t2: äº”å…ƒåˆ†é¡é–¾å€¼ï¼Œç™¾åˆ†æ¯”
    """
    # # è®€å–åƒ¹æ ¼ CSV
    # price_df = pd.read_csv(f"{PRICE_CSV_PATH}/{COIN_SHORT_NAME}_price.csv")
    # price_df['snapped_at'] = pd.to_datetime(price_df['snapped_at'], format="%Y-%m-%d %H:%M:%S %Z")
    # price_df.set_index('snapped_at', inplace=True)
    # price_df.index = price_df.index.tz_localize(None)  # ç§»é™¤æ™‚å€

    # # å»ºç«‹æ¯å¤©åƒ¹æ ¼çš„ dictï¼Œæ–¹ä¾¿æŸ¥è©¢
    # price_lookup = price_df['price'].to_dict()  # å‡è¨­ csv æœ‰ price æ¬„
    # print("price_lookup:", list(price_lookup.items())[:5])



    # # å…ˆå»ºç«‹ç©ºé™£åˆ—
    # Y_pct = np.zeros_like(Y_diff, dtype=float)

    # for i, (coin, date, tweet_id) in tqdm(enumerate(ids), total=len(ids), desc="æ­£åœ¨å°‡åƒ¹å·®è½‰æˆåƒ¹éŒ¢è®ŠåŒ–ç‡..."):
    #     # ç¢ºä¿ date æ˜¯ datetime.date
    #     if isinstance(date, str):
    #         date = pd.to_datetime(date)
    #     # æŸ¥ç•¶å¤©åƒ¹æ ¼
    #     if date in price_lookup:
    #         price_today = price_lookup[date]
    #     else:
    #         # å¦‚æœæ‰¾ä¸åˆ°æ—¥æœŸï¼Œæ”¹ç”¨ 1 é¿å…é™¤é›¶
    #         print("æ‰¾ä¸åˆ°æ—¥æœŸ:", date)
    #         price_today = 1.0

    #     # å°‡æ•´åˆ—çš„åƒ¹æ ¼å·®è½‰ç™¾åˆ†æ¯”
    #     Y_pct[i, :] = Y_diff[i, :] / price_today

    print(Y.shape)

    # äº”å…ƒåˆ†é¡
    labels = np.full_like(Y, 2, dtype=int)  # é è¨­æŒå¹³
    labels[Y <= -t1] = 0  # å¤§è·Œ
    labels[(Y > -t1) & (Y <= -t2)] = 1  # è·Œ
    labels[(Y >= t3) & (Y < t4)] = 3  # æ¼²
    labels[Y >= t4] = 4  # å¤§æ¼²

    if np.any(Y == 0):  # æª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½•å…ƒç´ ç­‰æ–¼ 0
        count = np.sum(Y == 0)
        print(f"å…±æœ‰ {count} å€‹ Y == 0")
        labels[Y == 0] = 4  # ç‚ºäº†æ ¡æ­£TRUMPå‰å…©å¤©çš„åƒ¹æ ¼ç›¸åŒ ç¬¬ä¸€å¤©è¨­ç‚ºå¤§æ¼²

    return labels



def load_and_preprocess():
    if RUN_FIRST_CLASSIFIER:
        # å–å¾— ML çš„ X
        X_train = sparse.load_npz(f"{INPUT_PATH}/X_train.npz")
        X_test = sparse.load_npz(f"{INPUT_PATH}/X_test.npz")

        print(X_train.shape)

        # åŒ¯å…¥ Y
        y_train = np.load(f"{INPUT_PATH}/Y_train.npz")
        y_train = y_train['Y']
        y_test = np.load(f"{INPUT_PATH}/Y_test.npz")
        y_test = y_test['Y']

        print(y_train.shape)

        # with open(f"{INPUT_PATH}/ids_train_filtered.pkl", 'rb') as file:
        #     ids_train_all = pickle.load(file)
        # with open(f"{INPUT_PATH}/ids_test.pkl", 'rb') as file:
        #     ids_test_all = pickle.load(file)
        
        # å»ºç«‹ target labelï¼šäº”å…ƒåˆ†é¡
        y_train_categorized = categorize_array_multi(y_train, T1, T2, T3, T4)  # shape (N,)
        y_test_categorized  = categorize_array_multi(y_test, T1, T2, T3, T4)   # shape (N,)
        print("å·²æˆåŠŸåˆ†é¡åˆ¥")

        # çµ±è¨ˆæ¯å€‹é¡åˆ¥æ•¸é‡
        print(f"å¤§è·Œï¼š-{T1 * 100:.2f}%ä»¥ä¸‹, è·Œï¼š-{T1 * 100:.2f}% ~ -{T2 * 100}%, æŒå¹³ï¼š-{T2 * 100}% ~ {T3 * 100}%, æ¼²ï¼š{T3 * 100}% ~ {T4 * 100:.2f}%, å¤§æ¼²ï¼š{T4 * 100:.2f}%ä»¥ä¸Š")
        train_total_row = y_train_categorized.shape[0]
        test_total_row = y_test_categorized.shape[0]
        # for col in range(y_train_categorized.shape[1]):
        counts = np.bincount(y_train_categorized, minlength=5)
        percentages = counts / train_total_row * 100
        percentages_str = " ".join([f"{p:.2f}%" for p in percentages])
        print(f"[TRAIN] column é¡åˆ¥: {percentages_str}")

        counts = np.bincount(y_test_categorized, minlength=5)
        percentages = counts / test_total_row * 100
        percentages_str = " ".join([f"{p:.2f}%" for p in percentages])
        print(f"[TEST]  column é¡åˆ¥: {percentages_str}\n")

        input("pasue...")



        scaler = StandardScaler(with_mean=False)  # é©åˆ sparse matrix
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)


        # å–å¾— all_keywords(features_name)
        with open(f"{INPUT_PATH}/keyword/filtered_keywords.json", "r", encoding="utf-8-sig") as jsonfile:
            features_name = json.load(jsonfile)


        # # å–å¾— price çš„ csv æª”
        # price_path = "../data/coin_price"
        # df = pd.read_csv(f"{price_path}/{COIN_SHORT_NAME}_current_tweet_price_output.csv")
        # df['date'] = pd.to_datetime(df['date'], format="%Y/%m/%d")  # æŠŠ date æ¬„ä½è½‰æˆæ—¥æœŸæ ¼å¼

        # # æŠŠç•¶å¤©æ²’æœ‰æŠ“åˆ°æ¨æ–‡çš„æ—¥æœŸå­˜èµ·ä¾†
        # unprocessed_dates = []
        # for i in range(len(df)):
        #     if df.loc[i, "has_tweet"] == False:
        #         unprocessed_dates.append(df.loc[i, "date"].strftime("%Y/%m/%d"))



        # === ç‰¹å¾µé¸æ“‡ ===
        # parser = argparse.ArgumentParser()
        # parser.add_argument("--fs", type=str, default="none", help="Feature selection method")
        # parser.add_argument("--k", type=int, default=600, help="Top k features")
        # args = parser.parse_args()

        # selector = make_selector(task="clf", method=args.fs, k=args.k)
        # if selector is not None:
        #     X_train = selector.fit_transform(X_train, y_train_categorized)
        #     X_test = selector.transform(X_test)
        #     features_name = selector.get_feature_names_out(features_name)  # æ›´æ–° features_name
        #     print(f"[INFO] Feature selection ({args.fs}) done, X_train shape = {X_train.shape}")


        return X_train, X_test, y_train_categorized, y_test_categorized, scaler, features_name
    
    if RUN_SECOND_CLASSIFIER:
        X = 1



def main():

    if RUN_FIRST_CLASSIFIER:
        # --- è¼‰å…¥è³‡æ–™ ---
        X_train, X_test, y_train, y_test, scaler, features_name = load_and_preprocess()

        # for count in range(LABELS):

        pipeline_path = f"{SAVE_MODEL_PATH}/logreg_best_pipeline_{N_SAMPLES}.joblib"  # å„²å­˜è¨“ç·´æ¨¡å‹çš„ä½ç½®

        if IS_TRAIN:
            # --- è¨“ç·´æ¨¡å‹ --- 
            train_function(X_train, X_test, y_train, y_test, scaler, features_name, pipeline_path)

            # --- é æ¸¬æ¨¡å‹ ---
            predict_function(X_train, X_test, y_train, y_test, pipeline_path)
        else:
            if not os.path.exists(pipeline_path):
                print("æ‰¾ä¸åˆ°å·²è¨“ç·´å¥½çš„ ç¬¬ä¸€å€‹åˆ†é¡å™¨ æ¨¡å‹ï¼Œè«‹å…ˆå°‡ IS_TRAIN è¨­ç‚º True")

            # --- é æ¸¬æ¨¡å‹ ---
            predict_function(X_train, X_test, y_train, y_test, pipeline_path)

    elif RUN_SECOND_CLASSIFIER:
        # --- è¼‰å…¥è³‡æ–™ ---
        X_train, X_test, y_train, y_test, scaler, features_name = load_and_preprocess()

        pipeline_path = f"{SAVE_MODEL_PATH}/logreg_best_pipeline_{N_SAMPLES}.joblib"  # å„²å­˜è¨“ç·´æ¨¡å‹çš„ä½ç½®

        if IS_TRAIN:
            # --- è¨“ç·´æ¨¡å‹ --- 
            train_function(X_train, X_test, y_train, y_test, scaler, features_name, pipeline_path)

            # --- é æ¸¬æ¨¡å‹ ---
            predict_function(X_train, X_test, y_train, y_test, pipeline_path)
        else:
            if not os.path.exists(pipeline_path):
                print("æ‰¾ä¸åˆ°å·²è¨“ç·´å¥½çš„ ç¬¬äºŒå€‹åˆ†é¡å™¨ æ¨¡å‹ï¼Œè«‹å…ˆå°‡ IS_TRAIN è¨­ç‚º True")

            # --- é æ¸¬æ¨¡å‹ ---
            predict_function(X_train, X_test, y_train, y_test, pipeline_path)



if __name__ == "__main__":
    main()