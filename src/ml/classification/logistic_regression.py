from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.metrics import accuracy_score, classification_report
from scipy.stats import loguniform  # ç”¨ä¾†éš¨æ©ŸæŠ½å– C å€¼ï¼ˆå°æ•¸åˆ†å¸ƒï¼‰
from scipy.sparse import csr_matrix
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import json
import os
from scipy import sparse
from sklearn.preprocessing import StandardScaler
import argparse
from collections import defaultdict
import joblib
import pickle
import gc


# === åŒ¯å…¥ config ===
from pathlib import Path
import sys
parent_dir = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(parent_dir))
from config import JSON_DICT_NAME, COIN_SHORT_NAME

# === utils for FS ===
from ml.utils.feature_selection import make_selector


'''å¯ä¿®æ”¹åƒæ•¸'''
N_SAMPLES = 500_000  # è¨­å®š random sampling è¦å–å¤šå°‘æ¨£æœ¬æ•¸

N_RUNS = 10  # è¨­å®š random sampling è¦è·‘å¹¾æ¬¡

INPUT_PATH = "../data/ml/dataset"

OUTPUT_PATH = "../data/ml/classification/logistic_regression"

SAVE_MODEL_PATH = "../data/ml/models/classification"
'''å¯ä¿®æ”¹åƒæ•¸'''

os.makedirs(OUTPUT_PATH, exist_ok=True)
os.makedirs(SAVE_MODEL_PATH, exist_ok=True)







def get_random_samples_sparse(X: csr_matrix, y: np.ndarray, seed: int = 42):
    n_total = X.shape[0]
    if N_SAMPLES > n_total:
        raise ValueError(f"æ¨£æœ¬æ•¸éå¤šï¼æœ€å¤§åªèƒ½ {n_total} ç­†")

    samples = []
    for run in range(N_RUNS):
        np.random.seed(seed + run)
        indices = np.random.choice(n_total, N_SAMPLES, replace=False)
        X_sample = X[indices]             # ä¿æŒ sparse CSR matrix
        y_sample = y[indices]
        samples.append((X_sample, y_sample))

        print(f"[INFO] Run {run}: æŠ½æ¨£å¾Œ X_train={X_sample.shape}, y_train={y_sample.shape}")
    
    return samples



# === é€æ—¥é€å¹£ç¨®å½™ç¸½ ===
def evaluate_by_coin_date(ids, y_true, y_pred):
    """
    ids: list/array of (coin, date, idx)
    y_true: å°æ‡‰çš„çœŸå¯¦æ¨™ç±¤
    y_pred: å°æ‡‰çš„é æ¸¬çµæœ
    """
    results = defaultdict(list)

    # ä¾ç…§ (coin, date) èšåˆ
    for (coin, date, _), t, p in zip(ids, y_true, y_pred):
        results[(coin, date)].append((t, p))

    daily_summary = {}
    for (coin, date), samples in results.items():
        truths, preds = zip(*samples)
        true_label = truths[0]  # åŒä¸€å¹£ç¨®ç•¶å¤©çš„æ¼²å¹…æ‡‰è©²æ˜¯ä¸€æ¨£çš„  å› æ­¤å–ç¬¬ä¸€å€‹å…ƒç´ å³å¯

        up = sum(p == 1 for p in preds)
        down = sum(p == 0 for p in preds)
        total = len(preds)
        up_ratio = up / total * 100
        down_ratio = down / total * 100
        majority_pred = 1 if up >= down else 0
        majority_correct = (majority_pred == true_label)

        daily_summary.setdefault(coin, {})
        daily_summary[coin][date] = {
            "true_label": int(true_label),
            "up": up,
            "down": down,
            "total": total,
            "up_ratio": up_ratio,
            "down_ratio": down_ratio,
            "majority_pred": int(majority_pred),
            "majority_correct": int(majority_correct)
        }

    return daily_summary



def main():
    # å–å¾— ML çš„ X, Y
    X_train = sparse.load_npz(f"{INPUT_PATH}/X_train_filtered.npz")
    X_test = sparse.load_npz(f"{INPUT_PATH}/X_test.npz")
    y_train = np.load(f"{INPUT_PATH}/Y_train_filtered.npy")
    y_test = np.load(f"{INPUT_PATH}/Y_test.npy")

    scaler = StandardScaler(with_mean=False)  # é©åˆ sparse matrix
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # å»ºç«‹ target labelï¼šä¸Šæ¼²ç‚º 1ï¼Œå¦å‰‡ç‚º 0ï¼ˆäºŒå…ƒåˆ†é¡ï¼‰
    y_train = (y_train >= 0).astype(int)
    y_test = (y_test >= 0).astype(int)

    # å–å¾— all_keywords(features_name)
    with open(f"{INPUT_PATH}/keyword/filtered_keywords.json", "r", encoding="utf-8-sig") as jsonfile:
        features_name = json.load(jsonfile)


    # å–å¾— price çš„ csv æª”
    price_path = "../data/coin_price"
    df = pd.read_csv(f"{price_path}/{COIN_SHORT_NAME}_current_tweet_price_output.csv")
    df['date'] = pd.to_datetime(df['date'], format="%Y/%m/%d")  # æŠŠ date æ¬„ä½è½‰æˆæ—¥æœŸæ ¼å¼

    # æŠŠç•¶å¤©æ²’æœ‰æŠ“åˆ°æ¨æ–‡çš„æ—¥æœŸå­˜èµ·ä¾†
    unprocessed_dates = []
    for i in range(len(df)):
        if df.loc[i, "has_tweet"] == False:
            unprocessed_dates.append(df.loc[i, "date"].strftime("%Y/%m/%d"))



    # === ç‰¹å¾µé¸æ“‡ ===
    parser = argparse.ArgumentParser()
    parser.add_argument("--fs", type=str, default="none", help="Feature selection method")
    parser.add_argument("--k", type=int, default=600, help="Top k features")
    args = parser.parse_args()

    selector = make_selector(task="clf", method=args.fs, k=args.k)
    if selector is not None:
        X_train = selector.fit_transform(X_train, y_train)
        X_test = selector.transform(X_test)
        features_name = selector.get_feature_names_out(features_name)  # æ›´æ–° features_name
        print(f"[INFO] Feature selection ({args.fs}) done, X_train shape = {X_train.shape}")



    all_results = []  # å„²å­˜æ‰€æœ‰è¨“ç·´çµæœ
    best_test_acc = -1
    best_run_info = None

    # --- åˆ†å±¤éš¨æ©ŸæŠ½æ¨£ 50 è¬ ---
    train_sample = get_random_samples_sparse(X_train, y_train)  # è£¡é¢å­˜[(X_sample), (y_sample), ...]

    # å®šç¾©æ¨¡å‹
    log_reg = LogisticRegression(solver='saga', max_iter=100000, verbose=1, penalty='l2', n_jobs=-1)

    # å®šç¾©åƒæ•¸åˆ†å¸ƒï¼ˆéš¨æ©ŸæŠ½æ¨£ï¼‰
    param_dist = {
        'C': loguniform(1e-3, 1e3),   # C å€¼åœ¨ [0.001, 1000] ç¯„åœéš¨æ©ŸæŠ½
    }

    

    for run in range(N_RUNS):  # ç¸½å…±è¨“ç·´ N_RUNS æ¬¡
        # éš¨æ©Ÿæœå°‹
        random_search = RandomizedSearchCV(
            estimator=log_reg,
            param_distributions=param_dist,
            n_iter=10,             # éš¨æ©ŸæŒ‘ 10 çµ„
            scoring='accuracy',   # è©•ä¼°æ–¹å¼
            cv=3,                 # 3 æŠ˜äº¤å‰é©—è­‰
            verbose=2,
            random_state=42 + run,
            n_jobs=1             # ä¸ä½¿ç”¨å¤šæ ¸å¿ƒ
        )

        # é–‹å§‹è¨“ç·´
        X_train_sample, y_train_sample = train_sample[run]
        random_search.fit(X_train_sample, y_train_sample)

        print("Random search æœ€ä½³åƒæ•¸:", random_search.best_params_)
        print("Random search æœ€ä½³äº¤å‰é©—è­‰æº–ç¢ºç‡:", random_search.best_score_)

        best_model = random_search.best_estimator_

        # --- è©•ä¼° ---
        train_acc = accuracy_score(y_train_sample, best_model.predict(X_train_sample))
        test_acc = accuracy_score(y_test, best_model.predict(X_test))

        print(f"[RUN {run}] Train acc={train_acc:.4f}, Test acc={test_acc:.4f}, Best params={random_search.best_params_}")

        # --- ä¿å­˜çµæœ ---
        all_results.append({
            "run": run,
            "train_acc": train_acc,
            "test_acc": test_acc,
            "best_params": random_search.best_params_
        })

        # --- æ›´æ–°æœ€ä½³æ¨¡å‹ ---
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            best_run_info = {
                "run": run,
                "model": best_model,
                "scaler": scaler,
                "selector": selector,
                "train_acc": train_acc,
                "test_acc": test_acc,
                "params": random_search.best_params_
            }
        
        # === å¼·åˆ¶æ¸…ç† ===
        del random_search
        del best_model
        gc.collect()


    # --- å…¨éƒ¨çµæœè¼¸å‡º ---
    results_df = pd.DataFrame(all_results)
    print("\n=== æ‰€æœ‰ Run çš„çµæœ ===")
    print(results_df)
    results_df.to_csv(f"{OUTPUT_PATH}/logreg_sampling_results.csv", index=False)


    # --- å„²å­˜æœ€ä½³æ¨¡å‹ ---
    pipeline_path = f"{SAVE_MODEL_PATH}/logreg_best_pipeline.joblib"
    joblib.dump({
        "model": best_run_info["model"],
        "scaler": best_run_info["scaler"],
        "selector": best_run_info["selector"]
    }, pipeline_path)

    print("\n=== æœ€ä½³æ¨¡å‹ ===")
    print(f"Run {best_run_info['run']} | Train acc={best_run_info['train_acc']:.4f}, Test acc={best_run_info['test_acc']:.4f}")
    print(f"æœ€ä½³åƒæ•¸: {best_run_info['params']}")
    print(f"å·²å„²å­˜æœ€ä½³ pipeline åˆ° {pipeline_path}")

    print("\nåˆ†é¡å ±å‘Š (Test set):")
    print(classification_report(y_test, best_run_info["model"].predict(X_test)))



    # === ç”¨æœ€ä½³æ¨¡å‹åšè¼¸å‡ºå’Œé æ¸¬ ===
    most_best_model = best_run_info["model"]


    # é—œéµå­—ä¿‚æ•¸
    coefficients = pd.Series(most_best_model.coef_[0], index=features_name).sort_values(ascending=False)
    coeff_dict = coefficients.to_dict()

    coeff_path = f"{OUTPUT_PATH}/logistic_regression_keyword_coefficients.json"
    with open(coeff_path, "w", encoding="utf-8") as f:
        json.dump(coeff_dict, f, ensure_ascii=False, indent=4)

    print(f"é—œéµè©ä¿‚æ•¸å·²å­˜æˆ JSONï¼š{coeff_path}")

    print("\nè¢«æ’é™¤çš„æ—¥æœŸï¼ˆæ²’æœ‰æ¨æ–‡æˆ–ç„¡æ³•è¨ˆç®—åƒ¹æ ¼è®ŠåŒ–ï¼‰:")
    print(unprocessed_dates)

    # æœ€å¾Œä¸€ç­†ä¹Ÿç„¡æ³•è¨ˆç®—ï¼ˆå› ç‚ºæ²’ã€Œæ˜å¤©ã€ï¼‰
    # unprocessed_dates.append(df.loc[len(df)-1, "date"].strftime("%Y/%m/%d"))




    # === é æ¸¬æ‰€æœ‰æ¨£æœ¬ ===
    y_pred_train = most_best_model.predict(X_train)
    y_pred_test = most_best_model.predict(X_test)

    # === è¼‰å…¥æ¨æ–‡ ID å°æ‡‰è¡¨ ===
    with open(f"{INPUT_PATH}/ids_train.pkl", "rb") as f:   # rb = read binary
        ids_train = pickle.load(f)
    with open(f"{INPUT_PATH}/ids_test.pkl", "rb") as f:   # rb = read binary
        ids_test = pickle.load(f)

    # å°‡ ids è½‰æˆ np.array æ–¹ä¾¿æ¥ä¸‹ä¾†çš„è™•ç†
    ids_train = np.array(ids_train)
    ids_test = np.array(ids_test)

    


    # === å¥—ç”¨åœ¨ train / test ===
    train_daily = evaluate_by_coin_date(ids_train, y_train, y_pred_train)
    test_daily  = evaluate_by_coin_date(ids_test,  y_test,  y_pred_test)

    # === å­˜æˆ JSON ===
    with open(f"{OUTPUT_PATH}/logreg_train_daily_results.json", "w", encoding="utf-8") as f:
        json.dump(train_daily, f, ensure_ascii=False, indent=4)

    with open(f"{OUTPUT_PATH}/logreg_test_daily_results.json", "w", encoding="utf-8") as f:
        json.dump(test_daily, f, ensure_ascii=False, indent=4)

    print("å·²è¼¸å‡ºé€æ—¥é æ¸¬çµæœï¼š")
    print(f"- train: {OUTPUT_PATH}/logreg_train_daily_results.json")
    print(f"- test:  {OUTPUT_PATH}/logreg_test_daily_results.json")

    # === ç¯„ä¾‹è¼¸å‡º (äººé¡å¯è®€ç‰ˆ) ===
    for coin, daily in test_daily.items():
        print(f"\n=== {coin} ===")
        for date, stats in sorted(daily.items()):
            line = (
                f"{date} â†’ ğŸ‘ {stats['up']}  ğŸ‘ {stats['down']}  ğŸ“Š {stats['total']}  "
                f"ğŸ‘æ¯”: {stats['up_ratio']:.2f}%  ğŸ‘æ¯”: {stats['down_ratio']:.2f}%  "
                f"å¤šæ•¸: {'up' if stats['majority_pred']==1 else 'down'}"
            )
            print(line)





    # --- ä¸‹æ¬¡é æ¸¬æ™‚å¯ç”¨ ---
    # pipeline = joblib.load(pipeline_path)

    # X_new = ...  # æ–°è³‡æ–™ (ç¨€ç–çŸ©é™£)
    # X_new_scaled = pipeline["scaler"].transform(X_new)
    # if pipeline["selector"] is not None:
    #     X_new_scaled = pipeline["selector"].transform(X_new_scaled)

    # y_pred = pipeline["model"].predict(X_new_scaled)

if __name__ == "__main__":
    main()