from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.metrics import accuracy_score, classification_report
from scipy.stats import loguniform  # ç”¨ä¾†éš¨æ©ŸæŠ½å– C å€¼ï¼ˆå°æ•¸åˆ†å¸ƒï¼‰
from scipy.sparse import csr_matrix
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import json
import os
from scipy import sparse
from sklearn.preprocessing import StandardScaler
import argparse
from collections import defaultdict
import joblib
import pickle
import gc
from tqdm import tqdm


# === utils for FS ===
# from ml.utils.feature_selection import make_selector


'''å¯ä¿®æ”¹åƒæ•¸'''
N_SAMPLES = 1_000_000  # è¨­å®š random sampling è¦å–å¤šå°‘æ¨£æœ¬æ•¸  (0: å–æ‰€æœ‰æ¨£æœ¬ ä¸” ä¸åš random sampling)

N_RUNS = 10  # è¨­å®š random sampling è¦è·‘å¹¾æ¬¡

# LABELS = 5  # æœ‰å¤šå°‘æ¨™ç±¤

# C = 0.18360757138767084
C = 0.18  # 0.18 è·‘ç¬¬ä¸€å€‹åˆ†é¡å™¨æ˜¯ rf

T1 = 0.0590 # 0.1

T2 = 0.0102 # 0.00125

T3 = 0.0060

T4 = 0.0657

PRICE_CSV_PATH = "../data/coin_price"

INPUT_PATH = "../data/ml/dataset"

OUTPUT_PATH = "../data/ml/classification/logistic_regression"

SAVE_MODEL_PATH = "../data/ml/models/classification"

MODEL_NAME = "rf"  # ç¬¬äºŒå€‹åˆ†é¡å™¨ç›®å‰è¼¸å…¥çš„æ¨¡å‹åå­—

RUN_FIRST_CLASSIFIER = False  # æ˜¯å¦è¦è·‘ç¬¬ä¸€å€‹åˆ†é¡å™¨

RUN_SECOND_CLASSIFIER = True  # æ˜¯å¦è¦è·‘ç¬¬äºŒå€‹åˆ†é¡å™¨

IS_GROUPED_CV = False  # æ˜¯å¦è¦è·‘ç¬¬äºŒå€‹åˆ†é¡å™¨çš„äº¤å‰é©—è­‰

IS_TRAIN = True  # çœ‹æ˜¯å¦è¦è¨“ç·´

IS_FILTERED = True  # çœ‹æ˜¯å¦æœ‰åˆ† normal èˆ‡ bot

IS_RUN_AUGUST = False  # çœ‹ç¾åœ¨æ˜¯ä¸æ˜¯è¦è·‘ 2025/08 çš„è³‡æ–™
'''å¯ä¿®æ”¹åƒæ•¸'''

os.makedirs(OUTPUT_PATH, exist_ok=True)
os.makedirs(SAVE_MODEL_PATH, exist_ok=True)

SUFFIX_FILTERED = "" if IS_FILTERED else "_non_filtered"
SUFFIX_AUGUST   = "_202508" if IS_RUN_AUGUST else ""



def get_random_samples_sparse_stratified(X: csr_matrix, y: np.ndarray, seed: int = 42):
    """
    X: csr_matrix
    y: np.ndarray, shape=(N,)  å¤šé¡åˆ¥æ¨™ç±¤
    """
    global N_SAMPLES
    # global ENABLE_SAMPLING
    n_total = X.shape[0]

    if N_SAMPLES == 0:
        print(f"[INFO] ä¸åš random samplingï¼Œä½¿ç”¨æ‰€æœ‰æ¨£æœ¬æ•¸: {n_total} ç­†")
        # ENABLE_SAMPLING = False
        return [(X, y)]  # å›å‚³ä¸€å€‹åŸå§‹æ•¸é‡çš„ (X, y) tuple

    classes = np.unique(y)
    n_classes = len(classes)
    if N_SAMPLES < n_classes:
        raise ValueError(f"æ¨£æœ¬æ•¸ {N_SAMPLES} å¤ªå°‘ï¼Œç„¡æ³•å¹³å‡åˆ†é…åˆ°æ¯å€‹é¡åˆ¥ ({n_classes})")
    
    samples_per_class = N_SAMPLES // n_classes

    # å»ºç«‹ç´¢å¼•å­—å…¸
    class_indices = defaultdict(list)
    for idx, label in enumerate(y):
        class_indices[label].append(idx)

    samples = []
    for run in range(N_RUNS):
        np.random.seed(seed + run)
        selected_indices = []

        for c in classes:
            idx_list = class_indices[c]
            if len(idx_list) <= samples_per_class:
                # å¦‚æœè©²é¡åˆ¥æ•¸é‡ä¸å¤ ï¼Œå°±å…¨éƒ¨æ‹¿
                selected_indices.extend(idx_list)
            else:
                selected_indices.extend(np.random.choice(idx_list, samples_per_class, replace=False))

        # å¦‚æœç¸½æ•¸å°‘æ–¼ N_SAMPLESï¼Œå¾å‰©é¤˜æ¨£æœ¬è£œè¶³
        if len(selected_indices) < N_SAMPLES:
            # set(range(n_total)) æ˜¯æ‰€æœ‰æ¨£æœ¬ç´¢å¼•ï¼ˆ0 ~ n_total-1ï¼‰   set(selected_indices) æ˜¯å·²è¢«é¸éçš„ç´¢å¼•é›†åˆ
            remaining_idx = list(set(range(n_total)) - set(selected_indices))
            remaining_needed = N_SAMPLES - len(selected_indices)
            selected_indices.extend(np.random.choice(remaining_idx, remaining_needed, replace=False))

        np.random.shuffle(selected_indices)  # æ‰“äº‚é †åº
        X_sample = X[selected_indices]
        y_sample = y[selected_indices]
        samples.append((X_sample, y_sample))

        # === æ–°å¢ï¼šçµ±è¨ˆé¡åˆ¥æ•¸é‡èˆ‡æ¯”ä¾‹ ===
        unique, counts = np.unique(y_sample, return_counts=True)
        total = len(y_sample)
        print(f"\n[INFO] Run {run}: Stratified sample X_train={X_sample.shape}, y_train={y_sample.shape}")
        for cls, cnt in zip(unique, counts):
            pct = cnt / total * 100
            print(f"   Class {cls}: {cnt} samples ({pct:.2f}%)")

    return samples



# def stratified_train_test_balance(X, y, ids, max_per_class=None, seed=42):
#     labels, counts = np.unique(y, return_counts=True)
#     print("\nå„é¡åˆ¥æ•¸é‡ï¼ˆnp.uniqueï¼‰:")
#     for label, count in zip(labels, counts):
#         print(f"é¡åˆ¥ {label}: {count} ç­†")

#     # å„²å­˜ index
#     train_idx = []
#     test_idx = []

#     # æ‰¾å‡ºæ¯å€‹é¡åˆ¥çš„æ‰€æœ‰ index
#     class_indices = defaultdict(list)
#     for idx, label in enumerate(y):
#         class_indices[label].append(idx)

#     rng = np.random.default_rng(seed)  # å»ºç«‹å›ºå®š seed çš„éš¨æ©Ÿç”Ÿæˆå™¨

#     for label, indices in class_indices.items():
#         indices = np.array(indices)
#         rng.shuffle(indices)  # ä½¿ç”¨ rng.shuffle ä»£æ›¿ np.random.shuffle
#         train_samples = indices[:max_per_class]
#         test_samples = indices[max_per_class:]

#         train_idx.extend(train_samples)
#         test_idx.extend(test_samples)

#     # ä¾ç…§ index å–å‡ºè³‡æ–™
#     X_train = X[train_idx]
#     y_train = y[train_idx]
#     ids_train = ids[train_idx]

#     X_test = X[test_idx]
#     y_test = y[test_idx]
#     ids_test = ids[test_idx]

#     return X_train, X_test, y_train, y_test, ids_train, ids_test



# æ–°å¢ä¸€å€‹å‡½å¼ä¾†å¹³è¡¡è¨“ç·´é›†
# def balance_train_data(X_train, y_train, ids_train):
    classes = np.unique(y_train)
    class_indices = defaultdict(list)
    for idx, label in enumerate(y_train):
        class_indices[label].append(idx)
    
    min_class_count = min(len(indices) for indices in class_indices.values())
    
    balanced_indices = []
    for c in classes:
        idx_list = class_indices[c]
        np.random.shuffle(idx_list)
        balanced_indices.extend(idx_list[:min_class_count])
    
    np.random.shuffle(balanced_indices)
    
    X_train_balanced = X_train[balanced_indices]
    y_train_balanced = y_train[balanced_indices]
    ids_train_balanced = [ids_train[i] for i in balanced_indices]
    
    return X_train_balanced, y_train_balanced, ids_train_balanced



# def evaluate_by_coin_date_2_category(ids, y_true, y_pred):
    """
    ids: list/array of (coin, date, idx)
    y_true: shape (N, num_labels) æˆ– (N,) å°æ‡‰çœŸå¯¦æ¨™ç±¤
    y_pred: shape (N, num_labels) æˆ– (N,) å°æ‡‰é æ¸¬çµæœ
    """
    results = defaultdict(list)

    # å°‡æ¨£æœ¬ä¾ç…§ (coin, date) èšåˆ
    for (coin, date, _), t, p in zip(ids, y_true, y_pred):
        results[(coin, date)].append((t, p))

    daily_summary = {}
    for (coin, date), samples in results.items():
        truths, preds = zip(*samples)
        truths = np.array(truths)
        preds  = np.array(preds)

        # å¦‚æœæ˜¯å–®æ¨™ç±¤ï¼Œè½‰æˆ 2D æ–¹ä¾¿çµ±ä¸€è™•ç†
        if truths.ndim == 1:
            truths = truths[:, None]
        if preds.ndim == 1:
            preds = preds[:, None]

        num_labels = truths.shape[1]
        majority_pred = []

        for i in range(num_labels):
            up   = np.sum(preds[:, i] == 1)
            down = np.sum(preds[:, i] == 0)
            majority_pred.append(1 if up >= down else 0)

        majority_pred = np.array(majority_pred)
        # åŒä¸€å¤©çš„çœŸå¯¦æ¨™ç±¤å–ç¬¬ä¸€å€‹æ¨£æœ¬ (å‡è¨­æ¯å¤©åŒå¹£ç¨®æ¼²è·Œç›¸åŒ)
        true_label = truths[0]

        # è¨ˆç®—æ¯å€‹æ¨™ç±¤æ˜¯å¦æ­£ç¢º
        majority_correct = majority_pred == true_label

        # --- è½‰æ›æˆç¬¦è™Ÿ ---
        pred_symbols = ["ğŸŸ¢" if p == 1 else "ğŸ”´" for p in majority_pred]
        true_symbols = ["âœ…" if c else "âŒ" for c in majority_correct]

        daily_summary.setdefault(coin, {})
        daily_summary[coin][date] = {
            "true_label": true_label.tolist(),
            "majority_pred": majority_pred.tolist(),
            "majority_correct": majority_correct.tolist(),
            "up_counts": np.sum(preds == 1, axis=0).tolist(),
            "down_counts": np.sum(preds == 0, axis=0).tolist(),
            "total_counts": len(preds),
            "pred_symbols": pred_symbols,
            "true_symbols": true_symbols,
        }

    return daily_summary, num_labels



def evaluate_by_coin_date(ids, y_true, y_pred):
    LABEL_SYMBOLS = {
        0: "ğŸ”´",  # å¤§è·Œ
        1: "ğŸŸ ",    # è·Œ
        2: "âšª",    # æŒå¹³
        3: "ğŸŸ¡",    # æ¼²
        4: "ğŸŸ¢"   # å¤§æ¼²
    }

    if RUN_FIRST_CLASSIFIER:
        results = defaultdict(list)

        # èšåˆ
        for (coin, date, _), t, p in zip(ids, y_true, y_pred):
            results[(coin, date)].append((t, p))

        daily_summary = {}
        for (coin, date), samples in results.items():
            truths, preds = zip(*samples)
            truths = np.array(truths)
            preds  = np.array(preds)

            # å¤šæ•¸æ±º
            values, counts = np.unique(preds, return_counts=True)
            majority_pred = values[np.argmax(counts)]

            true_label = truths[0]  # å‡è¨­åŒä¸€å¤©çœŸå¯¦æ¨™ç±¤ä¸€è‡´
            correct = (majority_pred == true_label)

            # è¨ˆç®—æ¯å€‹æ¨™ç±¤æ˜¯å¦æ­£ç¢º
            # majority_correct = majority_pred == true_label


            daily_summary.setdefault(coin, {})

            # å°‡å„é¡åˆ¥å‡ºç¾æ¬¡æ•¸è½‰æˆ listï¼ˆä¿æŒåŸæœ¬ up_counts/down_counts çš„æ„Ÿè¦ºï¼‰
            class_counts = [np.sum(preds == i) for i in range(5)]  # 0~4 äº”é¡
            pred_symbols = [LABEL_SYMBOLS[majority_pred]]           # å–®ä¸€é æ¸¬ç¬¦è™Ÿ

            # majority_pred = int(majority_pred)  # è½‰æˆ Python scalar
            # true_label = int(true_label)        # è½‰æˆ Python scalar
            # true_symbols = ["âœ…" if majority_pred == true_label else "âŒ"]

            true_symbols   = [LABEL_SYMBOLS[int(true_label)]]   # çœŸå¯¦ç¬¦è™Ÿ
            result_symbols = ["âœ…" if correct else "âŒ"]         # å°éŒ¯ç¬¦è™Ÿ


            daily_summary[coin][date] = {
                "true_label": int(true_label),
                "majority_pred": int(majority_pred),
                "majority_correct": bool(correct),
                "class_counts": class_counts,    # æ›¿ä»£ up_counts/down_counts
                "total_counts": len(preds),      # åŸæœ¬ total_counts
                "pred_symbols": pred_symbols,
                "true_symbols": true_symbols,     # çœŸå¯¦ç¬¦è™Ÿ
                "result_symbols": result_symbols  # å°éŒ¯ç¬¦è™Ÿ
            }

        return daily_summary, len(np.unique(y_true))
    
    # --- æœªå®Œæˆ ---
    elif RUN_SECOND_CLASSIFIER:
        daily_summary = {}

        for (coin, date), t, p in zip(ids, y_true, y_pred):
            correct = (p == t)

            # å„é¡åˆ¥è¨ˆæ•¸ (é€™è£¡å› ç‚ºåªæœ‰ä¸€ç­†ï¼Œåªæœ‰ä¸€å€‹é¡åˆ¥æœƒæ˜¯ 1ï¼Œå…¶é¤˜éƒ½æ˜¯ 0)
            class_counts = [1 if p == i else 0 for i in range(5)]

            daily_summary.setdefault(coin, {})
            daily_summary[coin][date] = {
                "true_label": int(t),
                "majority_pred": int(p),
                "majority_correct": bool(correct),
                "class_counts": class_counts,
                "total_counts": 1,
                "pred_symbols": [LABEL_SYMBOLS[int(p)]],
                "true_symbols": [LABEL_SYMBOLS[int(t)]],
                "result_symbols": ["âœ…" if correct else "âŒ"]
            }

        return daily_summary, len(np.unique(y_true))



# --- è¨“ç·´ç”¨å‡½å¼ ---
def train_function(X_train, X_test, y_train, y_test, pipeline_path, scaler = None, features_name = None):

    # print(f"\n=== Training label column {count} ===")
    # y_train = y_train[:, count]
    # y_test  = y_test[:, count]

        
    all_results = []  # å„²å­˜æ‰€æœ‰è¨“ç·´çµæœ
    best_test_acc = -1
    best_run_info = None

    # å®šç¾©æ¨¡å‹
    log_reg = LogisticRegression(
        solver='saga', 
        max_iter=100000, 
        verbose=1, 
        penalty='l2', 
        C = C, 
        n_jobs=-1,
        # tol=1e-6  # æ”¶æ–‚å®¹å¿åº¦ (è¶Šå°è¶Šåš´æ ¼ï¼Œè¨“ç·´å¯èƒ½æ›´ä¹…)
    )
    
    # model = OneVsRestClassifier(log_reg, n_jobs=-1)

    # å®šç¾©åƒæ•¸åˆ†å¸ƒï¼ˆéš¨æ©ŸæŠ½æ¨£ï¼‰
    # param_dist = {
    #     'C': 0.001,   # C å€¼åœ¨ [0.001, 1000] ç¯„åœéš¨æ©ŸæŠ½
    # }

    
    if RUN_FIRST_CLASSIFIER:
        # --- åˆ†å±¤éš¨æ©ŸæŠ½æ¨£ ---
        train_sample = get_random_samples_sparse_stratified(X_train, y_train)  # è£¡é¢å­˜[(X_sample, y_sample), ...]

        # ç¸½å…±è¨“ç·´ N_RUNS æ¬¡ (ä½†æ˜¯ä»¥ train_sample çš„é•·åº¦åˆ¤æ–·ï¼Œæ‰€ä»¥è‹¥æ²’æœ‰ç”¨ random sampling è·‘ä¸€æ¬¡ä¾¿æœƒåŸ·è¡Œå®Œæˆ)
        run_count = len(train_sample)

    elif RUN_SECOND_CLASSIFIER:
        train_sample = [(X_train, y_train)]
        run_count = 1

    else:
        raise ValueError("è«‹è¨­å®š RUN_FIRST_CLASSIFIER æˆ– RUN_SECOND_CLASSIFIER")

    # --- åŸ·è¡Œ N_RUNS æ¬¡ Random Sampling (ç„¡å‰‡åŸ·è¡Œä¸€æ¬¡) ---
    for run in range(run_count):  
        # éš¨æ©Ÿæœå°‹
        # random_search = RandomizedSearchCV(
        #     estimator=log_reg,
        #     param_distributions=param_dist,
        #     n_iter=1,             # éš¨æ©ŸæŒ‘ 10 çµ„
        #     scoring='accuracy',   # è©•ä¼°æ–¹å¼
        #     cv=3,                 # 3 æŠ˜äº¤å‰é©—è­‰
        #     verbose=2,
        #     random_state=42 + run,
        #     n_jobs=1             # ä¸ä½¿ç”¨å¤šæ ¸å¿ƒ
        # )

        # é–‹å§‹è¨“ç·´
        X_train_sample, y_train_sample = train_sample[run]
        log_reg.fit(X_train_sample, y_train_sample)

        # print("Random search æœ€ä½³åƒæ•¸:", random_search.best_params_)
        # print("Random search æœ€ä½³äº¤å‰é©—è­‰æº–ç¢ºç‡:", random_search.best_score_)

        # best_model = random_search.best_estimator_

        # --- è©•ä¼° ---
        train_acc = accuracy_score(y_train_sample, log_reg.predict(X_train_sample))
        test_acc = accuracy_score(y_test, log_reg.predict(X_test))

        print(f"[RUN {run}] Train acc={train_acc:.4f}, Test acc={test_acc:.4f}")

        # --- ä¿å­˜çµæœ ---
        all_results.append({
            "run": run,
            "train_acc": train_acc,
            "test_acc": test_acc,
            # "best_params": random_search.best_params_
        })

        # --- æ›´æ–°æœ€ä½³æ¨¡å‹ ---
        if (RUN_FIRST_CLASSIFIER and test_acc > best_test_acc) or RUN_SECOND_CLASSIFIER:
            best_test_acc = test_acc
            best_run_info = {
                "run": run,
                "model": log_reg,
                "scaler": scaler,
                "train_acc": train_acc,
                "test_acc": test_acc
                # "params": random_search.best_params_
            }


    # --- å…¨éƒ¨çµæœè¼¸å‡º ---
    results_df = pd.DataFrame(all_results)
    print("\n=== æ‰€æœ‰ Run çš„çµæœ ===")
    print(results_df)

    if RUN_FIRST_CLASSIFIER:
        results_df.to_csv(f"{OUTPUT_PATH}/logreg_sampling_results_{N_SAMPLES}{SUFFIX_FILTERED}.csv", index=False)
    elif RUN_SECOND_CLASSIFIER:
        results_df.to_csv(f"{OUTPUT_PATH}/logreg_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.csv", index=False)

    # å„²å­˜æœ€ä½³æ¨¡å‹
    model_dict = {"model": best_run_info["model"]}
    if scaler is not None:
        model_dict["scaler"] = scaler
    joblib.dump(model_dict, pipeline_path)

    print("\n=== æœ€ä½³æ¨¡å‹ ===")
    print(f"Run {best_run_info['run']} | Train acc={best_run_info['train_acc']:.4f}, Test acc={best_run_info['test_acc']:.4f}")
    # print(f"æœ€ä½³åƒæ•¸: {best_run_info['params']}")
    print(f"å·²å„²å­˜æœ€ä½³ pipeline åˆ° {pipeline_path}")

    # Test é›†åˆ†é¡å ±å‘Š
    print("\nåˆ†é¡å ±å‘Š (Test set):")
    print(classification_report(y_test, best_run_info["model"].predict(X_test)))



    # === ç”¨æœ€ä½³æ¨¡å‹åšè¼¸å‡ºå’Œé æ¸¬ ===
    # most_best_model = best_run_info["model"]


    # é—œéµå­—ä¿‚æ•¸
    # coefficients = pd.Series(most_best_model.coef_[0], index=features_name).sort_values(ascending=False)
    # coeff_dict = coefficients.to_dict()

    # coeff_path = f"{OUTPUT_PATH}/logistic_regression_keyword_coefficients.json"
    # with open(coeff_path, "w", encoding="utf-8") as f:
    #     json.dump(coeff_dict, f, ensure_ascii=False, indent=4)

    # print(f"é—œéµè©ä¿‚æ•¸å·²å­˜æˆ JSONï¼š{coeff_path}")

    # print("\nè¢«æ’é™¤çš„æ—¥æœŸï¼ˆæ²’æœ‰æ¨æ–‡æˆ–ç„¡æ³•è¨ˆç®—åƒ¹æ ¼è®ŠåŒ–ï¼‰:")
    # print(unprocessed_dates)

    # æœ€å¾Œä¸€ç­†ä¹Ÿç„¡æ³•è¨ˆç®—ï¼ˆå› ç‚ºæ²’ã€Œæ˜å¤©ã€ï¼‰
    # unprocessed_dates.append(df.loc[len(df)-1, "date"].strftime("%Y/%m/%d"))



def coin_month_cv(X, y, ids, C):
    # ids_classifier_2 æ˜¯ list/arrayï¼Œæ ¼å¼ [(coin, date), ...]
    ids_array = np.array(ids)
    coins, dates = ids_array[:,0], ids_array[:,1]
    dates = pd.to_datetime(dates)  # è½‰ datetime
    months = dates.to_period("M")   # å–å¾—æœˆä»½ï¼Œå¦‚ 2025-01

    # ç”Ÿæˆ (coin, month) æ¨™ç±¤
    coin_month_labels = np.array([f"{c}_{m}" for c, m in zip(coins, months)])

    unique_groups = np.unique(coin_month_labels)  # æ‰€æœ‰å¹£ç¨®æ¯æœˆçµ„åˆ
    results_all = []

    for group in unique_groups:
        # ç•™å‡ºç•¶å‰å¹£ç¨®æœˆä»½
        test_mask = coin_month_labels == group
        train_mask = ~test_mask

        X_train_cv, X_test_cv = X[train_mask], X[test_mask]
        y_train_cv, y_test_cv = y[train_mask], y[test_mask]
        ids_train_cv, ids_test_cv = [ids[i] for i in range(len(ids)) if train_mask[i]], [ids[i] for i in range(len(ids)) if test_mask[i]]

        # è¨“ç·´ Logistic Regression
        model = LogisticRegression(
            solver='saga', 
            max_iter=100000, 
            penalty='l2', 
            C=C, 
            n_jobs=-1
        )
        model.fit(X_train_cv, y_train_cv)

        # è©•ä¼°
        y_pred = model.predict(X_test_cv)
        acc = accuracy_score(y_test_cv, y_pred)
        print(f"[CV] Group {group} | Test acc: {acc:.4f}")

        results_all.append({
            "group": group,
            "test_acc": acc,
            "y_true": y_test_cv,
            "y_pred": y_pred,
            "ids_test": ids_test_cv
        })

    all_accs = [r['test_acc'] for r in results_all]
    print(f"\nAverage CV accuracy: {np.mean(all_accs):.4f}")

    return results_all



# --- é æ¸¬ç”¨å‡½å¼ ---
def predict_function(X_train, X_test, y_train, y_test, ids_train, ids_test, pipeline_path):

    # y_train = y_train[:, count]
    # y_test  = y_test[:, count]

    # === è¼‰å…¥æœ€ä½³æ¨¡å‹ ===
    pipeline = joblib.load(pipeline_path)
    model = pipeline["model"]
    
    # === é æ¸¬æ‰€æœ‰æ¨£æœ¬ ===
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    print(y_pred_train.shape)
    print(y_pred_test.shape)

    # # === è¼‰å…¥æ¨æ–‡ ID å°æ‡‰è¡¨ ===
    # with open(f"{INPUT_PATH}/ids_train.pkl", "rb") as f:   # rb = read binary
    #     ids_train = pickle.load(f)
    # with open(f"{INPUT_PATH}/ids_test.pkl", "rb") as f:   # rb = read binary
    #     ids_test = pickle.load(f)

    # å°‡ ids è½‰æˆ np.array æ–¹ä¾¿æ¥ä¸‹ä¾†çš„è™•ç†
    ids_train = np.array(ids_train)
    ids_test = np.array(ids_test)

    
    print("\nåˆ†é¡å ±å‘Š (Test set):")
    print(classification_report(y_test, model.predict(X_test), zero_division=0))


    # === å¥—ç”¨åœ¨ train / test ===
    train_daily, _ = evaluate_by_coin_date(ids_train, y_train, y_pred_train)
    test_daily, _  = evaluate_by_coin_date(ids_test,  y_test,  y_pred_test)

    if RUN_FIRST_CLASSIFIER:

        # === å­˜æˆ JSON ===
        with open(f"{OUTPUT_PATH}/logreg_train_daily_results_{N_SAMPLES}{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json", "w", encoding="utf-8") as f:
            json.dump(train_daily, f, ensure_ascii=False, indent=4, default=int)

        with open(f"{OUTPUT_PATH}/logreg_test_daily_results_{N_SAMPLES}{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json", "w", encoding="utf-8") as f:
            json.dump(test_daily, f, ensure_ascii=False, indent=4, default=int)

        print("å·²è¼¸å‡ºé€æ—¥é æ¸¬çµæœï¼š")
        print(f"- train: {OUTPUT_PATH}/logreg_train_daily_results_{N_SAMPLES}{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json")
        print(f"- test:  {OUTPUT_PATH}/logreg_test_daily_results_{N_SAMPLES}{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json")

        # === åˆä½µ train + test ===
        combined_daily = {}
        for coin, daily in train_daily.items():
            combined_daily.setdefault(coin, {}).update(daily)
        for coin, daily in test_daily.items():
            combined_daily.setdefault(coin, {}).update(daily)

        # === å­˜æˆåˆä½µå¾Œçš„ TXT ===
        txt_path = f"{OUTPUT_PATH}/logreg_combined_results_{N_SAMPLES}{SUFFIX_FILTERED}{SUFFIX_AUGUST}.txt"
        with open(txt_path, "w", encoding="utf-8") as f:
            # === åˆå§‹åŒ–çµ±è¨ˆå™¨ ===
            label_correct = np.zeros(1, dtype=int)
            label_total   = np.zeros(1, dtype=int)

            for coin, daily in combined_daily.items():
                f.write(f"\n=== {coin} ===\n")

                # ç”¨ä¾†å­˜æ”¾æ¯å¤©çš„ (date, pred_class)
                records = []

                for date, stats in sorted(daily.items()):
                    # --- æ¯æ—¥è¼¸å‡ºåˆ° TXT ---
                    class_str = " ".join(f"{x:5d}" for x in stats['class_counts'])
                    line = (
                        f"{date} â†’ ğŸ“Š {class_str}  "
                        f"ç¸½æ•¸: {stats['total_counts']:5d}  "
                        f"é æ¸¬: {''.join(stats['pred_symbols'])}  "
                        f"çœŸå¯¦: {''.join(stats['true_symbols'])}  "
                        f"çµæœ: {''.join(stats['result_symbols'])}\n"
                    )
                    f.write(line)

                    # --- æ›´æ–°ç´¯ç©æº–ç¢ºç‡ ---
                    label_total[0] += 1
                    if stats["majority_correct"]:
                        label_correct[0] += 1

                    # --- å–ç•¶å¤©é æ¸¬é¡åˆ¥ (class_counts æœ€å¤§çš„ index) ---
                    pred_class = int(np.argmax(stats["class_counts"]))
                    records.append((date, pred_class))

                # --- è¼¸å‡ºæ•´é«”æº–ç¢ºç‡ (ç™¾åˆ†æ¯”) ---
                accuracy_summary = " ".join(
                    f"{(c / t * 100):.2f}%" if t > 0 else "N/A"
                    for c, t in zip(label_correct, label_total)
                )
                f.write(f"\næ•´é«”æº–ç¢ºç‡: {accuracy_summary}\n")

                # === å­˜æˆ .npy (æ¯æ—¥é æ¸¬çµæœï¼Œä¾æ—¥æœŸæ’åº) ===
                if records:
                    records.sort(key=lambda x: x[0])
                    _, preds = zip(*records)
                    preds = np.array(preds, dtype=np.int32)

                    npy_path = f"{OUTPUT_PATH}/{coin}_logreg_classifier_1_result{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy"
                    np.save(npy_path, preds)
                    print(preds[:50])
                    print(f"{coin} â†’ {npy_path} å·²å®Œæˆ, shape={preds.shape}")


        print(f"\nåˆä½µå¾Œçš„äººé¡å¯è®€ç‰ˆçµæœå·²è¼¸å‡ºåˆ°ï¼š{txt_path}")

    elif RUN_SECOND_CLASSIFIER:
        # === å­˜æˆ JSON ===
        with open(f"{OUTPUT_PATH}/logreg_train_daily_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json", "w", encoding="utf-8") as f:
            json.dump(train_daily, f, ensure_ascii=False, indent=4, default=int)

        with open(f"{OUTPUT_PATH}/logreg_test_daily_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json", "w", encoding="utf-8") as f:
            json.dump(test_daily, f, ensure_ascii=False, indent=4, default=int)

        print("å·²è¼¸å‡ºé€æ—¥é æ¸¬çµæœï¼š")
        print(f"- train: {OUTPUT_PATH}/logreg_train_daily_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json")
        print(f"- test:  {OUTPUT_PATH}/logreg_test_daily_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json")

        # === åˆä½µ train + test ===
        combined_daily = {}
        for coin, daily in train_daily.items():
            combined_daily.setdefault(coin, {}).update(daily)
        for coin, daily in test_daily.items():
            combined_daily.setdefault(coin, {}).update(daily)

        # === å­˜æˆåˆä½µå¾Œçš„ TXT ===
        txt_path = f"{OUTPUT_PATH}/logreg_combined_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.txt"
        with open(txt_path, "w", encoding="utf-8") as f:
            label_correct = 0
            label_total = 0

            for coin, daily in combined_daily.items():
                f.write(f"\n=== {coin} ===\n")

                records = []
                for date, stats in sorted(daily.items()):
                    # --- æ¯æ—¥è¼¸å‡ºåˆ° TXT ---
                    line = (
                        f"{date} â†’ "
                        f"é æ¸¬: {''.join(stats['pred_symbols'])}  "
                        f"çœŸå¯¦: {''.join(stats['true_symbols'])}  "
                        f"çµæœ: {''.join(stats['result_symbols'])}\n"
                    )
                    f.write(line)

                    # --- æ›´æ–°ç´¯ç©æº–ç¢ºç‡ ---
                    label_total += 1
                    if stats["majority_correct"]:
                        label_correct += 1

                    # --- ä¿å­˜æ¯æ—¥é æ¸¬é¡åˆ¥ ---
                    records.append((date, stats["majority_pred"]))

                # --- è¼¸å‡ºæ•´é«”æº–ç¢ºç‡ ---
                acc = (label_correct / label_total * 100) if label_total > 0 else 0
                f.write(f"\næ•´é«”æº–ç¢ºç‡: {acc:.2f}%\n")

        print(f"\nåˆä½µå¾Œçš„äººé¡å¯è®€ç‰ˆçµæœå·²è¼¸å‡ºåˆ°ï¼š{txt_path}")



def predict_august_function(pipeline_path):
    combined_daily = {}  # ç”¨ä¾†æ”¾ åˆä½µ ä¸‰ç¨®å¹£ç¨® çš„è³‡æ–™ ===

    # --- è¼‰å…¥è³‡æ–™ ---
    for coin_short_name in ['DOGE', 'PEPE', 'TRUMP']:
        if RUN_FIRST_CLASSIFIER:
            X_august = sparse.load_npz(f'{INPUT_PATH}/keyword/{coin_short_name}_X_sparse{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npz')
            y_august = np.load(f'{INPUT_PATH}/coin_price/{coin_short_name}_price_diff{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy')
            with open(f'{INPUT_PATH}/keyword/{coin_short_name}_ids{SUFFIX_FILTERED}{SUFFIX_AUGUST}.pkl', 'rb') as file:
                ids_august = pickle.load(file)

        elif RUN_SECOND_CLASSIFIER:
            X_august = np.load(f"{INPUT_PATH}/keyword/{coin_short_name}_{MODEL_NAME}_X_classifier_2{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy")
            y_august = np.load(f"{INPUT_PATH}/coin_price/{coin_short_name}_price_diff_original{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy")
            with open(f"{INPUT_PATH}/keyword/{coin_short_name}_{MODEL_NAME}_ids_classifier_2{SUFFIX_FILTERED}{SUFFIX_AUGUST}.pkl", 'rb') as file:
                ids_august = pickle.load(file)

        y_august_categorized = categorize_array_multi(y_august, T1, T2, T3, T4)

        # === è¼‰å…¥æœ€ä½³æ¨¡å‹ ===
        pipeline = joblib.load(pipeline_path)
        model = pipeline["model"]
        
        # === é æ¸¬æ‰€æœ‰æ¨£æœ¬ ===
        y_pred_august = model.predict(X_august)
        print(y_pred_august.shape)

        # å°‡ ids è½‰æˆ np.array æ–¹ä¾¿æ¥ä¸‹ä¾†çš„è™•ç†
        ids_august = np.array(ids_august)

        
        print(f"\nåˆ†é¡å ±å‘Š ({coin_short_name} August set):")
        print(classification_report(y_august_categorized, y_pred_august, zero_division=0))

        # august_score = knn.score(X_august, Y_august)
        print(f'{coin_short_name} August accuracy')  

        print("ids_august[:5]", ids_august[:5])

        august_daily, _ = evaluate_by_coin_date(ids_august, y_august_categorized, y_pred_august)

        if RUN_FIRST_CLASSIFIER:
            # === å­˜æˆ JSON ===
            with open(f"{OUTPUT_PATH}/{coin_short_name}_logreg_august_daily_results_{N_SAMPLES}{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json", "w", encoding="utf-8") as f:
                json.dump(august_daily, f, ensure_ascii=False, indent=4, default=int)

            print("å·²è¼¸å‡ºé€æ—¥é æ¸¬çµæœï¼š")
            print(f"- august: {OUTPUT_PATH}/{coin_short_name}_logreg_august_daily_results_{N_SAMPLES}{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json")

            # === åˆä½µ ä¸‰ç¨®å¹£ç¨® ===
            for coin, daily in august_daily.items():
                combined_daily.setdefault(coin, {}).update(daily)

            # === å­˜æˆåˆä½µå¾Œçš„ TXT ===
            txt_path = f"{OUTPUT_PATH}/logreg_combined_results_{N_SAMPLES}{SUFFIX_FILTERED}{SUFFIX_AUGUST}.txt"
            with open(txt_path, "w", encoding="utf-8") as f:
                # === åˆå§‹åŒ–çµ±è¨ˆå™¨ ===
                label_correct = np.zeros(1, dtype=int)
                label_total   = np.zeros(1, dtype=int)

                for coin, daily in combined_daily.items():
                    f.write(f"\n=== {coin} ===\n")

                    # ç”¨ä¾†å­˜æ”¾æ¯å¤©çš„ (date, pred_class)
                    records = []

                    for date, stats in sorted(daily.items()):
                        # --- æ¯æ—¥è¼¸å‡ºåˆ° TXT ---
                        class_str = " ".join(f"{x:5d}" for x in stats['class_counts'])
                        line = (
                            f"{date} â†’ ğŸ“Š {class_str}  "
                            f"ç¸½æ•¸: {stats['total_counts']:5d}  "
                            f"é æ¸¬: {''.join(stats['pred_symbols'])}  "
                            f"çœŸå¯¦: {''.join(stats['true_symbols'])}  "
                            f"çµæœ: {''.join(stats['result_symbols'])}\n"
                        )
                        f.write(line)

                        # --- æ›´æ–°ç´¯ç©æº–ç¢ºç‡ ---
                        label_total[0] += 1
                        if stats["majority_correct"]:
                            label_correct[0] += 1

                        # --- å–ç•¶å¤©é æ¸¬é¡åˆ¥ (class_counts æœ€å¤§çš„ index) ---
                        pred_class = int(np.argmax(stats["class_counts"]))
                        records.append((date, pred_class))

                    # --- è¼¸å‡ºæ•´é«”æº–ç¢ºç‡ (ç™¾åˆ†æ¯”) ---
                    accuracy_summary = " ".join(
                        f"{(c / t * 100):.2f}%" if t > 0 else "N/A"
                        for c, t in zip(label_correct, label_total)
                    )
                    f.write(f"\næ•´é«”æº–ç¢ºç‡: {accuracy_summary}\n")

                    # === å­˜æˆ .npy (æ¯æ—¥é æ¸¬çµæœï¼Œä¾æ—¥æœŸæ’åº) ===
                    if records:
                        records.sort(key=lambda x: x[0])
                        _, preds = zip(*records)
                        preds = np.array(preds, dtype=np.int32)

                        npy_path = f"{OUTPUT_PATH}/{coin}_logreg_classifier_1_result{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy"
                        np.save(npy_path, preds)
                        print(preds[:50])
                        print(f"{coin} â†’ {npy_path} å·²å®Œæˆ, shape={preds.shape}")


            print(f"\nåˆä½µå¾Œçš„äººé¡å¯è®€ç‰ˆçµæœå·²è¼¸å‡ºåˆ°ï¼š{txt_path}")

        elif RUN_SECOND_CLASSIFIER:

            # === å­˜æˆ JSON ===
            with open(f"{OUTPUT_PATH}/{coin_short_name}_logreg_train_daily_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json", "w", encoding="utf-8") as f:
                json.dump(august_daily, f, ensure_ascii=False, indent=4, default=int)

            print("å·²è¼¸å‡ºé€æ—¥é æ¸¬çµæœï¼š")
            print(f"- august: {OUTPUT_PATH}/{coin_short_name}_logreg_train_daily_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json")

            # === åˆä½µ ä¸‰ç¨®å¹£ç¨® ===
            for coin, daily in august_daily.items():
                combined_daily.setdefault(coin, {}).update(daily)

            # === å­˜æˆåˆä½µå¾Œçš„ TXT ===
            txt_path = f"{OUTPUT_PATH}/logreg_combined_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.txt"
            with open(txt_path, "w", encoding="utf-8") as f:
                label_correct = 0
                label_total = 0

                for coin, daily in combined_daily.items():
                    f.write(f"\n=== {coin} ===\n")

                    records = []
                    for date, stats in sorted(daily.items()):
                        # --- æ¯æ—¥è¼¸å‡ºåˆ° TXT ---
                        line = (
                            f"{date} â†’ "
                            f"é æ¸¬: {''.join(stats['pred_symbols'])}  "
                            f"çœŸå¯¦: {''.join(stats['true_symbols'])}  "
                            f"çµæœ: {''.join(stats['result_symbols'])}\n"
                        )
                        f.write(line)

                        # --- æ›´æ–°ç´¯ç©æº–ç¢ºç‡ ---
                        label_total += 1
                        if stats["majority_correct"]:
                            label_correct += 1

                        # --- ä¿å­˜æ¯æ—¥é æ¸¬é¡åˆ¥ ---
                        records.append((date, stats["majority_pred"]))

                    # --- è¼¸å‡ºæ•´é«”æº–ç¢ºç‡ ---
                    acc = (label_correct / label_total * 100) if label_total > 0 else 0
                    f.write(f"\næ•´é«”æº–ç¢ºç‡: {acc:.2f}%\n")

            print(f"\nåˆä½µå¾Œçš„äººé¡å¯è®€ç‰ˆçµæœå·²è¼¸å‡ºåˆ°ï¼š{txt_path}")



def categorize_array_multi(Y, t1, t2, t3, t4, ids=None):
    """
    Y: np.ndarray, shape = (num_labels,), åƒ¹æ ¼è®ŠåŒ–ç‡
    t1, t2: äº”å…ƒåˆ†é¡é–¾å€¼ï¼Œç™¾åˆ†æ¯”
    """

    print(Y.shape)
    # print(len(ids))

    # äº”å…ƒåˆ†é¡
    labels = np.full_like(Y, 2, dtype=int)  # é è¨­æŒå¹³
    labels[Y <= -t1] = 0  # å¤§è·Œ
    labels[(Y > -t1) & (Y <= -t2)] = 1  # è·Œ
    labels[(Y >= t3) & (Y < t4)] = 3  # æ¼²
    labels[Y >= t4] = 4  # å¤§æ¼²

    if ids is not None:
        # æ‰¾å‡º Y==0 çš„ç´¢å¼•
        zero_idx = np.where(Y == 0)[0]
        # åªå–å°æ‡‰çš„ ids
        dates_is_0 = set((ids[i][0], ids[i][1]) for i in zero_idx)
        if len(dates_is_0) > 0:
            print(f"å…±æœ‰ {len(dates_is_0)} å¤© Y==0")
            for id in sorted(dates_is_0):
                print(id)

    if np.any(Y == 0):  # æª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½•å…ƒç´ ç­‰æ–¼ 0
        count = np.sum(Y == 0)
        print(f"å…±æœ‰ {count} å€‹ Y == 0")
        labels[Y == 0] = 4  # ç‚ºäº†æ ¡æ­£ TRUMP å‰å…©å¤©çš„åƒ¹æ ¼ç›¸åŒ ç¬¬ä¸€å¤©è¨­ç‚ºå¤§æ¼²

    return labels



def load_and_preprocess():
    if RUN_FIRST_CLASSIFIER:
        # å–å¾— ML çš„ X
        X_train = sparse.load_npz(f"{INPUT_PATH}/X_train{SUFFIX_FILTERED}.npz")
        X_test = sparse.load_npz(f"{INPUT_PATH}/X_test{SUFFIX_FILTERED}.npz")

        print(X_train.shape)

        # åŒ¯å…¥ Y
        y_train = np.load(f"{INPUT_PATH}/Y_train{SUFFIX_FILTERED}.npz")
        y_train = y_train['Y']
        y_test = np.load(f"{INPUT_PATH}/Y_test{SUFFIX_FILTERED}.npz")
        y_test = y_test['Y']

        print(y_train.shape)

        with open(f"{INPUT_PATH}/ids_train{SUFFIX_FILTERED}.pkl", 'rb') as file:
            ids_train = pickle.load(file)
        with open(f"{INPUT_PATH}/ids_test{SUFFIX_FILTERED}.pkl", 'rb') as file:
            ids_test = pickle.load(file)

        scaler = StandardScaler(with_mean=False)  # é©åˆ sparse matrix
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        # å–å¾— all_keywords(features_name)
        with open(f"{INPUT_PATH}/keyword/filtered_keywords{SUFFIX_FILTERED}.json", "r", encoding="utf-8-sig") as jsonfile:
            features_name = json.load(jsonfile)


        # # å–å¾— price çš„ csv æª”
        # price_path = "../data/coin_price"
        # df = pd.read_csv(f"{price_path}/{COIN_SHORT_NAME}_current_tweet_price_output.csv")
        # df['date'] = pd.to_datetime(df['date'], format="%Y/%m/%d")  # æŠŠ date æ¬„ä½è½‰æˆæ—¥æœŸæ ¼å¼

        # # æŠŠç•¶å¤©æ²’æœ‰æŠ“åˆ°æ¨æ–‡çš„æ—¥æœŸå­˜èµ·ä¾†
        # unprocessed_dates = []
        # for i in range(len(df)):
        #     if df.loc[i, "has_tweet"] == False:
        #         unprocessed_dates.append(df.loc[i, "date"].strftime("%Y/%m/%d"))



        # === ç‰¹å¾µé¸æ“‡ ===
        # parser = argparse.ArgumentParser()
        # parser.add_argument("--fs", type=str, default="none", help="Feature selection method")
        # parser.add_argument("--k", type=int, default=600, help="Top k features")
        # args = parser.parse_args()

        # selector = make_selector(task="clf", method=args.fs, k=args.k)
        # if selector is not None:
        #     X_train = selector.fit_transform(X_train, y_train_categorized)
        #     X_test = selector.transform(X_test)
        #     features_name = selector.get_feature_names_out(features_name)  # æ›´æ–° features_name
        #     print(f"[INFO] Feature selection ({args.fs}) done, X_train shape = {X_train.shape}")
    
    elif RUN_SECOND_CLASSIFIER:
        # å–å¾—è³‡æ–™
        X = np.load(f"{INPUT_PATH}/{MODEL_NAME}_X_classifier_2{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy")
        y = np.load(f"{INPUT_PATH}/{MODEL_NAME}_Y_classifier_2{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy")
        with open(f"{INPUT_PATH}/{MODEL_NAME}_ids_classifier_2{SUFFIX_FILTERED}{SUFFIX_AUGUST}.pkl", 'rb') as file:
            ids = pickle.load(file)

        X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(
            X, y, ids, test_size=0.2, random_state=42, shuffle=True
        )

        # X_train, y_train, ids_train = balance_train_data(X_train, y_train, ids_train)

        # # å»ºç«‹ target labelï¼šäº”å…ƒåˆ†é¡
        # y_categorized = categorize_array_multi(y, ids, T1, T2, T3, T4)  # shape (N,)
        # print("å·²æˆåŠŸåˆ†é¡åˆ¥")

        # # åˆ†å‰²æˆ Train / Test
        # X_train, X_test, y_train_categorized, y_test_categorized, ids_train, ids_test = stratified_train_test_balance(
        #     X, y_categorized, ids, max_per_class=303
        # )

        print("X_test shape:", X_test.shape)
        print("y_test shape:", y_test.shape)


        scaler = None
        features_name = None

        print("X_train shape:", X_train.shape)
        print("X_test shape:", X_test.shape)
        print("y_train shape:", y_train.shape)
        print("y_test shape:", y_test.shape)
        print("Train IDs count:", len(ids_train))
        print("Test IDs count:", len(ids_test))

    else:
        raise ValueError("å¿…é ˆæŒ‡å®š run_first_classifier æˆ– run_second_classifier")
    
    # å»ºç«‹ target labelï¼šäº”å…ƒåˆ†é¡
    y_train_categorized = categorize_array_multi(y_train, T1, T2, T3, T4, ids_train)  # shape (N,)
    y_test_categorized  = categorize_array_multi(y_test, T1, T2, T3, T4, ids_test)   # shape (N,)
    print("å·²æˆåŠŸåˆ†é¡åˆ¥")

    # çµ±è¨ˆæ¯å€‹é¡åˆ¥æ•¸é‡
    print(f"å¤§è·Œï¼š-{T1 * 100:.2f}%ä»¥ä¸‹, è·Œï¼š-{T1 * 100:.2f}% ~ -{T2 * 100}%, æŒå¹³ï¼š-{T2 * 100}% ~ {T3 * 100}%, æ¼²ï¼š{T3 * 100}% ~ {T4 * 100:.2f}%, å¤§æ¼²ï¼š{T4 * 100:.2f}%ä»¥ä¸Š")
    train_total_row = y_train_categorized.shape[0]
    test_total_row = y_test_categorized.shape[0]
    # for col in range(y_train_categorized.shape[1]):
    counts = np.bincount(y_train_categorized, minlength=5)
    percentages = counts / train_total_row * 100
    percentages_str = " ".join([f"{p:.2f}%" for p in percentages])
    print(f"[TRAIN] column é¡åˆ¥: {percentages_str}")

    counts = np.bincount(y_test_categorized, minlength=5)
    percentages = counts / test_total_row * 100
    percentages_str = " ".join([f"{p:.2f}%" for p in percentages])
    print(f"[TEST]  column é¡åˆ¥: {percentages_str}\n")

    input("pasue...")

    return X_train, X_test, y_train_categorized, y_test_categorized, ids_train, ids_test, scaler, features_name



def main():

    if RUN_FIRST_CLASSIFIER:

        pipeline_path = f"{SAVE_MODEL_PATH}/logreg_best_pipeline_{N_SAMPLES}{SUFFIX_FILTERED}.joblib"  # å„²å­˜è¨“ç·´æ¨¡å‹çš„ä½ç½®

        if not IS_RUN_AUGUST:
            # --- è¼‰å…¥è³‡æ–™ ---
            X_train, X_test, y_train, y_test, ids_train, ids_test, scaler, features_name = load_and_preprocess()

            # for count in range(LABELS):

            if IS_TRAIN:
                # --- è¨“ç·´æ¨¡å‹ --- 
                train_function(X_train, X_test, y_train, y_test, pipeline_path, scaler, features_name)

                # --- é æ¸¬æ¨¡å‹ ---
                predict_function(X_train, X_test, y_train, y_test, ids_train, ids_test, pipeline_path)
            else:
                if not os.path.exists(pipeline_path):
                    print("æ‰¾ä¸åˆ°å·²è¨“ç·´å¥½çš„ ç¬¬ä¸€å€‹åˆ†é¡å™¨ æ¨¡å‹ï¼Œè«‹å…ˆå°‡ IS_TRAIN è¨­ç‚º True")

                # --- é æ¸¬æ¨¡å‹ ---
                predict_function(X_train, X_test, y_train, y_test, ids_train, ids_test, pipeline_path)

        else:
            # --- é æ¸¬ 2025-08 ---
            predict_august_function(pipeline_path)

    elif RUN_SECOND_CLASSIFIER:

        pipeline_path = f"{SAVE_MODEL_PATH}/logreg_classifier_2{SUFFIX_FILTERED}.joblib"  # å„²å­˜è¨“ç·´æ¨¡å‹çš„ä½ç½®

        if not IS_RUN_AUGUST:
            if IS_GROUPED_CV == False:
                # --- è¼‰å…¥è³‡æ–™ ---
                X_train, X_test, y_train, y_test, ids_train, ids_test, _, _ = load_and_preprocess()

                if IS_TRAIN:
                    # --- è¨“ç·´æ¨¡å‹ --- 
                    train_function(X_train, X_test, y_train, y_test, pipeline_path)

                    # --- é æ¸¬æ¨¡å‹ ---
                    predict_function(X_train, X_test, y_train, y_test, ids_train, ids_test, pipeline_path)
                else:
                    if not os.path.exists(pipeline_path):
                        print("æ‰¾ä¸åˆ°å·²è¨“ç·´å¥½çš„ ç¬¬äºŒå€‹åˆ†é¡å™¨ æ¨¡å‹ï¼Œè«‹å…ˆå°‡ IS_TRAIN è¨­ç‚º True")

                    # --- é æ¸¬æ¨¡å‹ ---
                    predict_function(X_train, X_test, y_train, y_test, ids_train, ids_test, pipeline_path)

            else:
                # å–å¾—è³‡æ–™
                X = np.load(f"{INPUT_PATH}/{MODEL_NAME}_X_classifier_2{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy")
                y = np.load(f"{INPUT_PATH}/{MODEL_NAME}_Y_classifier_2{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy")
                with open(f"{INPUT_PATH}/{MODEL_NAME}_ids_classifier_2{SUFFIX_FILTERED}{SUFFIX_AUGUST}.pkl", 'rb') as file:
                    ids = pickle.load(file)

                y_categorized = categorize_array_multi(y, ids, T1, T2, T3, T4)  # shape (N,)

                results_all = coin_month_cv(X, y_categorized, ids, C=C)

        else:
            # --- é æ¸¬ 2025-08 ---
            predict_august_function(pipeline_path)  



if __name__ == "__main__":
    main()