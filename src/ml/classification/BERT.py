from datetime import datetime
import gc
import os
import json
import pickle
import numpy as np
from glob import glob
import random
from scipy import sparse
from sklearn.discriminant_analysis import StandardScaler
from tqdm import tqdm
import sys
import math
import pandas as pd

from collections import Counter, defaultdict

from sklearn.metrics import accuracy_score, classification_report

import torch
from torch.utils.data import Dataset, Subset
from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification
import transformers

from sklearn.model_selection import train_test_split

import joblib




'''å¯ä¿®æ”¹è®Šæ•¸'''
N_SAMPLES = 1000000  # 1_000_000  # random sampling å–çš„æ•¸é‡

N_RUNS = 1

NUM_CATEGORIES = 5  # é¡åˆ¥æ•¸é‡

EPOCHS = 5

T1 = 0.0590 # 0.1

T2 = 0.0102 # 0.00125
 
T3 = 0.0060

T4 = 0.0657

START_DATE = {"DOGE": "2013/12/15", "PEPE": "2024/02/01", "TRUMP": "2025/01/18"}

END_DATE   = {"DOGE": "2025/07/31", "PEPE": "2025/07/31", "TRUMP": "2025/07/31"}

COIN_SHORT_NAME = ["DOGE", "PEPE", "TRUMP"]

JSON_DICT_NAME = ["dogecoin", "PEPE", "(officialtrump OR \"official trump\" OR \"trump meme coin\" OR \"trump coin\" OR trumpcoin OR $TRUMP OR \"dollar trump\")"]

# PRICE_CSV_PATH = "../data/coin_price"

INPUT_PATH = "../data/ml/dataset"

OUTPUT_PATH = "../data/ml/classification/BERT"

SAVE_MODEL_PATH = "../data/ml/models/BERT"

MODEL_NAME = ["logistic_regression", "logreg"]  # ç¬¬äºŒå€‹åˆ†é¡å™¨ç›®å‰è¼¸å…¥çš„æ¨¡å‹åå­—(æœªå®Œæˆ)

BERT_MODEL_NAME = "google/bert_uncased_L-2_H-128_A-2"           
# "bert-base-uncased"                   110M
# "distilbert-base-uncased",            66M
# "google/bert_uncased_L-2_H-128_A-2"   4M

RUN_FIRST_CLASSIFIER = True  # æ˜¯å¦è¦è·‘ç¬¬ä¸€å€‹åˆ†é¡å™¨

RUN_SECOND_CLASSIFIER = False  # æ˜¯å¦è¦è·‘ç¬¬äºŒå€‹åˆ†é¡å™¨(æœªå®Œæˆ)

IS_GROUPED_CV = False  # æ˜¯å¦è¦è·‘ç¬¬äºŒå€‹åˆ†é¡å™¨çš„äº¤å‰é©—è­‰(æœªå®Œæˆ)

IS_TRAIN = True  # çœ‹æ˜¯å¦è¦è¨“ç·´

IS_FILTERED = True  # çœ‹æ˜¯å¦æœ‰åˆ† normal èˆ‡ bot

IS_RUN_AUGUST = False  # çœ‹ç¾åœ¨æ˜¯ä¸æ˜¯è¦è·‘ 2025/08 çš„è³‡æ–™(æœªå®Œæˆ)
'''å¯ä¿®æ”¹è®Šæ•¸'''

os.makedirs(OUTPUT_PATH, exist_ok=True)
os.makedirs(SAVE_MODEL_PATH, exist_ok=True)

SUFFIX_FILTERED = "" if IS_FILTERED else "_non_filtered"
SUFFIX_AUGUST   = "_202508" if IS_RUN_AUGUST else ""

# è½‰æˆ datetime æ–¹ä¾¿æ¯”è¼ƒ
START_DATE_DT = {k: pd.to_datetime(v, format="%Y/%m/%d") for k, v in START_DATE.items()}
END_DATE_DT   = {k: pd.to_datetime(v, format="%Y/%m/%d") for k, v in END_DATE.items()}



# --- è®€å–æª”æ¡ˆ (åªè™•ç† normal, non_filtered) ---
def load_and_preprocess():
    if RUN_FIRST_CLASSIFIER:
        X_train = []
        X_test = []
        y_train = []
        y_test = []
        ids_train = []
        ids_test = []

        for coin_short_name, json_dict_name in zip(COIN_SHORT_NAME, JSON_DICT_NAME):
            print(f"====== ç›®å‰åœ¨è™•ç† {coin_short_name} ======")

            # è®€å– price_diff ä½œç‚º y
            y_single_coin = np.load(f"{INPUT_PATH}/y_input/{coin_short_name}/{coin_short_name}_price_diff{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy")
            print("y_single_coin.shape[0]:", y_single_coin.shape[0])

            with open(f"{INPUT_PATH}/ids_input/{coin_short_name}/{coin_short_name}_ids{SUFFIX_FILTERED}{SUFFIX_AUGUST}.pkl", "rb") as f:   # è®€å–ä¸€é–‹å§‹è¨“ç·´ç”¨çš„ ids
                ids_single_coin = pickle.load(f)
                print("len(ids_single_coin):", len(ids_single_coin))
            
            # dates_single_coin = [(c, d) for (c, d, _) in ids_single_coin]  # åªå– date åŠ å…¥é›†åˆ
            # dates_single_coin = pd.to_datetime(dates_single_coin, format="%Y-%m-%d")

            # å…ˆæŠŠ dates_single_coint åªä¿ç•™ç•¶å‰å¹£ç¨®çš„æ—¥æœŸ
            # dates_single_coin = set([d for (c, d) in dates_single_coin if c == coin_short_name])
            # dates_single_coin = sorted(dates_single_coin)


            print("y_single_coin[:10]\n", y_single_coin[:10])
            print("ids_single_coin[:10]\n", ids_single_coin[:10])
            print()


            # è®€å– åŸå§‹æ¨æ–‡ text
            origianl_single_coin_tweet_text = []  # (N, 2) = (æ¨£æœ¬æ•¸, (text, date))
            IS_READ_TWEET = input("æ˜¯å¦è¦é‡æ–°è®€å–åŸå§‹æ¨æ–‡â“(Y / N):")
            if IS_READ_TWEET == "N":
                origianl_single_coin_tweet_text_path = f"{OUTPUT_PATH}/original_tweets/{coin_short_name}_original_tweets.pkl"
                if os.path.exists(origianl_single_coin_tweet_text_path):
                    print(f"âœ… {coin_short_name} çš„åŸå§‹æ¨æ–‡å­˜åœ¨")
                    with open(origianl_single_coin_tweet_text_path, "rb") as f:   # è®€å–ä¸€é–‹å§‹è¨“ç·´ç”¨çš„ ids
                        origianl_single_coin_tweet_text = pickle.load(f)
                    # print(origianl_single_coin_tweet_text[:10])
                else:
                    print(f"âŒ {coin_short_name} çš„åŸå§‹æ¨æ–‡ä¸å­˜åœ¨ï¼Œå¿…é ˆè¦è®€å–åŸå§‹æ¨æ–‡")
                    IS_READ_TWEET = "Y"

            if IS_READ_TWEET == "Y":
                if IS_FILTERED:
                    tweets_path = f"../data/filtered_tweets/normal_tweets/{coin_short_name}/*/*/{coin_short_name}_*_normal.json"
                else:
                    tweets_path = f"../data/tweets/{coin_short_name}/*/*/{coin_short_name}_*.json"

                original_tweets_file = glob(tweets_path)
                for file in tqdm(original_tweets_file, desc=f"è®€å– {coin_short_name} çš„åŸå§‹æ¨æ–‡èˆ‡æ—¥æœŸ..."):
                    with open(file, "r", encoding="utf-8-sig") as fp:
                        data = json.load(fp)
                    
                    tweets_single_coin = data[json_dict_name]
                    if not tweets_single_coin:
                        print("ç•¶å¤©æ²’æœ‰æ¨æ–‡ï¼š", file)
                        continue

                    # å–å¾—æ—¥æœŸ
                    date_str = datetime.strptime(
                        tweets_single_coin[0]['created_at'], "%a %b %d %H:%M:%S %z %Y"
                    ).strftime("%Y-%m-%d")
                    date_dt = pd.to_datetime(date_str)

                    # ğŸ”¹ éæ¿¾æ‰ä¸åœ¨ç¯„åœå…§çš„æ¨æ–‡
                    if not (START_DATE_DT[coin_short_name] <= date_dt <= END_DATE_DT[coin_short_name]):
                        print("ç•¶å¤©ä¸åœ¨æŒ‡å®šæ™‚é–“ç¯„åœå…§ï¼š", file)
                        continue

                    nos_single_coin_one_day = set([int(item[2]) for item in ids_single_coin if item[0] == coin_short_name and item[1] == date_str])
                    # print("\nlen(nos_single_coin_one_day):", len(nos_single_coin_one_day))
                    # print("nos_single_coin_one_day[:10]:\n", sorted(nos_single_coin_one_day)[:1000])

                    # print()
                    # input()
                    # å„²å­˜ åŸå§‹æ¨æ–‡, æ—¥æœŸ(datetime)
                    in_count = 0
                    non_count = 0
                    for tweet in tweets_single_coin:
                        if tweet["tweet_count"] in nos_single_coin_one_day:
                            in_count += 1
                            origianl_single_coin_tweet_text.append([tweet["text"], date_dt])
                    #     else:
                    #         non_count += 1
                    #         if coin_short_name == "TRUMP":
                    #             print(f"date: {date_str}, tweet_count: {tweet["tweet_count"]} çš„æ¨æ–‡ä¸åœ¨ ids ä¸­")
                                
                    # print("in_count =", in_count)
                    # print("non_count =", non_count)
                    

                # print(origianl_single_coin_tweet_text[:10])
                save_single_original_tweets_path = f"{OUTPUT_PATH}/original_tweets"
                os.makedirs(save_single_original_tweets_path, exist_ok=True)
                with open(f"{save_single_original_tweets_path}/{coin_short_name}_original_tweets.pkl", 'wb') as file:
                    pickle.dump(origianl_single_coin_tweet_text, file)
                print(f"âœ… {coin_short_name} çš„åŸå§‹æ¨æ–‡å·²å®Œæˆå„²å­˜")

            elif IS_READ_TWEET != "N":
                raise TypeError("è¼¸å…¥éŒ¯èª¤")
            
            print("len(origianl_single_coin_tweet_text):", len(origianl_single_coin_tweet_text))
                
            # --- è®€å– merge_and_splitset ä¸­å·²ç¶“åˆ‡å¥½è³‡æ–™é›†çš„ æ—¥æœŸ ---
            # è®€å– Train
            df_split_train = pd.read_csv(f"{INPUT_PATH}/split_dates/{coin_short_name}_train_dates{SUFFIX_FILTERED}.csv")
            df_split_train['date'] = pd.to_datetime(df_split_train['date'], format="%Y-%m-%d")  # æŠŠ date æ¬„ä½è½‰æˆæ—¥æœŸæ ¼å¼

            # è®€å– Test, Val ä¸¦æŠŠå…©å€‹åˆä½µ
            df_split_test = pd.read_csv(f"{INPUT_PATH}/split_dates/{coin_short_name}_test_dates{SUFFIX_FILTERED}.csv")
            df_split_test['date'] = pd.to_datetime(df_split_test['date'], format="%Y-%m-%d")  # æŠŠ date æ¬„ä½è½‰æˆæ—¥æœŸæ ¼å¼

            # df_split_val = pd.read_csv(f"{INPUT_PATH}/split_dates/{coin_short_name}_val_dates{SUFFIX_FILTERED}.csv")
            # df_split_val['date'] = pd.to_datetime(df_split_val['date'], format="%Y-%m-%d")  # æŠŠ date æ¬„ä½è½‰æˆæ—¥æœŸæ ¼å¼

            # df_split_test = pd.concat([df_split_only_test, df_split_val], ignore_index=True)


            # æŠŠ train/test/val çš„æ—¥æœŸé›†åˆåŒ–ï¼ŒåŠ é€ŸæŸ¥è©¢  åˆ‡å‰²è³‡æ–™é›†
            train_dates = set(df_split_train["date"])
            test_dates = set(df_split_test["date"])

            # åˆ‡å‰²è³‡æ–™é›†
            for (text, tweet_date), price_diff, (coin, ids_date, ids_idx) in zip(origianl_single_coin_tweet_text, y_single_coin, ids_single_coin):
                if tweet_date in train_dates:
                    X_train.append(text)
                    y_train.append(price_diff)  # é€™è£¡è¦å°æ‡‰ y_single_coin
                    ids_train.append([coin, ids_date, ids_idx])

                elif tweet_date in test_dates:
                    X_test.append(text)
                    y_test.append(price_diff)
                    ids_test.append([coin, ids_date, ids_idx])

            # mask_train = [date in df_split_train["date"] for date in origianl_single_coin_tweet_text[0]]
            # mask_test = [date in df_split_test["date"] for date in origianl_single_coin_tweet_text[0]]

            # X_train += origianl_single_coin_tweet_text[mask_train]
            # X_test += origianl_single_coin_tweet_text[mask_test]



            print("len(X_train):", len(X_train))
            print("len(X_test):", len(X_test))
            print("len(y_train):", len(y_train))
            print("len(y_test):", len(y_test))
            print("len(ids_train):", len(ids_train))
            print("len(ids_test):", len(ids_test))

            print(f"\nå·²æˆåŠŸåˆ‡å‰² {coin_short_name} çš„è³‡æ–™é›†\n")


        X_train = np.array(X_train)
        X_test = np.array(X_test)
        y_train = np.array(y_train)
        y_test = np.array(y_test)

        print("\nåˆä½µå®Œæˆå¾Œçš„å½¢ç‹€:")
        print("X_train.shape:", X_train.shape)
        print("X_test.shape:", X_test.shape)
        print("y_train.shape:", y_train.shape)
        print("y_test.shape:", y_test.shape)
        print("len(ids_train):", len(ids_train))
        print("len(ids_test):", len(ids_test))

        input("\næŒ‰ Enter ä»¥ç¹¼çºŒ...")
        
    
    elif RUN_SECOND_CLASSIFIER:
        # å–å¾—è³‡æ–™
        X = np.load(f"{INPUT_PATH}/final_input/price_classifier/{MODEL_NAME[0]}/{MODEL_NAME[1]}_X_classifier_2{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy")
        y = np.load(f"{INPUT_PATH}/final_input/price_classifier/{MODEL_NAME[0]}/{MODEL_NAME[1]}_Y_classifier_2{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy")
        with open(f"{INPUT_PATH}/final_input/price_classifier/{MODEL_NAME[0]}/{MODEL_NAME[1]}_ids_classifier_2{SUFFIX_FILTERED}{SUFFIX_AUGUST}.pkl", 'rb') as file:
            ids = pickle.load(file)

        X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(
            X, y, ids, test_size=0.2, random_state=42, shuffle=True
        )

        print("X_test shape:", X_test.shape)
        print("y_test shape:", y_test.shape)

        print("X_train shape:", X_train.shape)
        print("X_test shape:", X_test.shape)
        print("y_train shape:", y_train.shape)
        print("y_test shape:", y_test.shape)
        print("Train IDs count:", len(ids_train))
        print("Test IDs count:", len(ids_test))

    else:
        raise ValueError("å¿…é ˆæŒ‡å®š run_first_classifier æˆ– run_second_classifier")
    
    # å»ºç«‹ target labelï¼šäº”å…ƒåˆ†é¡
    y_train_categorized = categorize_array_multi(y_train, T1, T2, T3, T4, ids_train)  # shape (N,)
    y_test_categorized  = categorize_array_multi(y_test, T1, T2, T3, T4, ids_test)   # shape (N,)
    print("å·²æˆåŠŸåˆ†é¡åˆ¥")

    # çµ±è¨ˆæ¯å€‹é¡åˆ¥æ•¸é‡
    print(f"å¤§è·Œï¼š-{T1 * 100:.2f}%ä»¥ä¸‹, è·Œï¼š-{T1 * 100:.2f}% ~ -{T2 * 100}%, æŒå¹³ï¼š-{T2 * 100}% ~ {T3 * 100}%, æ¼²ï¼š{T3 * 100}% ~ {T4 * 100:.2f}%, å¤§æ¼²ï¼š{T4 * 100:.2f}%ä»¥ä¸Š")
    train_total_row = y_train_categorized.shape[0]
    test_total_row = y_test_categorized.shape[0]
    # for col in range(y_train_categorized.shape[1]):
    counts = np.bincount(y_train_categorized, minlength=5)
    percentages = counts / train_total_row * 100
    percentages_str = " ".join([f"{p:.2f}%" for p in percentages])
    print(f"[TRAIN] column é¡åˆ¥: {percentages_str}")

    counts = np.bincount(y_test_categorized, minlength=5)
    percentages = counts / test_total_row * 100
    percentages_str = " ".join([f"{p:.2f}%" for p in percentages])
    print(f"[TEST]  column é¡åˆ¥: {percentages_str}\n")

    input("pasue...")

    return X_train, X_test, y_train_categorized, y_test_categorized, ids_train, ids_test



# --- äº”å…ƒåˆ†é¡ ---
def categorize_array_multi(Y, t1, t2, t3, t4, ids=None):
    """
    Y: np.ndarray, shape = (num_labels,), åƒ¹æ ¼è®ŠåŒ–ç‡
    """

    print("Y.shape:", Y.shape)
    # print(len(ids))

    # äº”å…ƒåˆ†é¡
    labels = np.full_like(Y, 2, dtype=int)  # é è¨­æŒå¹³
    labels[Y <= -t1] = 0  # å¤§è·Œ
    labels[(Y > -t1) & (Y <= -t2)] = 1  # è·Œ
    labels[(Y >= t3) & (Y < t4)] = 3  # æ¼²
    labels[Y >= t4] = 4  # å¤§æ¼²

    if ids is not None:
        # æ‰¾å‡º Y==0 çš„ç´¢å¼•
        zero_idx = np.where(Y == 0)[0]
        # åªå–å°æ‡‰çš„ ids
        dates_is_0 = set((ids[i][0], ids[i][1]) for i in zero_idx)
        if len(dates_is_0) > 0:
            print(f"å…±æœ‰ {len(dates_is_0)} å¤© Y==0")
            for id in sorted(dates_is_0):
                print(id)

    if np.any(Y == 0):  # æª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½•å…ƒç´ ç­‰æ–¼ 0
        count = np.sum(Y == 0)
        print(f"å…±æœ‰ {count} å€‹ Y == 0")
        labels[Y == 0] = 4  # ç‚ºäº†æ ¡æ­£ TRUMP å‰å…©å¤©çš„åƒ¹æ ¼ç›¸åŒ ç¬¬ä¸€å¤©è¨­ç‚ºå¤§æ¼²

    return labels



# def get_random_samples_sparse_stratified(X, y, seed: int = 42):
    """
    X: åŸå§‹æ¨æ–‡ text
    y: shape=(N,)  å¤šé¡åˆ¥æ¨™ç±¤
    """
    X = np.array(X)  # å¼·åˆ¶è½‰æ›æˆ np.array
    y = np.array(y)

    global N_SAMPLES
    # global ENABLE_SAMPLING
    # n_total = X.shape[0]

    print(X)
    input()

    n_total = len(X['input_ids'])


    if N_SAMPLES == 0:
        print(f"[INFO] ä¸åš random samplingï¼Œä½¿ç”¨æ‰€æœ‰æ¨£æœ¬æ•¸: {n_total} ç­†")
        # ENABLE_SAMPLING = False
        return [(X, y)]  # å›å‚³ä¸€å€‹åŸå§‹æ•¸é‡çš„ (X, y) tuple

    classes = np.unique(y)
    n_classes = len(classes)
    if N_SAMPLES < n_classes:
        raise ValueError(f"æ¨£æœ¬æ•¸ {N_SAMPLES} å¤ªå°‘ï¼Œç„¡æ³•å¹³å‡åˆ†é…åˆ°æ¯å€‹é¡åˆ¥ ({n_classes})")
    
    samples_per_class = N_SAMPLES // n_classes

    # å»ºç«‹ç´¢å¼•å­—å…¸
    class_indices = defaultdict(list)
    for idx, label in enumerate(y):
        class_indices[label].append(idx)

    samples = []
    for run in range(N_RUNS):
        np.random.seed(seed + run)
        selected_indices = []

        for c in classes:
            idx_list = class_indices[c]
            if len(idx_list) <= samples_per_class:
                # å¦‚æœè©²é¡åˆ¥æ•¸é‡ä¸å¤ ï¼Œå°±å…¨éƒ¨æ‹¿
                selected_indices.extend(idx_list)
            else:
                selected_indices.extend(np.random.choice(idx_list, samples_per_class, replace=False))

        # å¦‚æœç¸½æ•¸å°‘æ–¼ N_SAMPLESï¼Œå¾å‰©é¤˜æ¨£æœ¬è£œè¶³
        if len(selected_indices) < N_SAMPLES:
            # set(range(n_total)) æ˜¯æ‰€æœ‰æ¨£æœ¬ç´¢å¼•ï¼ˆ0 ~ n_total-1ï¼‰   set(selected_indices) æ˜¯å·²è¢«é¸éçš„ç´¢å¼•é›†åˆ
            remaining_idx = list(set(range(n_total)) - set(selected_indices))
            remaining_needed = N_SAMPLES - len(selected_indices)
            selected_indices.extend(np.random.choice(remaining_idx, remaining_needed, replace=False))

        np.random.shuffle(selected_indices)  # æ‰“äº‚é †åº
        X_sample = X[selected_indices]
        y_sample = y[selected_indices]
        samples.append((X_sample, y_sample))

        # === æ–°å¢ï¼šçµ±è¨ˆé¡åˆ¥æ•¸é‡èˆ‡æ¯”ä¾‹ ===
        unique, counts = np.unique(y_sample, return_counts=True)
        total = len(y_sample)
        print(f"\n[INFO] Run {run}: Stratified sample X_train={X_sample.shape}, y_train={y_sample.shape}")
        for cls, cnt in zip(unique, counts):
            pct = cnt / total * 100
            print(f"   Class {cls}: {cnt} samples ({pct:.2f}%)")

    return samples



def get_random_samples_sparse_stratified(X, y, seed: int = 42):
    """
    X: dict, {'input_ids': np.array, 'attention_mask': np.array}
    y: shape=(N,)  å¤šé¡åˆ¥æ¨™ç±¤
    """
    # ä¸è¦è½‰æˆ np.arrayï¼Œä¿æŒ dict
    y = np.array(y)

    global N_SAMPLES, N_RUNS

    n_total = X['input_ids'].shape[0]

    if N_SAMPLES == 0:
        print(f"[INFO] ä¸åš random samplingï¼Œä½¿ç”¨æ‰€æœ‰æ¨£æœ¬æ•¸: {n_total} ç­†")
        return [(X, y)]  # å›å‚³åŸå§‹æ•¸é‡çš„ (X, y) tuple

    classes = np.unique(y)
    n_classes = len(classes)
    if N_SAMPLES < n_classes:
        raise ValueError(f"æ¨£æœ¬æ•¸ {N_SAMPLES} å¤ªå°‘ï¼Œç„¡æ³•å¹³å‡åˆ†é…åˆ°æ¯å€‹é¡åˆ¥ ({n_classes})")
    
    samples_per_class = N_SAMPLES // n_classes

    # å»ºç«‹ç´¢å¼•å­—å…¸
    class_indices = defaultdict(list)
    for idx, label in enumerate(y):
        class_indices[label].append(idx)

    samples = []
    for run in range(N_RUNS):
        np.random.seed(seed + run)
        selected_indices = []

        for c in classes:
            idx_list = class_indices[c]
            if len(idx_list) <= samples_per_class:
                selected_indices.extend(idx_list)
            else:
                selected_indices.extend(np.random.choice(idx_list, samples_per_class, replace=False))

        # å¦‚æœç¸½æ•¸å°‘æ–¼ N_SAMPLESï¼Œå¾å‰©é¤˜æ¨£æœ¬è£œè¶³
        if len(selected_indices) < N_SAMPLES:
            remaining_idx = list(set(range(n_total)) - set(selected_indices))
            remaining_needed = N_SAMPLES - len(selected_indices)
            selected_indices.extend(np.random.choice(remaining_idx, remaining_needed, replace=False))

        np.random.shuffle(selected_indices)  # æ‰“äº‚é †åº

        # âš¡ å°æ¯å€‹ key åˆ†åˆ¥ç´¢å¼•
        X_sample = {k: v[selected_indices] for k, v in X.items()}
        y_sample = y[selected_indices]
        samples.append((X_sample, y_sample))

        # === çµ±è¨ˆé¡åˆ¥æ•¸é‡èˆ‡æ¯”ä¾‹ ===
        unique, counts = np.unique(y_sample, return_counts=True)
        total = len(y_sample)
        print(f"\n[INFO] Run {run}: Stratified sample X_train keys={list(X_sample.keys())}, y_train={y_sample.shape}")
        for cls, cnt in zip(unique, counts):
            pct = cnt / total * 100
            print(f"   Class {cls}: {cnt} samples ({pct:.2f}%)")

    return samples




# è‡ªè¨‚ Dataset ä¾†é©é… Hugging Face
# class TweetDataset(Dataset):
#     def __init__(self, texts, labels, tokenizer, max_length=128):
#         self.texts = texts
#         self.labels = labels
#         self.tokenizer = tokenizer
#         self.max_length = max_length

#     def __len__(self):
#         return len(self.labels)

#     def __getitem__(self, idx):
#         text = str(self.texts[idx])
#         label = int(self.labels[idx])

#         encoding = self.tokenizer(
#             text,
#             truncation=True,
#             padding="max_length",   # å¯ä»¥æ”¹æˆ "longest" æˆ– "max_length"
#             max_length=self.max_length,
#             return_tensors="pt"
#         )

#         # squeeze 0 ç¶­ï¼Œè®Šæˆå–®ç­† tensor
#         item = {key: val.squeeze(0) for key, val in encoding.items()}
#         item["labels"] = torch.tensor(label, dtype=torch.long)
#         return item

class TweetDataset(Dataset):
    def __init__(self, encodings, labels):
        """
        encodings: dict, åŒ…å« 'input_ids', 'attention_mask', (optional: 'token_type_ids')
        labels: shape=(N,)
        """
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        if idx >= len(self.encodings['input_ids']):
            print(f"[ERROR] idx={idx} è¶…å‡ºç¯„åœï¼Œdataset é•·åº¦={len(self.encodings['input_ids'])}")
            raise IndexError
        # æ¯å€‹ sample å·²ç¶“æ˜¯ dict
        item = {key: torch.tensor(self.encodings[key][idx]) for key in self.encodings}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    


# ==========================================================
# åˆ†æ‰¹ Tokenize + å­˜æª”
# ==========================================================
def tokenize_and_save_in_batches(X, y, tokenizer, save_path, prefix, batch_size=5000, max_len=128):
    os.makedirs(save_path, exist_ok=True)

    total_batches = math.ceil(len(X) / batch_size)
    print(f"ğŸ“¦ {prefix}: å…±éœ€ {total_batches} å€‹ batchï¼Œæ¯å€‹å¤§å° {batch_size}ï¼ˆæœ€å¾Œä¸€æ‰¹å¯èƒ½è¼ƒå°‘ï¼‰")

    file_paths = []
    for batch_idx in tqdm(range(total_batches), desc=f"Tokenizing {prefix}"):
        start = batch_idx * batch_size
        end = start + batch_size

        batch_texts = X[start:end].astype(str).tolist() if isinstance(X, np.ndarray) else [str(x) for x in X[start:end]]
        batch_labels = y[start:end]

        # print(f"ğŸ” batch {batch_idx} å‹æ…‹ï¼š", type(batch_texts))
        # print(f"ğŸ” ç¬¬ä¸€å€‹å…ƒç´ å‹æ…‹ï¼š", type(batch_texts[0]))
        # print(f"ğŸ” ç¬¬ä¸€å€‹å…ƒç´ å…§å®¹ï¼š", batch_texts[0])

        encodings = tokenizer(
            batch_texts,
            truncation=True,
            padding="max_length",
            max_length=max_len,
            return_tensors="np"
        )

        file_path = os.path.join(save_path, f"{prefix}_batch{batch_idx}{SUFFIX_FILTERED}.pkl")
        with open(file_path, "wb") as f:
            pickle.dump((encodings, batch_labels), f)
        file_paths.append(file_path)

    print(f"âœ… {prefix} å…¨éƒ¨ {total_batches} å€‹ batch å·²å­˜æª”å®Œæˆ")

    return file_paths


# ==========================================================
# è¼‰å…¥åˆ†æ‰¹è³‡æ–™ â†’ åˆä½µæˆå–®ä¸€ Dataset
# ==========================================================
# def load_tokenized_batches(save_path, prefix):
#     all_encodings = []
#     all_labels = []

#     files = sorted([f for f in os.listdir(save_path) if f.startswith(prefix)])
#     for f_name in tqdm(files, desc=f"æ­£åœ¨è®€å–åˆ†æ‰¹çš„ {prefix} tokenize..."):
#         with open(os.path.join(save_path, f_name), "rb") as f:
#             encodings, labels = pickle.load(f)
#             all_encodings.append(encodings)
#             all_labels.extend(labels)

#     # åˆä½µæˆå–®ä¸€ dict (numpy)
#     merged_encodings = {
#         "input_ids": np.concatenate([e["input_ids"] for e in all_encodings]),
#         "attention_mask": np.concatenate([e["attention_mask"] for e in all_encodings]),
#     }

#     print(f"âœ… {prefix} æˆåŠŸåˆä½µæˆå–®ä¸€ tokenize")

#     return merged_encodings, all_labels

def load_tokenized_batches(save_path, prefix):
    merged_path = f"../data/ml/classification/BERT/tokenize/{prefix}_token_merged{SUFFIX_FILTERED}.pkl"

    # ğŸ”¹ è‹¥åˆä½µæª”å·²å­˜åœ¨ï¼Œç›´æ¥è¼‰å…¥
    if os.path.exists(merged_path):
        print(f"ğŸ“‚ åµæ¸¬åˆ°å·²å­˜åœ¨çš„åˆä½µæª”ï¼š{merged_path}")
        input("â“ æ˜¯å¦è¦ä½¿ç”¨é€™ä»½ åˆä½µçš„ Token? (æŒ‰ Enter ä»¥ç¹¼çºŒ æˆ– Ctrl + C ...)")
        with open(merged_path, "rb") as f:
            merged_encodings, labels_all = pickle.load(f)
        print(f"âœ… å·²ç›´æ¥è¼‰å…¥ {prefix}_merged.pkl")
        return merged_encodings, labels_all

    # ğŸ”¹ å¦å‰‡å°±é€²è¡Œåˆä½µ
    input_ids_list = []
    attention_mask_list = []
    labels_all = []

    files = sorted([f for f in os.listdir(save_path) if f.startswith(prefix)])
    for f_name in tqdm(files, desc=f"æ­£åœ¨è®€å–åˆ†æ‰¹çš„ {prefix} tokenize..."):
        file_path = os.path.join(save_path, f_name)

        # è®€å–å–®ä¸€æª”æ¡ˆ
        with open(file_path, "rb") as f:
            encodings, labels = pickle.load(f)

        # åˆä½µ
        input_ids_list.append(encodings["input_ids"])
        attention_mask_list.append(encodings["attention_mask"])
        labels_all.extend(labels)

        # æ¸…ç†æš«å­˜
        del encodings, labels
        gc.collect()

    labels_all = np.array(labels_all)  #------------------------------------------------------


    # ğŸ”¹ åˆä½µç‚ºå–®ä¸€é™£åˆ—
    merged_encodings = {
        "input_ids": np.concatenate(input_ids_list, axis=0),
        "attention_mask": np.concatenate(attention_mask_list, axis=0),
    }

    # æ¸…ç†æš«å­˜
    del input_ids_list, attention_mask_list
    gc.collect()

    print(f"âœ… {prefix} æˆåŠŸåˆä½µæˆå–®ä¸€ tokenize")

    # ğŸ”¹ å°‡çµæœå¿«å–ä¸‹ä¾†ï¼Œä¸‹æ¬¡å°±èƒ½ç›´æ¥è¼‰å…¥
    with open(merged_path, "wb") as f:
        pickle.dump((merged_encodings, labels_all), f)
    print(f"ğŸ’¾ å·²å°‡åˆä½µçµæœå­˜æˆ {merged_path}")

    return merged_encodings, labels_all


# ==========================================================
# ä¸»è¦ Tokenize & Save functionï¼ˆæ”¹ç”¨åˆ†æ‰¹ï¼‰
# ==========================================================
def tokenize_and_save(X_train, X_test, y_train, y_test, save_path, model_name=BERT_MODEL_NAME, batch_size=5000):
    os.makedirs(save_path, exist_ok=True)
    # tokenizer = BertTokenizerFast.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    print("ğŸ› ï¸ Tokenizing Train Data...")
    tokenize_and_save_in_batches(X_train, y_train, tokenizer, save_path, prefix="train", batch_size=batch_size)

    print("ğŸ› ï¸ Tokenizing Test Data...")
    tokenize_and_save_in_batches(X_test, y_test, tokenizer, save_path, prefix="test", batch_size=batch_size)

    print(f"âœ… All tokenized data saved to {save_path}")


def load_tokenized_data(save_path):
    X_train_enc, y_train = load_tokenized_batches(save_path, prefix="train")
    X_test_enc, y_test = load_tokenized_batches(save_path, prefix="test")
    return X_train_enc, X_test_enc, y_train, y_test



def train_function(X_train, X_test, y_train, y_test, pipeline_path, model_name=BERT_MODEL_NAME):

    print("transformers.__version__:", transformers.__version__)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("ğŸ’» Using device:", device)

    all_results = []
    best_test_acc = -1
    best_run_info = None

    # tokenizer = BertTokenizerFast.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    if RUN_FIRST_CLASSIFIER:
        # ç¢ºä¿æ˜¯ listï¼Œæ–¹ä¾¿ Trainer   #------------------------------------------------------
        # X_train = list(X_train)
        # X_test  = list(X_test)
        # y_train = list(y_train)
        # y_test  = list(y_test)

        if IS_FILTERED:
            tokenize_path = "filtered"
        else:
            tokenize_path = "non_filtered"

        # # æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰ tokenized data -------------------- è¦ä¿®æ”¹ -------------------------------
        # input("æœ‰é‡æ–°æ”¹éç¨‹å¼äº†å—ï¼Œè¦æ˜¯å…ˆåˆ¤æ–·æœ‰æ²’æœ‰ merge çš„æª”æ¡ˆï¼Œå†çœ‹çœ‹éœ€ä¸éœ€è¦ tokenizeï¼Œé‚„æ˜¯åªè¦ merge å°±å¥½")
        # if os.path.exists(f"{OUTPUT_PATH}/tokenize/{tokenize_path}/train_batch0{SUFFIX_FILTERED}.pkl"):
        #     print("ğŸ“‚ è¼‰å…¥å·²å­˜çš„ Tokenized Data")
        #     input("â“ æ˜¯å¦è¦ä½¿ç”¨é€™ä»½ Tokenized Data? (æŒ‰ Enter ä»¥ç¹¼çºŒ æˆ– Ctrl + C ...)")
        #     X_train_enc, X_test_enc, y_train, y_test = load_tokenized_data(f"{OUTPUT_PATH}/tokenize/{tokenize_path}")
        # else:
        #     print("ğŸ› ï¸ ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼Œé–‹å§‹ Tokenize ä¸¦å­˜æª”...")
        #     tokenize_and_save(X_train, X_test, y_train, y_test, save_path=f"{OUTPUT_PATH}/tokenize/{tokenize_path}", model_name="bert-base-uncased")
        #     X_train_enc, X_test_enc, y_train, y_test = load_tokenized_data(f"{OUTPUT_PATH}/tokenize/{tokenize_path}")


        merge_path_train = f"{OUTPUT_PATH}/tokenize/train_token_merged{SUFFIX_FILTERED}.pkl"
        merge_path_test  = f"{OUTPUT_PATH}/tokenize/test_token_merged{SUFFIX_FILTERED}.pkl"

        # 1ï¸âƒ£ å…ˆæª¢æŸ¥ merge æª”æ¡ˆ
        if os.path.exists(merge_path_train) and os.path.exists(merge_path_test):
            X_train_enc, y_train = load_tokenized_batches(f"{OUTPUT_PATH}/tokenize/{tokenize_path}", prefix="train")
            X_test_enc, y_test   = load_tokenized_batches(f"{OUTPUT_PATH}/tokenize/{tokenize_path}", prefix="test")
        else:
            # 2ï¸âƒ£ æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰ batch tokenize æª”æ¡ˆ
            first_batch_train = f"{OUTPUT_PATH}/tokenize/{tokenize_path}/train_batch0{SUFFIX_FILTERED}.pkl"
            first_batch_test  = f"{OUTPUT_PATH}/tokenize/{tokenize_path}/test_batch0{SUFFIX_FILTERED}.pkl"

            if os.path.exists(first_batch_train) and os.path.exists(first_batch_test):
                print("ğŸ“‚ åµæ¸¬åˆ°å·²æœ‰ batch tokenize æª”æ¡ˆï¼Œä½† merge æª”æ¡ˆä¸å­˜åœ¨ï¼Œé–‹å§‹ merge...")
                X_train_enc, y_train = load_tokenized_batches(f"{OUTPUT_PATH}/tokenize/{tokenize_path}", prefix="train")
                X_test_enc, y_test   = load_tokenized_batches(f"{OUTPUT_PATH}/tokenize/{tokenize_path}", prefix="test")
            else:
                # 3ï¸âƒ£ ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼Œéœ€è¦ tokenize
                print("ğŸ› ï¸ ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼Œé–‹å§‹ Tokenize ä¸¦å­˜æª”...")
                tokenize_and_save(X_train, X_test, y_train, y_test,
                                save_path=f"{OUTPUT_PATH}/tokenize/{tokenize_path}",
                                model_name=BERT_MODEL_NAME)
                # Tokenize å®Œå† merge
                X_train_enc, X_test_enc, y_train, y_test = load_tokenized_data(f"{OUTPUT_PATH}/tokenize/{tokenize_path}")

        
        # --- å–å¾—åˆ†å±¤éš¨æ©Ÿå–æ¨£ ---
        train_sample = get_random_samples_sparse_stratified(X_train_enc, y_train)  # [(X_sample, y_sample), ...]
        run_count = len(train_sample)

        # X_test_enc = list(X_test_enc)  #-----------------------------
        # y_test = list(y_test)

        # test åŒ…è£æˆ Dataset
        test_dataset = TweetDataset(X_test_enc, y_test)

    elif RUN_SECOND_CLASSIFIER:
        train_sample = [(X_train, y_train)]
        run_count = 1

        # X_test = list(X_test)  #---------------------------------
        # y_test = list(y_test)

        # test åŒ…è£æˆ Dataset
        test_dataset = TweetDataset(X_test, y_test)

    else:
        raise ValueError("è«‹è¨­å®š RUN_FIRST_CLASSIFIER æˆ– RUN_SECOND_CLASSIFIER")

    # --- åŸ·è¡Œ N_RUNS æ¬¡ ---
    for run in range(run_count):
        print(f"\n===== RUN {run} =====")

        X_train_sample, y_train_sample = train_sample[run]
        # X_train_sample = list(X_train_sample)    # -----------------------------------------------
        # y_train_sample = list(y_train_sample)
        train_dataset = TweetDataset(X_train_sample, y_train_sample)

        # åˆå§‹åŒ–æ¨¡å‹
        num_labels = len(set(y_train_sample))
        print("num_labels:", num_labels)
        # model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)
        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)
        model.to(device)

        # è¨“ç·´åƒæ•¸ï¼ˆé€™è£¡ä½ å¯ä»¥éš¨æ©ŸæŠ½ hyperparamsï¼Œæ¨¡æ“¬ RandomizedSearchCVï¼‰
        training_args = TrainingArguments(
            output_dir=f"./results_run_{run}",
            # evaluation_strategy="epoch",
            save_strategy="no",
            learning_rate=2e-5,
            per_device_train_batch_size=16,
            per_device_eval_batch_size=64,
            num_train_epochs=3,
            weight_decay=0.01,
            logging_dir=f"./logs_run_{run}",
            load_best_model_at_end=False,
            report_to="none",
            fp16=True,                      # ğŸ§  é–‹å•ŸåŠç²¾åº¦è¨“ç·´èˆ‡æ¨è«–
            fp16_opt_level="O1",            # (å¯é¸) æ··åˆç²¾åº¦å„ªåŒ–å±¤ç´š (è‡ªå‹•)
        )

        def compute_metrics(eval_pred):
            logits, labels = eval_pred
            preds = np.argmax(logits, axis=-1)
            acc = accuracy_score(labels, preds)
            return {"accuracy": acc}

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=test_dataset,
            compute_metrics=compute_metrics,
        )

        trainer.train()

        # è©•ä¼°
        # train_metrics = trainer.evaluate(train_dataset)
        # test_metrics = trainer.evaluate(test_dataset)

        # train_acc = train_metrics["eval_accuracy"]
        # test_acc = test_metrics["eval_accuracy"]

        # ç”¨ predictï¼Œåªå°å°æ‰¹é‡è³‡æ–™è·‘
        np.random.seed(42)
        print("len(train_dataset), len(test_dataset):", len(train_dataset), len(test_dataset))
        train_subset_size = min(1000, len(train_dataset))
        train_subset_indices = np.random.choice(len(train_dataset), size=train_subset_size, replace=False)
        small_train_dataset = Subset(train_dataset, train_subset_indices)
        test_subset_size = min(1000, len(test_dataset))
        test_subset_indices = np.random.choice(len(test_dataset), size=test_subset_size, replace=False)
        small_test_dataset = Subset(test_dataset, test_subset_indices)

        preds_train = trainer.predict(small_train_dataset)
        preds_test = trainer.predict(small_test_dataset)

        y_pred_train = np.argmax(preds_train.predictions, axis=-1)
        y_true_train = preds_train.label_ids
        y_pred_test = np.argmax(preds_test.predictions, axis=-1)
        y_true_test = preds_test.label_ids

        train_acc = accuracy_score(y_true_train, y_pred_train)
        test_acc = accuracy_score(y_true_test, y_pred_test)

        print(f"[RUN {run}] Train acc={train_acc:.4f}, Test acc={test_acc:.4f}")  # Train acc={train_acc:.4f}

        all_results.append({
            "run": run,
            "train_acc": train_acc,  # train_acc
            "test_acc": test_acc,
        })

        if (RUN_FIRST_CLASSIFIER and test_acc > best_test_acc) or RUN_SECOND_CLASSIFIER:
            best_test_acc = test_acc
            best_run_info = {
                "run": run,
                "model": model,
                "tokenizer": tokenizer,
                "train_acc": train_acc,  # train_acc
                "test_acc": test_acc,
            }

    # --- å„²å­˜æ‰€æœ‰çµæœ ---
    results_df = pd.DataFrame(all_results)
    results_df.to_csv(f"{OUTPUT_PATH}/bert_results.csv", index=False)

    # --- å„²å­˜æœ€ä½³æ¨¡å‹ ---
    best_model = best_run_info["model"]
    best_model.save_pretrained(pipeline_path)
    tokenizer.save_pretrained(pipeline_path)

    print("\n=== æœ€ä½³æ¨¡å‹ ===")
    print(f"Run {best_run_info['run']} | Train acc={best_run_info['train_acc']:.4f}, Test acc={best_run_info['test_acc']:.4f}")
    
    best_model = best_run_info["model"]
    best_model.half()     # ğŸš€ åŠç²¾åº¦æ¨è«–æ›´å¿«
    best_tokenizer = best_run_info["tokenizer"]

    trainer = Trainer(
        model=best_model,
        args=training_args,  # å¯ä»¥é‡ç”¨æœ€å¾Œä¸€å€‹ run çš„ training_args
        eval_dataset=test_dataset,
        compute_metrics=compute_metrics,
    )

    # preds = trainer.predict(test_dataset).predictions
    # preds = np.argmax(preds, axis=-1)
    # print(classification_report(y_test, preds))



def evaluate_by_coin_date(ids, y_true, y_pred):
    LABEL_SYMBOLS = {
        0: "ğŸ”´",  # å¤§è·Œ
        1: "ğŸŸ ",  # è·Œ
        2: "âšª",  # æŒå¹³
        3: "ğŸŸ¡",  # æ¼²
        4: "ğŸŸ¢"   # å¤§æ¼²
    }

    if RUN_FIRST_CLASSIFIER:
        results = defaultdict(list)

        # èšåˆ
        for (coin, date, _), t, p in zip(ids, y_true, y_pred):
            results[(coin, date)].append((t, p))

        daily_summary = {}
        for (coin, date), samples in results.items():
            truths, preds = zip(*samples)
            truths = np.array(truths)
            preds  = np.array(preds)

            # å¤šæ•¸æ±º
            values, counts = np.unique(preds, return_counts=True)
            majority_pred = values[np.argmax(counts)]

            true_label = truths[0]  # å‡è¨­åŒä¸€å¤©çœŸå¯¦æ¨™ç±¤ä¸€è‡´
            correct = (majority_pred == true_label)


            daily_summary.setdefault(coin, {})

            # å°‡å„é¡åˆ¥å‡ºç¾æ¬¡æ•¸è½‰æˆ listï¼ˆä¿æŒåŸæœ¬ up_counts/down_counts çš„æ„Ÿè¦ºï¼‰
            class_counts = [np.sum(preds == i) for i in range(5)]  # 0~4 äº”é¡
            pred_symbols = [LABEL_SYMBOLS[majority_pred]]           # å–®ä¸€é æ¸¬ç¬¦è™Ÿ

            true_symbols   = [LABEL_SYMBOLS[int(true_label)]]   # çœŸå¯¦ç¬¦è™Ÿ
            result_symbols = ["âœ…" if correct else "âŒ"]         # å°éŒ¯ç¬¦è™Ÿ


            daily_summary[coin][date] = {
                "true_label": int(true_label),
                "majority_pred": int(majority_pred),
                "majority_correct": bool(correct),
                "class_counts": class_counts,    # æ›¿ä»£ up_counts/down_counts
                "total_counts": len(preds),      # åŸæœ¬ total_counts
                "pred_symbols": pred_symbols,
                "true_symbols": true_symbols,     # çœŸå¯¦ç¬¦è™Ÿ
                "result_symbols": result_symbols  # å°éŒ¯ç¬¦è™Ÿ
            }

        return daily_summary, len(np.unique(y_true))
    
    # --- æœªå®Œæˆ ---
    elif RUN_SECOND_CLASSIFIER:
        daily_summary = {}

        for (coin, date), t, p in zip(ids, y_true, y_pred):
            correct = (p == t)

            # å„é¡åˆ¥è¨ˆæ•¸ (é€™è£¡å› ç‚ºåªæœ‰ä¸€ç­†ï¼Œåªæœ‰ä¸€å€‹é¡åˆ¥æœƒæ˜¯ 1ï¼Œå…¶é¤˜éƒ½æ˜¯ 0)
            class_counts = [1 if p == i else 0 for i in range(5)]

            daily_summary.setdefault(coin, {})
            daily_summary[coin][date] = {
                "true_label": int(t),
                "majority_pred": int(p),
                "majority_correct": bool(correct),
                "class_counts": class_counts,
                "total_counts": 1,
                "pred_symbols": [LABEL_SYMBOLS[int(p)]],
                "true_symbols": [LABEL_SYMBOLS[int(t)]],
                "result_symbols": ["âœ…" if correct else "âŒ"]
            }

        return daily_summary, len(np.unique(y_true))



def predict_function(X_train, X_test, y_train, y_test, ids_train, ids_test, model_path, model_name=BERT_MODEL_NAME):
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("ğŸ’» Using device:", device)

    torch.set_num_threads(16)  # ä½¿ç”¨ 16 å€‹ CPU threadsï¼ˆä¾ä½ æ©Ÿå™¨æ ¸å¿ƒæ•¸èª¿æ•´ï¼‰
    torch.set_num_interop_threads(16)
    print("CPU threads:", torch.get_num_threads())
    print("Interop threads:", torch.get_num_interop_threads())

    # tokenizer = BertTokenizerFast.from_pretrained(model_name)
    # model = BertForSequenceClassification.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    model.to(device)

    # âœ… å°‡ numpy é™£åˆ—è½‰å› listï¼Œç„¶å¾Œç”¨ tokenizer è™•ç†
    # X_train_encodings = tokenizer(
    #     X_train.tolist(),         # <-- è½‰æˆ list æ‰èƒ½è¢« tokenizer è™•ç†
    #     truncation=True,
    #     padding=True,
    #     max_length=128
    # )
    # X_test_encodings = tokenizer(
    #     X_test.tolist(),
    #     truncation=True,
    #     padding=True,
    #     max_length=128
    # )

    if IS_FILTERED:
        tokenize_path = "filtered"
    else:
        tokenize_path = "non_filtered"

    merge_path_train = f"{OUTPUT_PATH}/tokenize/train_token_merged{SUFFIX_FILTERED}.pkl"
    merge_path_test  = f"{OUTPUT_PATH}/tokenize/test_token_merged{SUFFIX_FILTERED}.pkl"

    # å…ˆæª¢æŸ¥ merge æª”æ¡ˆ
    if os.path.exists(merge_path_train) and os.path.exists(merge_path_test):
        X_train_enc, y_train = load_tokenized_batches(f"{OUTPUT_PATH}/tokenize/{tokenize_path}", prefix="train")
        X_test_enc, y_test   = load_tokenized_batches(f"{OUTPUT_PATH}/tokenize/{tokenize_path}", prefix="test")

    print(type(X_test_enc))
    if isinstance(X_test_enc, dict):
        print("âœ… X_test_enc æ˜¯ tokenizer è¼¸å‡º")
        print(X_test_enc.keys())
    else:
        print("âŒ X_test_enc ä¸æ˜¯ tokenizer è¼¸å‡º")

    # âœ… ç”¨æ­£ç¢ºæ ¼å¼å»ºç«‹ Dataset
    train_dataset = TweetDataset(X_train_enc, y_train)
    test_dataset  = TweetDataset(X_test_enc, y_test)


    # å»ºç«‹ Dataset
    # train_dataset = TweetDataset(X_train, y_train)
    # test_dataset  = TweetDataset(X_test, y_test)

    trainer = Trainer(model=model)  # åªç”¨ä¾†åš predictï¼Œä¸éœ€è¦ training args

    # é æ¸¬
    train_preds = trainer.predict(train_dataset).predictions
    test_preds  = trainer.predict(test_dataset).predictions

    # å– argmax
    train_preds = np.argmax(train_preds, axis=-1)
    test_preds  = np.argmax(test_preds, axis=-1)

    train_report = classification_report(y_train, train_preds, zero_division=0)
    test_report = classification_report(y_test, test_preds, zero_division=0)

    # è©•ä¼°åˆ†é¡å ±å‘Š
    print("\nTrain Classification Report:")
    print(train_report)
    print("\nTest Classification Report:")
    print(test_report)

    # æŒ‡å®šè¼¸å‡ºæª”æ¡ˆåç¨±
    output_path = "classification_report.txt"

    with open(f"{OUTPUT_PATH}/classification_report_{N_SAMPLES}.txt", "w", encoding="utf-8") as f:
        f.write("=== Train Classification Report ===\n")
        f.write(train_report)
        f.write("\n\n=== Test Classification Report ===\n")
        f.write(test_report)

    print(f"åˆ†é¡å ±å‘Šå·²å„²å­˜è‡³ï¼š{output_path}")

    # å¥—ç”¨ä½ åŸæœ¬çš„ daily aggregation
    train_daily, _ = evaluate_by_coin_date(ids_train, y_train, train_preds)
    test_daily, _  = evaluate_by_coin_date(ids_test, y_test, test_preds)

    if RUN_FIRST_CLASSIFIER:

        # === å­˜æˆ JSON ===
        with open(f"{OUTPUT_PATH}/BERT_train_daily_results_{N_SAMPLES}{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json", "w", encoding="utf-8") as f:
            json.dump(train_daily, f, ensure_ascii=False, indent=4, default=int)

        with open(f"{OUTPUT_PATH}/BERT_test_daily_results_{N_SAMPLES}{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json", "w", encoding="utf-8") as f:
            json.dump(test_daily, f, ensure_ascii=False, indent=4, default=int)

        print("å·²è¼¸å‡ºé€æ—¥é æ¸¬çµæœï¼š")
        print(f"- train: {OUTPUT_PATH}/BERT_train_daily_results_{N_SAMPLES}{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json")
        print(f"- test:  {OUTPUT_PATH}/BERT_test_daily_results_{N_SAMPLES}{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json")

        # === åˆä½µ train + test ===
        combined_daily = {}
        for coin, daily in train_daily.items():
            combined_daily.setdefault(coin, {}).update(daily)
        for coin, daily in test_daily.items():
            combined_daily.setdefault(coin, {}).update(daily)

        # === å­˜æˆåˆä½µå¾Œçš„ TXT ===
        txt_path = f"{OUTPUT_PATH}/BERT_combined_results_{N_SAMPLES}{SUFFIX_FILTERED}{SUFFIX_AUGUST}.txt"
        with open(txt_path, "w", encoding="utf-8") as f:
            # === åˆå§‹åŒ–çµ±è¨ˆå™¨ ===
            label_correct = np.zeros(1, dtype=int)
            label_total   = np.zeros(1, dtype=int)

            for coin, daily in combined_daily.items():
                f.write(f"\n=== {coin} ===\n")

                # ç”¨ä¾†å­˜æ”¾æ¯å¤©çš„ (date, pred_class)
                records = []

                for date, stats in sorted(daily.items()):
                    # --- æ¯æ—¥è¼¸å‡ºåˆ° TXT ---
                    class_str = " ".join(f"{x:5d}" for x in stats['class_counts'])
                    line = (
                        f"{date} â†’ ğŸ“Š {class_str}  "
                        f"ç¸½æ•¸: {stats['total_counts']:5d}  "
                        f"é æ¸¬: {''.join(stats['pred_symbols'])}  "
                        f"çœŸå¯¦: {''.join(stats['true_symbols'])}  "
                        f"çµæœ: {''.join(stats['result_symbols'])}\n"
                    )
                    f.write(line)

                    # --- æ›´æ–°ç´¯ç©æº–ç¢ºç‡ ---
                    label_total[0] += 1
                    if stats["majority_correct"]:
                        label_correct[0] += 1

                    # --- å–ç•¶å¤©é æ¸¬é¡åˆ¥ (class_counts æœ€å¤§çš„ index) ---
                    pred_class = int(np.argmax(stats["class_counts"]))
                    records.append((date, pred_class))

                # --- è¼¸å‡ºæ•´é«”æº–ç¢ºç‡ (ç™¾åˆ†æ¯”) ---
                accuracy_summary = " ".join(
                    f"{(c / t * 100):.2f}%" if t > 0 else "N/A"
                    for c, t in zip(label_correct, label_total)
                )
                f.write(f"\næ•´é«”æº–ç¢ºç‡: {accuracy_summary}\n")

                # === å­˜æˆ .npy (æ¯æ—¥é æ¸¬çµæœï¼Œä¾æ—¥æœŸæ’åº) ===
                if records:
                    records.sort(key=lambda x: x[0])
                    _, preds = zip(*records)
                    preds = np.array(preds, dtype=np.int32)

                    npy_path = f"{OUTPUT_PATH}/{coin}_BERT_classifier_1_result{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy"
                    np.save(npy_path, preds)
                    print(preds[:50])
                    print(f"{coin} â†’ {npy_path} å·²å®Œæˆ, shape={preds.shape}")


        print(f"\nåˆä½µå¾Œçš„äººé¡å¯è®€ç‰ˆçµæœå·²è¼¸å‡ºåˆ°ï¼š{txt_path}")

    elif RUN_SECOND_CLASSIFIER:
        # === å­˜æˆ JSON ===
        with open(f"{OUTPUT_PATH}/BERT_train_daily_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json", "w", encoding="utf-8") as f:
            json.dump(train_daily, f, ensure_ascii=False, indent=4, default=int)

        with open(f"{OUTPUT_PATH}/BERT_test_daily_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json", "w", encoding="utf-8") as f:
            json.dump(test_daily, f, ensure_ascii=False, indent=4, default=int)

        print("å·²è¼¸å‡ºé€æ—¥é æ¸¬çµæœï¼š")
        print(f"- train: {OUTPUT_PATH}/BERT_train_daily_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json")
        print(f"- test:  {OUTPUT_PATH}/BERT_test_daily_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json")

        # === åˆä½µ train + test ===
        combined_daily = {}
        for coin, daily in train_daily.items():
            combined_daily.setdefault(coin, {}).update(daily)
        for coin, daily in test_daily.items():
            combined_daily.setdefault(coin, {}).update(daily)

        # === å­˜æˆåˆä½µå¾Œçš„ TXT ===
        txt_path = f"{OUTPUT_PATH}/BERT_combined_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.txt"
        with open(txt_path, "w", encoding="utf-8") as f:
            label_correct = 0
            label_total = 0

            for coin, daily in combined_daily.items():
                f.write(f"\n=== {coin} ===\n")

                records = []
                for date, stats in sorted(daily.items()):
                    # --- æ¯æ—¥è¼¸å‡ºåˆ° TXT ---
                    line = (
                        f"{date} â†’ "
                        f"é æ¸¬: {''.join(stats['pred_symbols'])}  "
                        f"çœŸå¯¦: {''.join(stats['true_symbols'])}  "
                        f"çµæœ: {''.join(stats['result_symbols'])}\n"
                    )
                    f.write(line)

                    # --- æ›´æ–°ç´¯ç©æº–ç¢ºç‡ ---
                    label_total += 1
                    if stats["majority_correct"]:
                        label_correct += 1

                    # --- ä¿å­˜æ¯æ—¥é æ¸¬é¡åˆ¥ ---
                    records.append((date, stats["majority_pred"]))

                # --- è¼¸å‡ºæ•´é«”æº–ç¢ºç‡ ---
                acc = (label_correct / label_total * 100) if label_total > 0 else 0
                f.write(f"\næ•´é«”æº–ç¢ºç‡: {acc:.2f}%\n")

        print(f"\nåˆä½µå¾Œçš„äººé¡å¯è®€ç‰ˆçµæœå·²è¼¸å‡ºåˆ°ï¼š{txt_path}")



# --- æœªå®Œæˆ ---
def predict_august_function(pipeline_path):
    combined_daily = {}  # ç”¨ä¾†æ”¾ åˆä½µ ä¸‰ç¨®å¹£ç¨® çš„è³‡æ–™ ===

    # --- è¼‰å…¥è³‡æ–™ ---
    for coin_short_name in ['DOGE', 'PEPE', 'TRUMP']:
        if RUN_FIRST_CLASSIFIER:
            X_august = sparse.load_npz(f'{INPUT_PATH}/X_input/keyword_classifier/{coin_short_name}/{coin_short_name}_X_sparse{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npz')
            y_august = np.load(f'{INPUT_PATH}/y_input/{coin_short_name}/{coin_short_name}_price_diff{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy')
            with open(f'{INPUT_PATH}/ids_input/{coin_short_name}/{coin_short_name}_ids{SUFFIX_FILTERED}{SUFFIX_AUGUST}.pkl', 'rb') as file:
                ids_august = pickle.load(file)

        elif RUN_SECOND_CLASSIFIER:
            X_august = np.load(f"{INPUT_PATH}/X_input/keyword_classifier/{coin_short_name}/{coin_short_name}_{MODEL_NAME}_X_classifier_2{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy")
            y_august = np.load(f"{INPUT_PATH}/y_input/{coin_short_name}/{coin_short_name}_price_diff_original{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy")
            with open(f"{INPUT_PATH}/ids_input/{coin_short_name}/{coin_short_name}_{MODEL_NAME}_ids_classifier_2{SUFFIX_FILTERED}{SUFFIX_AUGUST}.pkl", 'rb') as file:
                ids_august = pickle.load(file)

        y_august_categorized = categorize_array_multi(y_august, T1, T2, T3, T4)

        # === è¼‰å…¥æœ€ä½³æ¨¡å‹ ===
        pipeline = joblib.load(pipeline_path)
        model = pipeline["model"]
        
        # === é æ¸¬æ‰€æœ‰æ¨£æœ¬ ===
        y_pred_august = model.predict(X_august)
        print(y_pred_august.shape)

        # å°‡ ids è½‰æˆ np.array æ–¹ä¾¿æ¥ä¸‹ä¾†çš„è™•ç†
        ids_august = np.array(ids_august)

        
        print(f"\nåˆ†é¡å ±å‘Š ({coin_short_name} August set):")
        print(classification_report(y_august_categorized, y_pred_august, zero_division=0))

        # august_score = knn.score(X_august, Y_august)
        print(f'{coin_short_name} August accuracy')  

        print("ids_august[:5]", ids_august[:5])

        august_daily, _ = evaluate_by_coin_date(ids_august, y_august_categorized, y_pred_august)

        if RUN_FIRST_CLASSIFIER:
            # === å­˜æˆ JSON ===
            with open(f"{OUTPUT_PATH}/{coin_short_name}_logreg_august_daily_results_{N_SAMPLES}{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json", "w", encoding="utf-8") as f:
                json.dump(august_daily, f, ensure_ascii=False, indent=4, default=int)

            print("å·²è¼¸å‡ºé€æ—¥é æ¸¬çµæœï¼š")
            print(f"- august: {OUTPUT_PATH}/{coin_short_name}_logreg_august_daily_results_{N_SAMPLES}{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json")

            # === åˆä½µ ä¸‰ç¨®å¹£ç¨® ===
            for coin, daily in august_daily.items():
                combined_daily.setdefault(coin, {}).update(daily)

            # === å­˜æˆåˆä½µå¾Œçš„ TXT ===
            txt_path = f"{OUTPUT_PATH}/logreg_combined_results_{N_SAMPLES}{SUFFIX_FILTERED}{SUFFIX_AUGUST}.txt"
            with open(txt_path, "w", encoding="utf-8") as f:
                # === åˆå§‹åŒ–çµ±è¨ˆå™¨ ===
                label_correct = np.zeros(1, dtype=int)
                label_total   = np.zeros(1, dtype=int)

                for coin, daily in combined_daily.items():
                    f.write(f"\n=== {coin} ===\n")

                    # ç”¨ä¾†å­˜æ”¾æ¯å¤©çš„ (date, pred_class)
                    records = []

                    for date, stats in sorted(daily.items()):
                        # --- æ¯æ—¥è¼¸å‡ºåˆ° TXT ---
                        class_str = " ".join(f"{x:5d}" for x in stats['class_counts'])
                        line = (
                            f"{date} â†’ ğŸ“Š {class_str}  "
                            f"ç¸½æ•¸: {stats['total_counts']:5d}  "
                            f"é æ¸¬: {''.join(stats['pred_symbols'])}  "
                            f"çœŸå¯¦: {''.join(stats['true_symbols'])}  "
                            f"çµæœ: {''.join(stats['result_symbols'])}\n"
                        )
                        f.write(line)

                        # --- æ›´æ–°ç´¯ç©æº–ç¢ºç‡ ---
                        label_total[0] += 1
                        if stats["majority_correct"]:
                            label_correct[0] += 1

                        # --- å–ç•¶å¤©é æ¸¬é¡åˆ¥ (class_counts æœ€å¤§çš„ index) ---
                        pred_class = int(np.argmax(stats["class_counts"]))
                        records.append((date, pred_class))

                    # --- è¼¸å‡ºæ•´é«”æº–ç¢ºç‡ (ç™¾åˆ†æ¯”) ---
                    accuracy_summary = " ".join(
                        f"{(c / t * 100):.2f}%" if t > 0 else "N/A"
                        for c, t in zip(label_correct, label_total)
                    )
                    f.write(f"\næ•´é«”æº–ç¢ºç‡: {accuracy_summary}\n")

                    # === å­˜æˆ .npy (æ¯æ—¥é æ¸¬çµæœï¼Œä¾æ—¥æœŸæ’åº) ===
                    if records:
                        records.sort(key=lambda x: x[0])
                        _, preds = zip(*records)
                        preds = np.array(preds, dtype=np.int32)

                        npy_path = f"{OUTPUT_PATH}/{coin}_logreg_classifier_1_result{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy"
                        np.save(npy_path, preds)
                        print(preds[:50])
                        print(f"{coin} â†’ {npy_path} å·²å®Œæˆ, shape={preds.shape}")


            print(f"\nåˆä½µå¾Œçš„äººé¡å¯è®€ç‰ˆçµæœå·²è¼¸å‡ºåˆ°ï¼š{txt_path}")

        elif RUN_SECOND_CLASSIFIER:

            # === å­˜æˆ JSON ===
            with open(f"{OUTPUT_PATH}/{coin_short_name}_logreg_train_daily_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json", "w", encoding="utf-8") as f:
                json.dump(august_daily, f, ensure_ascii=False, indent=4, default=int)

            print("å·²è¼¸å‡ºé€æ—¥é æ¸¬çµæœï¼š")
            print(f"- august: {OUTPUT_PATH}/{coin_short_name}_logreg_train_daily_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.json")

            # === åˆä½µ ä¸‰ç¨®å¹£ç¨® ===
            for coin, daily in august_daily.items():
                combined_daily.setdefault(coin, {}).update(daily)

            # === å­˜æˆåˆä½µå¾Œçš„ TXT ===
            txt_path = f"{OUTPUT_PATH}/logreg_combined_classifier_2_results{SUFFIX_FILTERED}{SUFFIX_AUGUST}.txt"
            with open(txt_path, "w", encoding="utf-8") as f:
                label_correct = 0
                label_total = 0

                for coin, daily in combined_daily.items():
                    f.write(f"\n=== {coin} ===\n")

                    records = []
                    for date, stats in sorted(daily.items()):
                        # --- æ¯æ—¥è¼¸å‡ºåˆ° TXT ---
                        line = (
                            f"{date} â†’ "
                            f"é æ¸¬: {''.join(stats['pred_symbols'])}  "
                            f"çœŸå¯¦: {''.join(stats['true_symbols'])}  "
                            f"çµæœ: {''.join(stats['result_symbols'])}\n"
                        )
                        f.write(line)

                        # --- æ›´æ–°ç´¯ç©æº–ç¢ºç‡ ---
                        label_total += 1
                        if stats["majority_correct"]:
                            label_correct += 1

                        # --- ä¿å­˜æ¯æ—¥é æ¸¬é¡åˆ¥ ---
                        records.append((date, stats["majority_pred"]))

                    # --- è¼¸å‡ºæ•´é«”æº–ç¢ºç‡ ---
                    acc = (label_correct / label_total * 100) if label_total > 0 else 0
                    f.write(f"\næ•´é«”æº–ç¢ºç‡: {acc:.2f}%\n")

            print(f"\nåˆä½µå¾Œçš„äººé¡å¯è®€ç‰ˆçµæœå·²è¼¸å‡ºåˆ°ï¼š{txt_path}")



def main():

    if RUN_FIRST_CLASSIFIER:

        pipeline_path = f"{SAVE_MODEL_PATH}/BERT_best_pipeline_{N_SAMPLES}{SUFFIX_FILTERED}.joblib"  # å„²å­˜è¨“ç·´æ¨¡å‹çš„ä½ç½®

        if not IS_RUN_AUGUST:
            # --- è¼‰å…¥è³‡æ–™ ---
            X_train, X_test, y_train, y_test, ids_train, ids_test = load_and_preprocess()

            # for count in range(LABELS):

            if IS_TRAIN:
                # --- è¨“ç·´æ¨¡å‹ --- 
                train_function(X_train, X_test, y_train, y_test, pipeline_path)

                # --- é æ¸¬æ¨¡å‹ ---
                predict_function(X_train, X_test, y_train, y_test, ids_train, ids_test, pipeline_path)
            else:
                if not os.path.exists(pipeline_path):
                    print("æ‰¾ä¸åˆ°å·²è¨“ç·´å¥½çš„ ç¬¬ä¸€å€‹åˆ†é¡å™¨ æ¨¡å‹ï¼Œè«‹å…ˆå°‡ IS_TRAIN è¨­ç‚º True")

                # --- é æ¸¬æ¨¡å‹ ---
                predict_function(X_train, X_test, y_train, y_test, ids_train, ids_test, pipeline_path)

        else:
            # --- é æ¸¬ 2025-08 ---
            predict_august_function(pipeline_path)

    elif RUN_SECOND_CLASSIFIER:

        pipeline_path = f"{SAVE_MODEL_PATH}/BERT_classifier_2{SUFFIX_FILTERED}.joblib"  # å„²å­˜è¨“ç·´æ¨¡å‹çš„ä½ç½®

        if not IS_RUN_AUGUST:
            if IS_GROUPED_CV == False:
                # --- è¼‰å…¥è³‡æ–™ ---
                X_train, X_test, y_train, y_test, ids_train, ids_test= load_and_preprocess()

                if IS_TRAIN:
                    # --- è¨“ç·´æ¨¡å‹ --- 
                    train_function(X_train, X_test, y_train, y_test, pipeline_path)

                    # --- é æ¸¬æ¨¡å‹ ---
                    predict_function(X_train, X_test, y_train, y_test, ids_train, ids_test, pipeline_path)
                else:
                    if not os.path.exists(pipeline_path):
                        print("æ‰¾ä¸åˆ°å·²è¨“ç·´å¥½çš„ ç¬¬äºŒå€‹åˆ†é¡å™¨ æ¨¡å‹ï¼Œè«‹å…ˆå°‡ IS_TRAIN è¨­ç‚º True")

                    # --- é æ¸¬æ¨¡å‹ ---
                    predict_function(X_train, X_test, y_train, y_test, ids_train, ids_test, pipeline_path)

            else:
                # å–å¾—è³‡æ–™
                X = np.load(f"{INPUT_PATH}/final_input/price_classifier/{MODEL_NAME[0]}/{MODEL_NAME[1]}_X_classifier_2{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy")
                y = np.load(f"{INPUT_PATH}/final_input/price_classifier/{MODEL_NAME[0]}/{MODEL_NAME[1]}_Y_classifier_2{SUFFIX_FILTERED}{SUFFIX_AUGUST}.npy")
                with open(f"{INPUT_PATH}/final_input/price_classifier/{MODEL_NAME[0]}/{MODEL_NAME[1]}_ids_classifier_2{SUFFIX_FILTERED}{SUFFIX_AUGUST}.pkl", 'rb') as file:
                    ids = pickle.load(file)

                y_categorized = categorize_array_multi(y, ids, T1, T2, T3, T4)  # shape (N,)

                # results_all = coin_month_cv(X, y_categorized, ids, C=C)

        else:
            # --- é æ¸¬ 2025-08 ---
            predict_august_function(pipeline_path)  




    # texts = load_tweets()
    # Y = load_price_diff(price_dir, coin_short_name)  # (N_coin, )

    # # print(len(texts))
    # # print(Y.shape[0])

    # assert len(texts) == Y.shape[0], f"{coin_short_name} texts and Y length mismatch!"

    # all_texts.extend(texts)
    # all_Y.append(Y)

    # all_Y = np.concatenate(all_Y)  # shape = (N_total, )

    # if IS_TRAIN:
    #     print(f"=== Processing Y (all coins combined) ===")
    #     labels = categorize_array_multi(all_Y)
    #     model_dir = f"{SAVE_PATH}/allcoins_y"

    #     # è¨“ç·´ + é æ¸¬å…¨éƒ¨æ¨æ–‡
    #     trainer = train_single_model(
    #         all_texts,
    #         labels,
    #         num_categories=NUM_CATEGORIES,
    #         model_dir=model_dir,
    #         epochs=EPOCHS,
    #         n_samples=N_SAMPLES,
    #         balanced=True
    #     )
        
    # print("\né–‹å§‹é æ¸¬å…¨éƒ¨æ¨æ–‡...")
    # # é æ¸¬å…¨éƒ¨æ¨æ–‡ + è¼¸å‡º CSV/JSON
    # fast_predict_all_models(all_texts, all_Y, tokenized_path=f"{SAVE_PATH}/tokenized_tweets.pt")





if __name__ == "__main__":
    main()