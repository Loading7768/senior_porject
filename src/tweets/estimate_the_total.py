import pandas as pd
import json
from glob import glob
from datetime import datetime
from pathlib import Path
import sys
from tqdm import tqdm
import os
import random
import matplotlib.pyplot as plt
import csv
from collections import defaultdict
from datetime import timedelta

import sys
from pathlib import Path
parent_dir = Path(__file__).resolve().parent.parent
sys.path.append(str(parent_dir))
from config import COIN_SHORT_NAME, JSON_DICT_NAME

'''å¯ä¿®æ”¹åƒæ•¸'''
OUTPUT_FILE = "../data/tweets/count/estimate/"

START_DATE = "2013/12/15"

END_DATE = "2025/07/31"
'''å¯ä¿®æ”¹åƒæ•¸'''
os.makedirs(OUTPUT_FILE, exist_ok=True)

hour_counter = []
partial_json_files = []
completed_json_files = []

# æ‰€æœ‰åŸå§‹æª”æ¡ˆ (æŠŠæ‰€æœ‰çµå°¾æ˜¯ .json çš„æª”æ¡ˆæŠ“å‡ºä¾†)
json_files = glob(f'../data/tweets/{COIN_SHORT_NAME}/*/*/{COIN_SHORT_NAME}_*.json')


# è½‰æ›é–‹å§‹èˆ‡çµæŸæ—¥æœŸ
START_DT = datetime.strptime(START_DATE, "%Y/%m/%d")
END_DT = datetime.strptime(END_DATE, "%Y/%m/%d") + timedelta(days=1) - timedelta(seconds=1)

def is_in_range(tweets):
    """æª¢æŸ¥è©²æª”æ¡ˆæ˜¯å¦åœ¨æ™‚é–“ç¯„åœå…§ï¼ˆç”¨ç¬¬ä¸€ç­†æ¨æ–‡æ™‚é–“åˆ¤æ–·ï¼‰"""
    if not tweets:
        return False
    date_dt = datetime.strptime(tweets[0]['created_at'], "%a %b %d %H:%M:%S %z %Y").replace(tzinfo=None)
    return START_DT <= date_dt <= END_DT



def hour_distribution():
    weekday_hour_counter = defaultdict(list)

    for json_file in tqdm(json_files, desc="çµ±è¨ˆå®Œæ•´æª”æ¡ˆå°æ™‚æ¯”ä¾‹"):
        with open(json_file, 'r', encoding="utf-8-sig") as file:
            data = json.load(file)

        tweets = data[JSON_DICT_NAME]
        if not tweets or not is_in_range(tweets):
            continue

        # æŠ“æœ€æ—©æ™‚é–“ä¾†åˆ¤æ–·æ˜¯å¦å®Œæ•´
        earliest_str = tweets[-1]["created_at"]
        earliest_dt = datetime.strptime(earliest_str, "%a %b %d %H:%M:%S %z %Y")
        earliest_time = earliest_dt.strftime("%H:%M:%S")

        # åˆ¤æ–·æœ‰æ²’æœ‰æŠ“å®Œ æ˜¯å¦æ˜¯ 00:XX:XX
        if earliest_time.startswith("00:"):
            for tweet in tweets:
                created_at = tweet.get('created_at')
                created_dt = datetime.strptime(created_at, "%a %b %d %H:%M:%S %z %Y")
                weekday = created_dt.weekday()  # 0=Monday, 6=Sunday
                hour = created_dt.hour
                weekday_hour_counter[weekday].append(hour)  # åªå–å°æ™‚æ•¸

            completed_json_files.append(json_file)  # æŠŠå®Œæ•´æŠ“åˆ°çš„æª”æ¡ˆåå…ˆå­˜èµ·ä¾†
        else:
            partial_json_files.append(json_file)  # æŠŠæ²’æœ‰å®Œæ•´æŠ“åˆ°çš„æª”æ¡ˆåå…ˆå­˜èµ·ä¾†

    # è¨ˆç®—æ¯å€‹ weekday çš„ hour åˆ†å¸ƒæ¯”ä¾‹
    weekday_hour_distribution = {}

    for weekday, hours in weekday_hour_counter.items():
        df = pd.DataFrame({'hour': hours})
        dist = df['hour'].value_counts().sort_index()
        dist = dist / dist.sum()
        weekday_hour_distribution[weekday] = dist

    for weekday in sorted(weekday_hour_distribution):
        print(f"âœ… æ˜ŸæœŸ {weekday} å°æ™‚æ¯”ä¾‹ï¼š")
        print(str(weekday_hour_distribution[weekday]))

    # å„²å­˜æ–‡å­—ç‰ˆ
    output_path_completed = f"{OUTPUT_FILE}/{COIN_SHORT_NAME}_weekday_hour_distribution.txt"
    with open(output_path_completed, 'w', encoding="utf-8-sig") as txtfile:
        for weekday in sorted(weekday_hour_distribution):
            txtfile.write(f"ğŸ—“ï¸ æ˜ŸæœŸ {weekday} åˆ†ä½ˆï¼š\n")
            txtfile.write(str(weekday_hour_distribution[weekday]))
            txtfile.write("\n\n")

    return weekday_hour_distribution



def estimate(weekday_hour_distribution):
    results = []

    # å…ˆæ¸…ç©º txt å…§çš„è³‡æ–™
    output_path_partial_txt = f"{OUTPUT_FILE}/{COIN_SHORT_NAME}_estimate.txt"
    with open(output_path_partial_txt, 'w', encoding="utf-8-sig") as txtfile:
        txtfile.write("")

    output_path_partial_csv = f"{OUTPUT_FILE}/{COIN_SHORT_NAME}_estimate.csv"

    for json_file in tqdm(json_files, desc="ä¼°è¨ˆæ•¸é‡ä¸­..."):
        with open(json_file, 'r', encoding="utf-8-sig") as file:
            data = json.load(file)

        tweets = data[JSON_DICT_NAME]
        if not tweets or not is_in_range(tweets):
            continue

        # å–å¾—æª”å
        file_name_with_json = os.path.basename(json_file)
        file_name = os.path.splitext(file_name_with_json)[0]

        # å–å¾—æ—¥æœŸ
        date_dt = datetime.strptime(tweets[0]['created_at'], "%a %b %d %H:%M:%S %z %Y")
        date_str = date_dt.strftime("%Y-%m-%d")
        weekday = date_dt.weekday()

        # ä¼°ç®—æ¯å€‹ partial æª”æ¡ˆçš„ç¸½æ¨æ–‡æ•¸
        if json_file in partial_json_files:
            # çµ±è¨ˆè©²æª”æ¡ˆç›®å‰å·²æŠ“åˆ°çš„æ¨æ–‡æ•¸é‡èˆ‡å°æ™‚åˆ†ä½ˆ
            partial_hour_counter = [datetime.strptime(tweet['created_at'], "%a %b %d %H:%M:%S %z %Y").hour for tweet in tweets]
            
            # è½‰æˆ DataFrame å†å–å”¯ä¸€å°æ™‚
            partial_hour_df = pd.DataFrame({'hour': partial_hour_counter})
            observed_hours = partial_hour_df['hour'].unique()  # .unique() å–å¾—ä¸é‡è¤‡çš„å°æ™‚æ•¸

            weekday_dist = weekday_hour_distribution.get(weekday)
            if weekday_dist is None or weekday_dist.sum() == 0:
                continue  # è‹¥è©² weekday æ²’æœ‰åˆ†å¸ƒï¼Œè·³é

            observed_percentage = weekday_dist.loc[observed_hours].sum()  # .loc[observed_hours] â†’ åªå–å‡ºè©²æª”æ¡ˆæœ‰æŠ“åˆ°çš„å°æ™‚ç¯„åœçš„æ¯”ä¾‹
            estimated_total = len(tweets) / observed_percentage

            # è¨ˆç®—æ”¾å¤§è¶´æ•¸
            expansion_ratio = estimated_total / len(tweets)

            # å„²å­˜çµæœ csv
            results.append({
                "filename": file_name,
                "date": date_str,
                "isCompleteData": False,
                "original_count": len(tweets),
                "predicted_count": int(round(estimated_total)),
                "expansion_ratio": expansion_ratio
            })

            with open(output_path_partial_txt, 'a', encoding="utf-8-sig") as txtfile:
                txtfile.write(f"ğŸ“„ æª”æ¡ˆï¼š{json_file}\n")
                txtfile.write(f"å¯¦éš›å·²æŠ“æ•¸ï¼š{len(tweets)}ï¼Œè§€å¯Ÿå°æ™‚ç¯„åœï¼š{observed_hours}\n")
                txtfile.write(f"å·²æŠ“ä½”æ¯”ï¼š{observed_percentage:.2%}ï¼Œä¼°ç®—ç¸½æ•¸ï¼šç´„ {int(round(estimated_total))} ç­†\n\n")
        
        # è‹¥æ˜¯æœ‰æŠ“å®Œçš„æª”æ¡ˆ å°±ç›´æ¥æŠŠæ•¸é‡å­˜é€² csv æª”
        else:

            # å„²å­˜çµæœ csv
            results.append({
                "filename": file_name,
                "date": date_str,
                "isCompleteData": True,
                "original_count": len(tweets),
                "predicted_count": len(tweets),
                "expansion_ratio": 1
            })

        # æŒ‰æ—¥æœŸæ’åº
        results.sort(key=lambda x: x['date'])

        with open(output_path_partial_csv, 'w', newline='', encoding='utf-8-sig') as csvfile:
            fieldnames = ['filename', 'date', 'isCompleteData', 'original_count', 'predicted_count', 'expansion_ratio']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for row in results:
                writer.writerow(row)

    print(f"âœ… å·²å…¨éƒ¨åŸ·è¡Œå®Œæˆ è³‡æ–™è·¯å¾‘: {OUTPUT_FILE}")


def accuracy(weekday_hour_distribution):
    errors = []
    results = []  # å„²å­˜è¦è¼¸å‡ºçš„çµæœ

    for json_file in tqdm(completed_json_files, desc="é©—è­‰ä¼°ç®—æº–ç¢ºåº¦"):
        with open(json_file, 'r', encoding="utf-8-sig") as file:
            data = json.load(file)

        tweets = data[JSON_DICT_NAME]
        if not tweets or not is_in_range(tweets):
            continue

        # å–å¾—æ—¥æœŸï¼ˆå‡è¨­æ¯å€‹jsonæª”éƒ½æ˜¯ä¸€å¤©è³‡æ–™ï¼‰
        # ä»¥ç¬¬ä¸€ç­†æ¨æ–‡æ™‚é–“ç‚ºè©²æ—¥ä»£è¡¨
        date_dt = datetime.strptime(tweets[0]['created_at'], "%a %b %d %H:%M:%S %z %Y")
        date_str = date_dt.strftime("%Y-%m-%d")
        weekday = date_dt.weekday()

        # æ¨¡æ“¬ã€Œåªä¿ç•™æŸäº›å°æ™‚ã€ï¼š (éƒ½æ˜¯é€£çºŒçš„ä¸”å¾ 23 é–‹å§‹å¾€å›)
        k = random.randint(1, 23)  # ä½ è¦é¸çš„é€£çºŒå°æ™‚æ•¸é‡
        end_hour = 23
        start_hour = max(0, end_hour - k + 1)  # å¾€å‰æ¨ k å°æ™‚ï¼Œä½†ä¸èƒ½å°æ–¼ 0
        partial_hours = list(range(start_hour, end_hour + 1))
        partial_tweets = [tweet for tweet in tweets if datetime.strptime(
            tweet['created_at'], "%a %b %d %H:%M:%S %z %Y").hour in partial_hours]

        if not partial_tweets:
            continue

        # ç”¨åŸæœ¬é‚è¼¯ä¼°ç®—ï¼š
        weekday_dist = weekday_hour_distribution.get(weekday)
        if weekday_dist is None or weekday_dist.sum() == 0:
            continue

        observed_percentage = weekday_dist.loc[partial_hours].sum()
        estimated_total = len(partial_tweets) / observed_percentage

        # è¨ˆç®—èª¤å·®
        real_total = len(tweets)
        error_rate = abs(estimated_total - real_total) / real_total

        errors.append(error_rate)

        # å„²å­˜çµæœ
        results.append({
            "date": date_str,
            "actual_count": real_total,
            "predicted_count": int(round(estimated_total))
        })

    # è¼¸å‡º CSV
    os.makedirs(OUTPUT_FILE, exist_ok=True)
    output_csv_path = os.path.join(OUTPUT_FILE, f"{COIN_SHORT_NAME}_accuracy_predictions.csv")

    # æŒ‰æ—¥æœŸæ’åº
    results.sort(key=lambda x: x['date'])

    with open(output_csv_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = ['date', 'actual_count', 'predicted_count']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for row in results:
            writer.writerow(row)



def plot_errors():
    # å‡è¨­è®€å…¥ä¸€å€‹csvï¼ŒåŒ…å«æ—¥æœŸã€çœŸå¯¦å€¼ã€é ä¼°å€¼
    df = pd.read_csv(f"../data/tweets/count/estimate/{COIN_SHORT_NAME}_accuracy_predictions.csv")

    # è¨ˆç®—èª¤å·®
    df['abs_error'] = (df['predicted_count'] - df['actual_count']).abs()
    df['percentage_error'] = df['abs_error'] / df['actual_count'] * 100  # ç™¾åˆ†æ¯”èª¤å·®

    # åŸºæœ¬çµ±è¨ˆæŒ‡æ¨™
    mean_abs_error = df['abs_error'].mean()
    mean_percentage_error = df['percentage_error'].mean()
    median_percentage_error = df['percentage_error'].median()
    max_percentage_error = df['percentage_error'].max()
    min_percentage_error = df['percentage_error'].min()

    print(f"å¹³å‡çµ•å°èª¤å·® (Mean Absolute Error): {mean_abs_error:.2f} (å¹³å‡ä¸€å¤©é ä¼°èˆ‡å¯¦éš›ç›¸å·®æ¨æ–‡æ•¸)")
    print(f"å¹³å‡ç™¾åˆ†æ¯”èª¤å·® (Mean Absolute Percentage Error, MAPE): {mean_percentage_error:.2f}% (é ä¼°æ•¸é‡å¹³å‡åé›¢çœŸå¯¦å€¼)")
    print(f"ä¸­ä½æ•¸ç™¾åˆ†æ¯”èª¤å·®: {median_percentage_error:.2f}% (è¶…éä¸€åŠå¤©çš„èª¤å·®ç‡ä½æ–¼çš„è¶´æ•¸)")
    print(f"æœ€å¤§ç™¾åˆ†æ¯”èª¤å·®: {max_percentage_error:.2f}% (æœ€åš´é‡ä¸€å¤©é ä¼°åå·®)")
    print(f"æœ€å°ç™¾åˆ†æ¯”èª¤å·®: {min_percentage_error:.2f}% (æœ€å¥½çš„ä¸€å¤©é ä¼°åå·®)")

    output_path_completed = f"{OUTPUT_FILE}/{COIN_SHORT_NAME}_hour_distribution_and_errors.txt"
    with open(output_path_completed, 'a', encoding="utf-8-sig") as txtfile:
        txtfile.write(f"å¹³å‡çµ•å°èª¤å·® (Mean Absolute Error): {mean_abs_error:.2f} (å¹³å‡ä¸€å¤©é ä¼°èˆ‡å¯¦éš›ç›¸å·®æ¨æ–‡æ•¸)\n")
        txtfile.write(f"å¹³å‡ç™¾åˆ†æ¯”èª¤å·® (Mean Absolute Percentage Error, MAPE): {mean_percentage_error:.2f}% (é ä¼°æ•¸é‡å¹³å‡åé›¢çœŸå¯¦å€¼)\n")
        txtfile.write(f"ä¸­ä½æ•¸ç™¾åˆ†æ¯”èª¤å·®: {median_percentage_error:.2f}% (è¶…éä¸€åŠå¤©çš„èª¤å·®ç‡ä½æ–¼çš„è¶´æ•¸)\n")
        txtfile.write(f"æœ€å¤§ç™¾åˆ†æ¯”èª¤å·®: {max_percentage_error:.2f}% (æœ€åš´é‡ä¸€å¤©é ä¼°åå·®)\n")
        txtfile.write(f"æœ€å°ç™¾åˆ†æ¯”èª¤å·®: {min_percentage_error:.2f}% (æœ€å¥½çš„ä¸€å¤©é ä¼°åå·®)\n")

    output_figures = "../outputs/figures"

    # ç¹ªè£½é ä¼°èˆ‡å¯¦éš›æ•¸é‡æ¯”è¼ƒåœ–
    plt.figure(figsize=(12,6))
    plt.plot(df['date'], df['actual_count'], label='actual_count')
    plt.plot(df['date'], df['predicted_count'], label='predicted_count', linestyle='--')
    plt.xticks(rotation=45)
    plt.xlabel('date')
    plt.ylabel('tweet_count')
    plt.title('actual_count vs. predicted_count')
    plt.legend()
    plt.tight_layout()
    plt.savefig(f'{output_figures}/{COIN_SHORT_NAME}_actual_count_vs_predicted_count.png', dpi=300, bbox_inches='tight')
    plt.close()  # é—œé–‰åœ–è¡¨ï¼Œé¿å…ä½”ç”¨è¨˜æ†¶é«”æˆ–å½±éŸ¿å¾ŒçºŒç¹ªåœ–

    # ç¹ªè£½ç™¾åˆ†æ¯”èª¤å·®åˆ†å¸ƒåœ–
    plt.figure(figsize=(8,5))
    plt.hist(df['percentage_error'], bins=30, color='orange', alpha=0.7)
    plt.xlabel('Percentage Error (%)')
    plt.ylabel('Number of Days')
    plt.title('Distribution of Estimation Percentage Errors')
    plt.savefig(f'{output_figures}/{COIN_SHORT_NAME}_percentage_error_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()



def main():
    weekday_hour_distribution = hour_distribution()
    estimate(weekday_hour_distribution)
    accuracy(weekday_hour_distribution)
    plot_errors()


if __name__ == '__main__':
    main()